{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa4daf0-04ce-4fe8-8780-46e09d239f06",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import sqlite3\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import open3d\n",
        "import seaborn as sns\n",
        "import random\n",
        "import sys\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from visualize_model import Model\n",
        "from database import blob_to_array, pair_id_to_image_ids\n",
        "import plotly.graph_objects as go\n",
        "# import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25bfed4a-95ed-4c25-ae03-dfa1be34bfa4",
      "metadata": {},
      "source": [
        "# 1. 3D mesh reconstruction from a set of images from the Gerrard Hall dataset.\n",
        "\n",
        "This exercise was used as an initial exploration of COLMAP in order to become familiar with its workflow, reconstruction pipeline, and the different representations it produces.\n",
        "\n",
        "First, we installed Colmap and runned the automatic reconstruction on the Gerrard Hall dataset. This pipeline consists of two main stages: **sparse reconstruction** and **dense reconstruction**.\n",
        "\n",
        "## Sparse reconstruction\n",
        "\n",
        "After running the automatic reconstruction, we visualized the *sparse* reconstruction directly in COLMAP. This visualization shows the recovered camera poses (in red) and the 3D sparse point cloud representing the structure of the scene. As shown in **Figure 1**, this sparse model allows us to verify that the cameras have been correctly localized and that the general geometry of the scene has been recovered.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"figures/gerrard_hall_colmap.png\" width=\"45%\">\n",
        "</p>\n",
        "<p align=\"center\"><b>Figure 1:</b> Sparse reconstruction visualized in COLMAP, showing the estimated camera poses (red) and the sparse 3D point cloud.</p>\n",
        "\n",
        "## Dense reconstruction\n",
        "\n",
        "Once the sparse reconstruction was completed, COLMAP computed a *dense* reconstruction, which produces a much denser point cloud and allows generating a surface mesh.\n",
        "\n",
        "For the meshing step, different reconstruction methods can be used, such as:\n",
        "\n",
        "* **Poisson Meshing**: Produces smooth and detailed surfaces, preserving fine geometric details and color information. However, it may introduce holes or missing regions in areas with insufficient data.\n",
        "\n",
        "* **Delaunay Meshing**: Produces a more complete surface and tends to close gaps, but usually lacks fine details and does not preserve color information as well as Poisson.\n",
        "\n",
        "A visual comparison between the two meshing strategies is shown in **Figure 2**. In our case, we chose the **Poisson** mesher for the reconstruction, as it provided better visual quality, finer details, and more accurate color information.\n",
        "\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_poisson.png\" width=\"85%\">\n",
        "    <p><b>(a)</b> Poisson mesh reconstruction.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_delaunay.png\" width=\"85%\">\n",
        "    <p><b>(b)</b> Delaunay mesh reconstruction.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\"><b>Figure 2:</b> Comparison between Poisson and Delaunay meshing. The Poisson mesh preserves fine geometric details and color information, while the Delaunay mesh produces a more complete but less detailed surface.</p>\n",
        "\n",
        "After selecting the Poisson mesher, COLMAP produced two different mesh representations: \n",
        "\n",
        "* `meshed-poisson.ply`: A smooth surface mesh obtained via Poisson reconstruction. It preserves fine details and color but may contain holes or artifacts.\n",
        "\n",
        "* `fused.ply`: A fused point-based surface representation, which is less smooth but more directly connected to the original dense point cloud.\n",
        "\n",
        "A visual comparison between both representations is shown in **Figure 3**. \n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_poisson.png\" style=\"width: 85%;\">\n",
        "    <p><b>(a)</b> <code>meshed-poisson.ply</code></p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_poisson_fused.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> <code>fused.ply</code></p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\"><b>Figure 3:</b> Comparison between the two mesh representations generated by COLMAP. The Poisson mesh (a) produces a smoother and more visually detailed surface, while the fused representation (b) is noisier but more directly connected to the original dense point cloud.</p>\n",
        "\n",
        "Based on this comparison, we selected the `meshed-poisson.ply` representation as our \"final\" mesh, as it provides smoother surfaces, better geometric details, and more accurate color information. Although it may contain some small holes or artifacts, these can be corrected through post-processing. \n",
        "\n",
        "## Mesh Post-processing\n",
        "\n",
        "As we can see in **Figure 4**, the raw mesh contained several artifacts and noisy components, especially around the borders and less constrained regions.\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_mesh_before_1.png\" style=\"width: 85%;\">\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_mesh_before_2.png\" style=\"width: 85%;\">\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 4:</b> Two different views of the raw <code>meshed-poisson.ply</code> mesh, showing artifacts and noisy regions, especially near the borders.</p>\n",
        "\n",
        "\n",
        "To improve the final result, we applied different cleaning filters in MeshLab, such as:\n",
        "\n",
        "*  **Removing isolated components (wrt Diameter)**, \n",
        "* **Manually removed a connected region** that corresponded to a reconstruction artifact by selecting the undesired faces with a rectangular selection tool and deleting the selected vertices and faces. \n",
        "\n",
        "**Figure 5** hows the cleaned versions of the previous meshes, obtained after applying the post-processing steps described above.\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_mesh_after_1.png\" style=\"width: 85%;\">\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_mesh_after_2.png\" style=\"width: 85%;\">\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 5:</b> Two different views of the cleaned <code>meshed-poisson.ply</code> mesh after post-processing, showing a significant reduction of artifacts and noisy regions.</p>\n",
        "\n",
        "\n",
        "The final processed mesh can be downloaded from the following link:\n",
        "[Gerrard Hall Mesh](https://drive.google.com/file/d/1yNNZeB9sxfI0jd5833KQu7Z_Lk0r3WLD/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19c31494-3050-4951-aed1-b463f3fba821",
      "metadata": {},
      "source": [
        "# 2. Analyze reconstructions using python\n",
        "## 2.1. Run the notebook, using the Gerrard Hall reconstruction (0.5)\n",
        "#### <span style='color:Green'> - Add the path to your reconstruction. Answer the questions at the end  </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41796fd2-2304-487f-a74c-3f1a51ad83f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add your path\n",
        "reconstruction_path = \"./gerrard-hall/sparse/0\"\n",
        "database_path = \"./gerrard-hall/database.db\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a701bb3-945a-434c-880c-849dad97a97d",
      "metadata": {},
      "source": [
        "#### Load an existing reconstruction and print its contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf644962-ba41-403f-a1a4-3e1b08d16151",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model()\n",
        "model.read_model(reconstruction_path, ext='.bin') # Should also work with .txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b629e852-0407-4eff-ba62-b9d1d51015bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "images = model.images\n",
        "cameras = model.cameras\n",
        "points3D = model.points3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca47fb58-dbcd-4c6f-8168-e296832aacf5",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(f\"Loaded {len(images)} images. This is the information available for one of them:\")\n",
        "print(images[1])\n",
        "print(f\"\\nLoaded {len(cameras)} cameras. This is the information available for one of them:\")\n",
        "print(cameras[1])\n",
        "print(f\"\\nLoaded {len(points3D)} 3D points. This is the information available for one of them:\")\n",
        "print(points3D[1]) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ce97039-6c3e-40e9-af81-e0795fc5b41a",
      "metadata": {
        "tags": []
      },
      "source": [
        "#### Load the database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74d1dde-024f-47b6-85cc-99f0801f414c",
      "metadata": {},
      "outputs": [],
      "source": [
        "db = sqlite3.connect(database_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a83e688",
      "metadata": {},
      "source": [
        "> <span style=\"font-weight:bold;\">Note:</span>\n",
        "> In COLMAP, keypoints are stored in the database as a binary BLOB with shape \n",
        "><strong>(rows × cols)</strong>, where <code>rows</code> is the number of keypoints and \n",
        "><code>cols</code> is the number of values per keypoint.\n",
        ">\n",
        "> According to the COLMAP documentation:\n",
        ">    \"*If the keypoints have 4 columns, the feature\n",
        ">    geometry is a similarity and the third and fourth columns correspond to the scale and\n",
        ">    orientation of the feature, following SIFT conventions. If the keypoints have 6 columns,\n",
        ">    the feature geometry is an affinity and the last four columns encode the affine shape\n",
        ">    of the feature.*\"\n",
        ">\n",
        "> Therefore, we had to change the initial code to load the keypoints properly. Same is taken into account in the subsequent sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6ff5d3-a33f-41f5-882d-d370bf3dd489",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load keypoints with proper reshaping based on actual cols per image\n",
        "# Each image may have different cols (4, or 6), so we query all fields together\n",
        "keypoints = dict()\n",
        "for image_id, data, rows, cols in db.execute(\n",
        "        \"SELECT image_id, data, rows, cols FROM keypoints\"):\n",
        "    # Reshape to actual (rows, cols) and keep only (x, y) coordinates\n",
        "    kpts = blob_to_array(data, np.float32, (rows, cols))[:, :2]\n",
        "    keypoints[image_id] = kpts\n",
        "\n",
        "# Check what column counts were found\n",
        "unique_cols = set()\n",
        "for image_id, data, rows, cols in db.execute(\n",
        "        \"SELECT image_id, data, rows, cols FROM keypoints\"):\n",
        "    unique_cols.add(cols)\n",
        "print(f\"Keypoint column counts in database: {unique_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c843a569-d812-4bfe-8f61-e68cc9d9dfbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loaded keypoints from {len(keypoints)} images. These are the {len(keypoints[1])} keypoints for one of them:\")\n",
        "print(keypoints[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27f31f2a-d3b3-48af-b0cc-1276f5cfe177",
      "metadata": {},
      "outputs": [],
      "source": [
        "matches = dict()\n",
        "count_no_data = 0\n",
        "for pair_id, data in db.execute(\"SELECT pair_id, data FROM matches\"):\n",
        "    if data is None:\n",
        "        count_no_data += 1\n",
        "    else:\n",
        "        matches[pair_id_to_image_ids(pair_id)] = blob_to_array(data, np.uint32, (-1, 2))\n",
        "print(f\"Loaded {len(matches)} matches. {count_no_data}/{len(matches)+count_no_data} matches contained no data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8472b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temporal\n",
        "dir(matches)\n",
        "matches.__class__\n",
        "matches.items()\n",
        "matches.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa07761b-eaec-488d-b1e9-36720e203a31",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"These are the matches between two images:\")\n",
        "print(matches[1,3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a12c0675-6841-4519-b79f-cc2b811b94fd",
      "metadata": {},
      "source": [
        "#### Visualize the point cloud and cameras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb95e07c-b1d6-4f08-8842-8fc41b676a10",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.create_window()\n",
        "model.add_points()\n",
        "model.add_cameras(scale=0.25)\n",
        "model.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d5e272-83de-4136-b047-885051bd7e78",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'>  How many keypoints there are in total? </span> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73905673",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"#Keypoints total: {sum(kpts.shape[0] for kpts in keypoints.values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c782bbf7",
      "metadata": {},
      "source": [
        "There are **1,061,700 keypoints** in total across all 100 images. That averages to **10,617 keypoints per image**. This is a normal amount of keypoints for a SIFT detector in high resolution images of **$5616 \\times 3744$**\n",
        "\n",
        "Not all detected keypoints will contribute to the final 3D reconstruction. Only keypoints that are successfully matched across multiple images and pass geometric verification will be triangulated into 3D points."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c13142-d04b-4f48-9b28-5e78b9c72a8b",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'>  How many 3D points originated from a keypoint in the first image? </span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d1b7c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_points3d_originated_in_image_1 = sum(1 for point3D in points3D.values() if 1 in point3D.image_ids)\n",
        "\n",
        "print(f\"#3d points from image 1: {num_points3d_originated_in_image_1}\")\n",
        "\n",
        "num_keypoints_image_1 = keypoints[1].shape[0]\n",
        "\n",
        "print(f\"#Kepoints from image 1: {num_keypoints_image_1}\")\n",
        "\n",
        "print(f\"Conversion percentage: {num_points3d_originated_in_image_1 / num_keypoints_image_1 * 100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acd85b6f",
      "metadata": {},
      "source": [
        "There are **2,824 3D points** that originated from a keypoint in image 1, out of the **12,207 keypoints** detected in that image. This means only about **23%** of the keypoints in image 1 were successfully triangulated into 3D points.\n",
        "\n",
        "This relatively low percentage is expected for several reasons:\n",
        "- Many keypoints may not have reliable matches in other images (occlusions, changes in the viewpoint, or their descriptor not being distinct enough-Lowe's ratio).\n",
        "- Some matches are rejected during geometric verification (RANSAC) as outliers.\n",
        "\n",
        "The fact that image 1 contributes 2,824 points out of the total 42,815 3D points (~6.6%) is reasonable, considering there are 100 images in the dataset and each 3D point is typically observed by multiple images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "075a83ca-0ad4-493f-a7b7-9e11ae8153c0",
      "metadata": {},
      "source": [
        "## 2.2 Plot the 3D points coloured according to the number of images and error. (0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6803fb41-cb01-43c2-9853-9f4458640a75",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'> - Plot the 3D points coloured according to the **number of images** from which it originated. </span> Can you extract any conclusions from the visualization? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d55db2-d7b4-4eda-91c2-8b2001fbf6e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "xyz = []\n",
        "num_images = []\n",
        "errors = []\n",
        "\n",
        "for point3D in points3D.values():\n",
        "    xyz.append(point3D.xyz)\n",
        "    num_images.append(len(point3D.image_ids))\n",
        "    errors.append(point3D.error)\n",
        "\n",
        "xyz = np.array(xyz)\n",
        "num_images = np.array(num_images)\n",
        "errors = np.array(errors)\n",
        "\n",
        "num_images_norm = (num_images - num_images.min()) / (num_images.max() - num_images.min())\n",
        "errors_norm = (errors - errors.min()) / (errors.max() - errors.min())\n",
        "\n",
        "\n",
        "colors_by_images = cm.jet(num_images_norm)[:, :3]\n",
        "colors_by_error = cm.jet(errors_norm)[:, :3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "240545d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 1))\n",
        "fig.subplots_adjust(bottom=0.5)\n",
        "\n",
        "norm = plt.Normalize(vmin=num_images.min(), vmax=num_images.max())\n",
        "cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap='jet'), \n",
        "                    cax=ax, orientation='horizontal')\n",
        "cbar.set_label('Number of images')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e4a222",
      "metadata": {},
      "outputs": [],
      "source": [
        "vis = open3d.visualization.Visualizer()\n",
        "vis.create_window()\n",
        "\n",
        "pcd = open3d.geometry.PointCloud()\n",
        "\n",
        "pcd.points = open3d.utility.Vector3dVector(xyz)\n",
        "pcd.colors = open3d.utility.Vector3dVector(colors_by_images)\n",
        "\n",
        "vis.add_geometry(pcd)\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "vis.run()\n",
        "vis.destroy_window()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db190967",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's plot a histogram too\n",
        "images_per_point3d = [len(p3d.image_ids) for p3d in points3D.values()]\n",
        "counts, bins = np.histogram(images_per_point3d, bins=[i for i in range(max(images_per_point3d) + 1)])\n",
        "_ = plt.hist(bins[:-1], bins, weights=counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a19cc260",
      "metadata": {},
      "source": [
        "As can be seen in the histogram, most 3D points are visible from only a few images (2 to 10 images), with the distribution peaking at 3 images and then following a long-tailed pattern. This is expected behavior in Structure-from-Motion reconstructions:\n",
        "\n",
        "- Points in areas with **limited camera coverage**, like the buildings and vegetation around the hall, are only observed by a small number of views.\n",
        "- Points in **well-covered regions**, in this case the hall building, tend to be observed by more cameras.\n",
        "- The long tail indicates that only a small subset of points are visible from many viewpoints.\n",
        "\n",
        "When visualizing the point cloud colored by number of images, there are no clear regions with consistently higher or lower observation counts. The variation appears to be distributed somewhat homogeneously throughout the whole scene rather than concentrated in specific areas. This suggests that the camera trajectory provides relatively uniform coverage, and the number of images per point depends more on local properties of the point than on global position.\n",
        "\n",
        "<img src=\"figures/images_per_point3d.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da04601",
      "metadata": {},
      "source": [
        "To better understand which points are most reliably reconstructed, we visualize below only the points observed by at least 10 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29ed04b",
      "metadata": {},
      "outputs": [],
      "source": [
        "min_observations = 10\n",
        "\n",
        "xyz_plus10 = []\n",
        "colors_plus10 = []\n",
        "for point3D in points3D.values():\n",
        "    if len(point3D.image_ids) >= min_observations:\n",
        "        xyz_plus10.append(point3D.xyz)\n",
        "        colors_plus10.append(point3D.rgb / 255)\n",
        "\n",
        "print(f\"Points with >= {min_observations} observations: {len(xyz_plus10)} / {len(points3D)} ({100*len(xyz_plus10)/len(points3D):.1f}%)\")\n",
        "\n",
        "vis = open3d.visualization.Visualizer()\n",
        "vis.create_window()\n",
        "\n",
        "pcd = open3d.geometry.PointCloud()\n",
        "\n",
        "pcd.points = open3d.utility.Vector3dVector(xyz_plus10)\n",
        "pcd.colors = open3d.utility.Vector3dVector(colors_plus10)\n",
        "\n",
        "vis.add_geometry(pcd)\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "vis.run()\n",
        "vis.destroy_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e44073",
      "metadata": {},
      "source": [
        "<img src=\"figures/points_from_plus10_images.png\"/>\n",
        "\n",
        "The visualization shows that the 3D points visible from 10 or more images are distributed accross the 4 walls of the building. There is a gap on one of the walls close to the corner. This gap is present in the leftmost part of the backside wall of the hall. When looking at the whole pointcloud, we see that there are slightly less 3D points there than in other regions, because the area is occluded in some images by vegetation.\n",
        "\n",
        "<img src=\"figures/gerrard_hall_tree_occlusion.jpg\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ba4033-e370-45e0-9dcd-2964d3c5763a",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'> - Plot the 3D points coloured according to the **error**. </span> - What is this parameter? Can you extract any conclusions from the visualization?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b23532f",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 1))\n",
        "fig.subplots_adjust(bottom=0.5)\n",
        "\n",
        "norm = plt.Normalize(vmin=errors_norm.min(), vmax=errors_norm.max())\n",
        "cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap='jet'), \n",
        "                    cax=ax, orientation='horizontal')\n",
        "cbar.set_label('Error')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb29f94-1864-4e21-b534-7eb681dd9057",
      "metadata": {},
      "outputs": [],
      "source": [
        "vis = open3d.visualization.Visualizer()\n",
        "vis.create_window()\n",
        "\n",
        "pcd = open3d.geometry.PointCloud()\n",
        "\n",
        "pcd.points = open3d.utility.Vector3dVector(xyz)\n",
        "pcd.colors = open3d.utility.Vector3dVector(colors_by_error)\n",
        "\n",
        "vis.add_geometry(pcd)\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "vis.run()\n",
        "vis.destroy_window()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d01f26e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's plot a histogram too\n",
        "error_per_point3d = [p3d.error for p3d in points3D.values()]\n",
        "counts, bins = np.histogram(error_per_point3d)\n",
        "_ = plt.hist(bins[:-1], bins, weights=counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a36ce4",
      "metadata": {},
      "source": [
        "<span style='color:darkcyan'>\n",
        "\n",
        "The **error** property of a 3D point represents the **mean reprojection error** across all images that observe that point. Reprojection error measures the pixel distance between:\n",
        "- The detected 2D keypoint location in an image\n",
        "- The projected location of the triangulated 3D point back onto that image\n",
        "\n",
        "A low reprojection error indicates that the 3D point is geometrically consistent across all its observations, while a high error suggests localization uncertainty or potential outliers.\n",
        "\n",
        "From the histogram, we observe that **most points have low reprojection error**, around 0.5, which indicates that the reconstruction is good. However, there is a **tail of points with higher errors**.\n",
        "\n",
        "When visualizing the point cloud colored by error, **no clear spatial pattern emerges**. The high-error points appear scattered throughout the scene, resembling \"salt noise\" rather than being concentrated in specific regions. This suggests that high-error points are likely caused by:\n",
        "- **Matching errors**: Incorrect feature correspondences that passed geometric verification\n",
        "- **Depth discontinuities**: Points near object boundaries where small errors in localization cause large reprojection errors (???)\n",
        "- **Repetitive textures**: Areas where similar-looking features may be incorrectly matched. This is specially relevant in the wall of bricks, that may contain some repeating patterns.\n",
        "\n",
        "As can be seen in the image (ref Figure!!!) the points with the most error are not concentrated  in specific areas, but are distributed around the scene.\n",
        "\n",
        "<img src=\"figures/gerrard_hall_error_viz.png\">\n",
        "\n",
        "To better understand the error distribution, we analyze below the correlation between reprojection error and the number of observing images.\n",
        "\n",
        "</span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87l2y0s2rtw",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze correlation between error and number of observations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scatter plot: error vs number of images\n",
        "axes[0].scatter(num_images, errors, alpha=0.3, s=1)\n",
        "axes[0].set_xlabel('Number of observing images')\n",
        "axes[0].set_ylabel('Reprojection error (pixels)')\n",
        "axes[0].set_title('Error vs Number of Observations')\n",
        "\n",
        "# Box plot: error distribution grouped by number of images\n",
        "max_imgs_for_boxplot = min(20, num_images.max())  # Limit for readability\n",
        "mask = num_images <= max_imgs_for_boxplot\n",
        "data_for_boxplot = [errors[num_images == i] for i in range(2, max_imgs_for_boxplot + 1) if np.sum(num_images == i) > 0]\n",
        "labels = [str(i) for i in range(2, max_imgs_for_boxplot + 1) if np.sum(num_images == i) > 0]\n",
        "axes[1].boxplot(data_for_boxplot, labels=labels)\n",
        "axes[1].set_xlabel('Number of observing images')\n",
        "axes[1].set_ylabel('Reprojection error (pixels)')\n",
        "axes[1].set_title('Error Distribution by Observation Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compute correlation coefficient\n",
        "correlation = np.corrcoef(num_images, errors)[0, 1]\n",
        "print(f\"Pearson correlation coefficient: {correlation:.4f}\")\n",
        "print(f\"Mean error: {errors.mean():.4f} pixels\")\n",
        "print(f\"Median error: {np.median(errors):.4f} pixels\")\n",
        "print(f\"Std error: {errors.std():.4f} pixels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qp6q302mvtp",
      "metadata": {},
      "source": [
        "<span style='color:darkcyan'>\n",
        "\n",
        "The analysis shows a **weak positive correlation (0.24)** between the number of images and the reprojection error. This means that points seen by more images tend to have **slightly higher error**.\n",
        "\n",
        "Several factors may explain this behavior:\n",
        "\n",
        "1. **Averaging effect**: The error metric is the *mean* reprojection error across all observations. With more images, there are more opportunities for one or two views to have slightly inaccurate keypoint detections (due to motion blur, slight defocus, or viewpoint-dependent appearance changes), which increases the average error even if most observations are accurate.\n",
        "\n",
        "2. **Prominent features are harder to localize precisely**: Points visible from many viewpoints are typically located on salient features (corners, edges, high-contrast regions). These features may exhibit viewpoint-dependent appearance variations that make sub-pixel localization less consistent across views.\n",
        "\n",
        "3. **Bundle adjustment trade-offs**: During optimization, the solver must find a 3D position that minimizes the total reprojection error across all observations. With more observations, finding a single 3D point that perfectly satisfies all views becomes harder, leading to slightly higher average residuals.\n",
        "\n",
        "Despite this trend, the overall reprojection errors remain low (mean: **0.64 pixels**, median: **0.54 pixels**), indicating a high-quality reconstruction.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe177d5-893b-41b4-aa1c-6d6fb06b54b7",
      "metadata": {},
      "source": [
        "## 2.3 Plot the 3D points that correspond to a keypoint in the first image. Also plot the image with the keypoints (1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1888156a-c5f5-4695-a75b-2883f1a7c8b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "xyz = []\n",
        "colors = []\n",
        "\n",
        "# The images are not sorted as in the folder\n",
        "first_image_id = None\n",
        "for id, image in images.items():\n",
        "    if image.name == \"IMG_2331.JPG\":\n",
        "        first_image_id = id\n",
        "\n",
        "for point3D in points3D.values():\n",
        "    if first_image_id not in point3D.image_ids:\n",
        "        continue\n",
        "    xyz.append(point3D.xyz)\n",
        "    colors.append(point3D.rgb / 255)\n",
        "\n",
        "vis = open3d.visualization.Visualizer()\n",
        "vis.create_window()\n",
        "\n",
        "pcd = open3d.geometry.PointCloud()\n",
        "\n",
        "pcd.points = open3d.utility.Vector3dVector(xyz)\n",
        "pcd.colors = open3d.utility.Vector3dVector(colors)\n",
        "\n",
        "vis.add_geometry(pcd)\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "vis.run()\n",
        "vis.destroy_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40c16d16",
      "metadata": {},
      "source": [
        "As can be seen in the visualization, the 3D points follow the shape of the wall visible on the first image.\n",
        "\n",
        "<img src=\"figures/gerrard_hall_image1_points3d.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea7ff3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "image1_keypoints = keypoints[first_image_id]\n",
        "\n",
        "cv_keypoints = [cv2.KeyPoint(x=float(pt[0]), y=float(pt[1]), size=50) \n",
        "                for pt in image1_keypoints]\n",
        "\n",
        "\n",
        "images_path = \"./gerrard-hall/images/\"\n",
        "\n",
        "print(type(image1_keypoints))\n",
        "print(image1_keypoints.shape)\n",
        "print(image1_keypoints.dtype)\n",
        "\n",
        "print(type(images[1]))\n",
        "\n",
        "im1 = cv2.imread(images_path + images[first_image_id].name)\n",
        "print(type(im1))\n",
        "im_with_kpts = cv2.drawKeypoints(im1, cv_keypoints, None, \n",
        "                                   color=(255, 0, 255),\n",
        "                                   flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "plt.imshow(cv2.cvtColor(im_with_kpts, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e117ea99-bc47-41f6-8b14-a42213ee0482",
      "metadata": {},
      "source": [
        "## 2.4 Create a visualization for the number of matches between all images. (1.0)\n",
        "For example: https://seaborn.pydata.org/generated/seaborn.heatmap.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13fba504-a871-4287-9833-46aa1c883637",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create mapping from image_id to sorted index (by filename)\n",
        "id_name_pairs = [(img_id, img.name) for img_id, img in images.items()]\n",
        "id_name_pairs_sorted = sorted(id_name_pairs, key=lambda x: x[1])  # Sort by filename\n",
        "id_to_sorted_idx = {img_id: idx for idx, (img_id, name) in enumerate(id_name_pairs_sorted)}\n",
        "\n",
        "# Build match matrix using sorted indices\n",
        "n_images = len(images)\n",
        "match_matrix = np.zeros((n_images, n_images), dtype=np.int32)\n",
        "for (id1, id2), match_data in matches.items():\n",
        "    i = id_to_sorted_idx[id1]\n",
        "    j = id_to_sorted_idx[id2]\n",
        "    num_matches = len(match_data)\n",
        "    match_matrix[i, j] = num_matches\n",
        "    match_matrix[j, i] = num_matches\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(match_matrix, xticklabels=10, yticklabels=10)\n",
        "plt.xlabel('Image Index (sorted by filename)')\n",
        "plt.ylabel('Image Index (sorted by filename)')\n",
        "plt.title('Number of matches between images')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2pknijhsfv7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Check if image IDs correspond to filename order\n",
        "print(\"Image ID -> Filename mapping:\")\n",
        "id_name_pairs = [(img_id, img) for img_id, img in images.items()]\n",
        "id_name_pairs_sorted_by_id = sorted(id_name_pairs, key=lambda x: x[0])\n",
        "id_name_pairs_sorted_by_name = sorted(id_name_pairs, key=lambda x: x[1])\n",
        "\n",
        "print(\"\\nSorted by image_id:\")\n",
        "for img_id, name in id_name_pairs_sorted_by_id[:10]:\n",
        "    print(f\"  ID {img_id}: {name}\")\n",
        "print(\"  ...\")\n",
        "\n",
        "print(\"\\nSorted by filename:\")\n",
        "for img_id, name in id_name_pairs_sorted_by_name[:10]:\n",
        "    print(f\"  ID {img_id}: {name}\")\n",
        "print(\"  ...\")\n",
        "\n",
        "# Check if the orderings match\n",
        "ids_by_id = [x[0] for x in id_name_pairs_sorted_by_id]\n",
        "ids_by_name = [x[0] for x in id_name_pairs_sorted_by_name]\n",
        "print(f\"\\nDo image IDs match filename order? {ids_by_id == ids_by_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc84087",
      "metadata": {},
      "source": [
        "Matches are concentrated around each image. An image won't have matches with images too far apart, as they represent different parts of the scene - images far appart may show different facades of the building. The fact that the corners of the heatmap are not 0 indicates that the last few images match with the first ones, indicating a somewhat closed track."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab06d62-e966-439e-87d7-5d7ba973bca1",
      "metadata": {},
      "source": [
        "## 2.5 Visualize the keypoints and matches between the two images used in lab 3 using Colmap, how it compares to the results from lab 3? (1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36969388",
      "metadata": {},
      "source": [
        "> **Note:**  \n",
        "> Before proceeding, we clarify the choice of camera intrinsics used in Sections 2.5, 2.6, and 2.7. It was not clear whether to use the intrinsic matrix $K$ estimated in Lab 3 or to let COLMAP estimate the intrinsics again.\n",
        "> \n",
        "> We chose to let COLMAP estimate the camera intrinsics automatically. In particular, we used the `SIMPLE_RADIAL` camera model, which assumes a single focal length ($f_x = f_y$) and one radial distortion parameter. We also tested the `PINHOLE` model, since in Lab 3 $f_x$ and $f_y$ were different, but it did not produce better results. Therefore, we used the `SIMPLE_RADIAL` intrinsics for the following experiments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c883bb90",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'> You can use the GUI to get the keypoints and matches and then visualize it here, following the same style as in lab 3 to get comparable results. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50ea0b73",
      "metadata": {},
      "source": [
        "As shown in **Figure 6**, for this section, we will use the images `0001_s.png` and `0002_s.png`. \n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/0001_s.png\" width=\"85%\">\n",
        "    <p><b>(a)</b> <code> 0001_s.png </code> </p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/0002_s.png\" width=\"85%\">\n",
        "    <p><b>(b)</b> <code> 0002_s.png </code> </p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\">\n",
        "  <b>Figure 6:</b> Input image pair used for keypoint detection and matching experiments: \n",
        "  (a) <code>0001_s.png</code> and (b) <code>0002_s.png</code>.\n",
        "</p>\n",
        "\n",
        "In Lab 3, the keypoints and correspondences were obtained using a classical pipeline: keypoint detection with ORB, brute-force matching, and outlier rejection using RANSAC.\n",
        "\n",
        "Now, instead, we use COLMAP to compute the keypoints and matches. COLMAP employs more advanced and robust feature detection and description methods, such as SIFT, as well as more reliable matching strategies (e.g., exhaustive, sequential, or spatial matching), which typically provide more accurate and consistent correspondences (we will use the default strategy: `sequential`).\n",
        "\n",
        "To do this, we performed an automatic reconstruction in COLMAP using the two images `0001_s.png` and `0002_s.png`, and we follow the steps described in Section 2.1 to obtain the keypoints and matches.\n",
        "\n",
        "It is important to note that the matches stored in the COLMAP database (`matches` table) correspond to **raw descriptor-based correspondences**, which have **not yet undergone geometric verification**. The geometric consistency of these matches is evaluated in a later stage, during reconstruction (`two_view_geometries` table). \n",
        "\n",
        "The same procedure can also be carried out manually by running `Processing > Feature Extraction` (selecting the camera model) and then `Processing > Feature Matching`, choosing the desired matching strategy. However, by using the automatic reconstruction, all the required data are already prepared and can be directly reused in Sections 2.6 and 2.7.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f50cc03b-672c-491c-bd61-e40bcc757f43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# reconstruction_path = \"./castle_dense/sparse/0\"\n",
        "# database_path = \"./castle_dense/database.db\"\n",
        "\n",
        "reconstruction_path = \"./castle/sparse/0\"\n",
        "database_path = \"./castle/database.db\"\n",
        "\n",
        "model = Model()\n",
        "model.read_model(reconstruction_path, ext='.bin') # Should also work with .txt\n",
        "\n",
        "images = model.images\n",
        "cameras = model.cameras\n",
        "points3D = model.points3D\n",
        "\n",
        "print(f\"Loaded {len(images)} images. This is the information available for one of them:\")\n",
        "print(images[1])\n",
        "print(f\"\\nLoaded {len(cameras)} cameras. This is the information available for one of them:\")\n",
        "print(cameras[1])\n",
        "print(f\"\\nLoaded {len(points3D)} 3D points. This is the information available for one of them:\")\n",
        "print(points3D[1]) \n",
        "\n",
        "db = sqlite3.connect(database_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec45c7c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "image_id_to_name = dict(\n",
        "    (image_id, name)\n",
        "    for image_id, name in db.execute(\n",
        "        \"SELECT image_id, name FROM images\")\n",
        ")\n",
        "\n",
        "# Shifted ids!!!\n",
        "print(image_id_to_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c06ffc",
      "metadata": {},
      "source": [
        "<span style=\"color:red; font-weight:bold;\">Attention:</span>\n",
        "\n",
        "In COLMAP, keypoints are stored in the database as a binary BLOB with shape \n",
        "<strong>(rows × cols)</strong>, where <code>rows</code> is the number of keypoints and \n",
        "<code>cols</code> is the number of values per keypoint.\n",
        "\n",
        "According to the COLMAP documentation:\n",
        "    \"*If the keypoints have 4 columns, the feature\n",
        "    geometry is a similarity and the third and fourth columns correspond to the scale and\n",
        "    orientation of the feature, following SIFT conventions. If the keypoints have 6 columns,\n",
        "    the feature geometry is an affinity and the last four columns encode the affine shape\n",
        "    of the feature.*\"\n",
        "\n",
        "In our case, <code>cols = 6</code>. Therefore, using <code>reshape(-1, 6)</code> reconstructs each\n",
        "keypoint correctly as a row with six values. Since only the 2D image coordinates are\n",
        "required for visualization and matching, we keep only the first two columns\n",
        "<code>(x, y)</code> and discard the remaining attributes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7304cf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load keypoints with proper reshaping based on actual cols per image\n",
        "# Each image may have different cols (4, or 6), so we query all fields together\n",
        "keypoints = dict()\n",
        "for image_id, data, rows, cols in db.execute(\n",
        "        \"SELECT image_id, data, rows, cols FROM keypoints\"):\n",
        "    # Reshape to actual (rows, cols) and keep only (x, y) coordinates\n",
        "    kpts = blob_to_array(data, np.float32, (rows, cols))[:, :2]\n",
        "    keypoints[image_id] = kpts\n",
        "\n",
        "# Check what column counts were found\n",
        "unique_cols = set()\n",
        "for image_id, data, rows, cols in db.execute(\n",
        "        \"SELECT image_id, data, rows, cols FROM keypoints\"):\n",
        "    unique_cols.add(cols)\n",
        "    \n",
        "print(f\"Keypoint column counts in database: {unique_cols}\")\n",
        "print(f\"Loaded keypoints from {len(keypoints)} images\")\n",
        "print(f\"Total keypoints: {sum(kpts.shape[0] for kpts in keypoints.values())}\")\n",
        "print(f\"These are the {len(keypoints[2])} keypoints in the first image and {len(keypoints[1])} keypoints in the second image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f37bcb",
      "metadata": {},
      "outputs": [],
      "source": [
        "image1 = cv2.imread('./castle/images/0001_s.png')\n",
        "image2 = cv2.imread('./castle/images/0002_s.png')\n",
        "\n",
        "# Convert keypoints to cv2.KeyPoint format\n",
        "kp1 = [cv2.KeyPoint(x=pt[0], y=pt[1], size=1) for pt in keypoints[2]] \n",
        "kp2 = [cv2.KeyPoint(x=pt[0], y=pt[1], size=1) for pt in keypoints[1]]\n",
        "\n",
        "def show_keypoints(image, cv_keypoints, title=\"Keypoints\"):\n",
        "    im_with_kpts = cv2.drawKeypoints(\n",
        "        image, \n",
        "        cv_keypoints, \n",
        "        None,\n",
        "        color=(255, 0, 255),\n",
        "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
        "    )\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(cv2.cvtColor(im_with_kpts, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "show_keypoints(image1, kp1, title=\"Keypoints Image 1\")\n",
        "show_keypoints(image2, kp2, title=\"Keypoints Image 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76fcc6bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "matches = dict()\n",
        "count_no_data = 0\n",
        "for pair_id, data in db.execute(\"SELECT pair_id, data FROM matches\"):\n",
        "    if data is None:\n",
        "        count_no_data += 1\n",
        "    else:\n",
        "        matches[pair_id_to_image_ids(pair_id)] = blob_to_array(data, np.uint32, (-1, 2))\n",
        "print(f\"Loaded {len(matches)} match. \")\n",
        "print(f\"There are {len(matches[1,2])} matches between the two images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db07d78",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"matches[(1,2)] shape: {matches[(1,2)].shape}\")\n",
        "print(f\"matches[(1,2)] dtype: {matches[(1,2)].dtype}\")\n",
        "print(f\"First 5 matches: {matches[(1,2)][:5]}\")\n",
        "print(f\"Max values: {matches[(1,2)].max(axis=0)}\")\n",
        "print(f\"len(keypoints[1]): {len(keypoints[1])}\")\n",
        "print(f\"len(keypoints[2]): {len(keypoints[2])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5238c735",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert matches to cv2.DMatch format\n",
        "cv_matches = []\n",
        "for match in matches[(1, 2)]:\n",
        "    query_idx = int(match[1])  \n",
        "    train_idx = int(match[0]) \n",
        "    cv_matches.append(cv2.DMatch(_queryIdx=query_idx, _trainIdx=train_idx, _distance=0))\n",
        "\n",
        "# Draw matches\n",
        "img_matches = cv2.drawMatches(\n",
        "    cv2.cvtColor(image1, cv2.COLOR_BGR2RGB), kp1, \n",
        "    cv2.cvtColor(image2, cv2.COLOR_BGR2RGB), kp2,\n",
        "    cv_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "# Display the matches\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(img_matches)\n",
        "plt.axis('off')\n",
        "plt.title('Matches between Image 1 and Image 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30abcf64",
      "metadata": {},
      "source": [
        "As we can see from the obtained results, the number of matches obtained with COLMAP is very high (**1571**), and most of them appear to be geometrically consistent. However, from a visual inspection alone, it is not possible to determine with certainty if all correspondences are true inliers. \n",
        "\n",
        "However, by randomly visualizing small subsets of the matches, we were able to identify some outliers (up to 6) that are not easily noticeable when all correspondences are displayed at once. This confirms that a purely visual inspection is not sufficient to reliably separate correct matches from incorrect ones when there are many matches obtained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba01a86",
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_match_sample(img1, kp1, img2, kp2, matches, n=10, title=\"\"):\n",
        "    if len(matches) > n:\n",
        "        matches = random.sample(matches, n)\n",
        "    vis = cv2.drawMatches(\n",
        "        cv2.cvtColor(img1, cv2.COLOR_BGR2RGB), kp1,\n",
        "        cv2.cvtColor(img2, cv2.COLOR_BGR2RGB), kp2,\n",
        "        matches, None,\n",
        "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
        "    )\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    plt.imshow(vis)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "draw_match_sample(image1, kp1, image2, kp2, cv_matches, n=50, title=\"Random Matches\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6662d192",
      "metadata": {},
      "source": [
        "If we compare these results with those obtained in Lab 3, we can observe some differences.\n",
        "\n",
        "In Lab 3, we obtained **1290** raw matches before applying RANSAC, which were reduced to **650** inliers after geometric verification. This clearly shows that a significant number of outliers were present and that applying RANSAC was essential to remove incorrect correspondences. The effect of this filtering step can be clearly observed in **Figure 7**.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/lab3_matches_before_ransac.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Matches before RANSAC (1290).</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/lab3_matches_after_ransac.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Matches after RANSAC (650).</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 7:</b> Comparison of Lab 3 results before and after applying RANSAC. \n",
        "  A large number of incorrect correspondences are removed after geometric verification, \n",
        "  leading to a more consistent set of matches.\n",
        "</p>\n",
        "\n",
        "ORB is a simpler and faster descriptor, and it generally detects fewer keypoints compared to SIFT. As a result, the total number of matches is lower. However, after RANSAC, the remaining correspondences appear to be more geometrically consistent (*although, as mentioned before, a purely visual inspection is not sufficient to guarantee that all matches are true inliers; by randomly sampling small subsets, we were only able to identify at most **two** remaining outliers*). \n",
        "\n",
        "Overall, this comparison highlights an important trade-off between both approaches. Simpler methods such as ORB may produce fewer correspondences, but these can be highly consistent after geometric filtering. On the other hand, more advanced pipelines tend to generate a much denser set of matches, which is beneficial for tasks such as 3D reconstruction, even if some outliers are introduced.\n",
        "\n",
        "In the case of COLMAP, the matches shown at this stage have **not yet passed any geometric verification**. These are raw descriptor-based matches stored in the database, and some outliers are still present. Geometric verification is applied later during the reconstruction process, where inconsistent matches are filtered out and only geometrically valid correspondences are used to create the 3D points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "244a4a42-5c40-4983-af8a-2c0ef286ce5d",
      "metadata": {},
      "source": [
        "## 2.6 Triangulate and visualize the 3D points from the keypoints extracted using Colmap on the two images used in lab 3, how it compares to the results from lab 3? (1.0) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27dc65de",
      "metadata": {},
      "source": [
        "In this section, we triangulate 3D points from the two images used in Lab 3 (`0001_s.png` and `0002_s.png`) by reusing the triangulation pipeline developed in that lab, but replacing the keypoints and matches with those obtained from COLMAP.\n",
        "\n",
        "We use the camera intrinsics and extrinsics estimated by COLMAP during the automatic reconstruction to build the projection matrices for both images. We will compare these intrinsics and camera poses with those estimated in Lab 3. Using the resulting projection matrices and the 2D–2D correspondences from the COLMAP matches, we triangulate the 3D points with the same DLT-based method used in Lab 3.\n",
        "\n",
        "The resulting 3D point cloud is then visualized following the same style as in Lab 3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e805e938",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'> - Use the triangulation from lab 3 to the get the 3D points and visualize them following the same style. </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9c9b69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters for COLMAP\n",
        "img1_id, img2_id = 2, 1  # Shifted ids\n",
        "\n",
        "# Get the images\n",
        "img1 = images[img1_id]\n",
        "img2 = images[img2_id]\n",
        "\n",
        "# Get the camera (shared intrinsics)\n",
        "camera = cameras[img1.camera_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c476baa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison between Lab 3 camera matrices and COLMAP estimated intrinsic matrices\n",
        "\n",
        "# Lab 3 K matrix\n",
        "K_3 = np.array([[708.636, 0,  456.207],\n",
        "              [0, 709.836,  302.043],\n",
        "              [0, 0,  1]], dtype=float)\n",
        "print(\"\\nIntrinsic matrix K from Lab 3:\")\n",
        "print(K_3)\n",
        "\n",
        "# COLMAP estimated K matrix\n",
        "f, cx, cy, k = camera.params\n",
        "K = np.array([[f, 0,  cx],\n",
        "              [0, f,  cy],\n",
        "              [0, 0,  1]], dtype=float)\n",
        "print(\"\\nIntrinsic matrix estimated by COLMAP::\")\n",
        "print(K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e898272d",
      "metadata": {},
      "source": [
        "In Lab 3 we obtained $f_x = 708.64$ and $f_y = 709.84$, while COLMAP estimates a single focal length $f_x = f_y = 810.46$. This corresponds to an increase of approximately **+101.8 px** with respect to $f_x$ and **+100.6 px** with respect to $f_y$, which is around **14% larger**.\n",
        "\n",
        "For the principal point, Lab 3 gives $(c_x, c_y) = (456.21, 302.04)$, whereas COLMAP estimates $(c_x, c_y) = (461.00, 307.00)$ . This represents a shift of about **(+4.8 px, +5.0 px)**, which is relatively small compared to the image resolution.\n",
        "\n",
        "Overall, COLMAP recovers a principal point very close to the one estimated in Lab 3, and a focal length of the same order of magnitude but a bit larger. This difference is expected given the simplified camera model (`SIMPLE_RADIAL`) and the fact that the estimation is performed using only two images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9a7885",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison between Lab 3 camera matrices and COLMAP estimated camera matrices\n",
        "\n",
        "P1_3 = np.array([[708.636,   0.   , 456.207,   0.   ],\n",
        "               [  0.   , 709.836, 302.043,   0.   ],\n",
        "               [  0.   ,   0.   ,   1.   ,   0.   ]], dtype=float)\n",
        "\n",
        "P2_3 = np.array([[ 6.224e+02,  3.430e+01,  5.672e+02, -5.331e+02],\n",
        "                [-8.658e+01,  7.075e+02,  2.951e+02,  1.645e+02],      \n",
        "                [-1.655e-01, -4.747e-03,  9.862e-01,  3.030e-01]], dtype=float)\n",
        "\n",
        "print(\"\\nProjection matrices from Lab 3:\")\n",
        "print(\"P1: \\n\", P1_3)\n",
        "\n",
        "print(\"P2: \\n\", P2_3)\n",
        "\n",
        "# Projection matrices\n",
        "def P_from_image(img, K):\n",
        "    R = img.qvec2rotmat()\n",
        "    t = img.tvec.reshape(3, 1)\n",
        "    return K @ np.hstack([R, t])\n",
        "\n",
        "P1 = P_from_image(img1, K)\n",
        "P2 = P_from_image(img2, K)\n",
        "\n",
        "print(\"\\nProjection matrices from COLMAP:\")\n",
        "print(\"P1: \\n\", P1)\n",
        "print(\"P2: \\n\", P2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dc5272",
      "metadata": {},
      "source": [
        "The numerical values clearly reflect the different choices of reference frame used in each case.\n",
        "\n",
        "In Lab 3, the first camera matrix is exactly of the form $P_1 = K[I \\mid 0]$, with zero translation and an identity rotation, confirming that the world coordinate system was explicitly fixed at the first camera. Consequently, the second camera matrix $P_2$ encodes a relative rotation and translation with respect to this canonical reference.\n",
        "\n",
        "In contrast, the projection matrices estimated by COLMAP show non-zero translation components and non-identity rotations already in $P_1$. This indicates that COLMAP does not fix any camera to a canonical pose, but instead estimates all camera poses jointly in a global coordinate system. The large translation values observed in both $P_1$ and $P_2$ are therefore expected and simply reflect the arbitrary global reference frame chosen by COLMAP during optimization.\n",
        "\n",
        "Despite these differences in absolute values, the rotation blocks of the matrices are well-formed and consistent, and the relative geometry between the two cameras is preserved. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81dd9718",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lab3 code for triangulation\n",
        "def image_normalization_matrix(imsize):\n",
        "    #Normalization matrix based on image size, maps image coordinates to [-1,1] range\n",
        "    w, h = imsize\n",
        "    return np.array([\n",
        "        [2 / w,     0,     -1], #Scale x-coordinates to [-1, 1]\n",
        "        [0,     2 / h,     -1], #Scale y-coordinates to [-1, 1]\n",
        "        [0,         0,      1]\n",
        "    ])\n",
        "\n",
        "def hartley_normalization_matrix(x: np.ndarray) -> np.ndarray:\n",
        "    #Normalization matrix based on Hartley normalization\n",
        "    x = x.copy()\n",
        "    x = x / x[2:3, :]                #Homogeneous to Euclidean coordinates\n",
        "    pts = x[:2, :]                   #2D image coordinates\n",
        "\n",
        "    c = np.mean(pts, axis=1)         #Centroid\n",
        "    pts_c = pts - c[:, None]         #Translate points to centroid\n",
        "\n",
        "    d = np.sqrt(np.sum(pts_c**2, axis=0)) #Euclidean distance to centroid\n",
        "    mean_d = np.mean(d)                   #Mean distance from the centroid\n",
        "\n",
        "    s = np.sqrt(2) / (mean_d + 1e-12) #Scale to get mean distance equals sqrt(2)\n",
        "\n",
        "    T = np.array([\n",
        "        [s, 0, -s * c[0]],\n",
        "        [0, s, -s * c[1]],\n",
        "        [0, 0, 1]\n",
        "    ], dtype=float)         #Norm Matrix\n",
        "    return T\n",
        "\n",
        "def triangulate(x1, x2, P1, P2, imsize, norm='hartley') -> np.ndarray:\n",
        "    #Triangulate 3D points from two views using the DLT method\n",
        "    assert P1.shape == (3,4) == P2.shape\n",
        "    assert x1.shape == x2.shape and x1.shape[0] == 3\n",
        "\n",
        "    #Apply normalization to improve numerical stability\n",
        "    if norm == 'image_size':\n",
        "        T = image_normalization_matrix(imsize)\n",
        "        P1n = T @ P1\n",
        "        P2n = T @ P2\n",
        "        x1n = T @ x1\n",
        "        x2n = T @ x2\n",
        "    elif norm == 'hartley':\n",
        "        T1 = hartley_normalization_matrix(x1)\n",
        "        T2 = hartley_normalization_matrix(x2)\n",
        "        P1n = T1 @ P1\n",
        "        P2n = T2 @ P2\n",
        "        x1n = T1 @ x1\n",
        "        x2n = T2 @ x2\n",
        "    elif norm is None:\n",
        "        P1n, P2n, x1n, x2n = P1, P2, x1, x2\n",
        "    else:\n",
        "        raise ValueError(\"norm must be None, 'image_size' or 'hartley'\")\n",
        "\n",
        "    def generate_Ai(x, P):\n",
        "        #Construct the DLT constraint matrix for a single correspondenc\n",
        "        x = x.copy()\n",
        "        x = x / x[2] #Dehomogenize image point\n",
        "        u, v = x[0], x[1]\n",
        "        return np.vstack([\n",
        "            v * P[2,:] - P[1,:],\n",
        "            P[0,:] - u * P[2,:]\n",
        "        ])\n",
        "\n",
        "    N = x1.shape[1]\n",
        "    X = np.zeros((4, N), dtype=float) #Homogeneous 3D points\n",
        "\n",
        "    for i in range(N):\n",
        "        #Build the full DLT system by stacking constraints from both views\n",
        "        A = np.vstack([\n",
        "            generate_Ai(x1n[:, i], P1n),\n",
        "            generate_Ai(x2n[:, i], P2n)\n",
        "        ])\n",
        "\n",
        "        #Solve A X = 0 using SVD\n",
        "        _, _, Vt = np.linalg.svd(A, full_matrices=False)\n",
        "        Xi = Vt[-1, :] #Solution associated with smallest singular value\n",
        "\n",
        "        #Normalize homogeneous coordinates\n",
        "        if abs(Xi[3]) > 1e-12:\n",
        "            Xi = Xi / Xi[3]\n",
        "\n",
        "        X[:, i] = Xi\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f479aff",
      "metadata": {},
      "outputs": [],
      "source": [
        "m = matches[(1, 2)]   # shape (M,2)\n",
        "\n",
        "idx1 = m[:, 1]        # indices in image 1 keypoints\n",
        "idx2 = m[:, 0]        # indices in image 2 keypoints\n",
        "\n",
        "x1 = keypoints[img1_id][idx1].T  # shape (2,M)\n",
        "x2 = keypoints[img2_id][idx2].T  # shape (2,M)\n",
        "\n",
        "x1 = np.vstack([x1, np.ones((1, x1.shape[1]))]) # shape (3,M)\n",
        "x2 = np.vstack([x2, np.ones((1, x2.shape[1]))]) # shape (3,M)\n",
        "\n",
        "# Color\n",
        "x_img = np.transpose(x1[:2]).astype(int)\n",
        "rgb_txt = (image1[x_img[:,1], x_img[:,0]])/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6de078f",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_pred = triangulate(x1, x2, P1, P2, [camera.width, camera.height])\n",
        "X_eucl = X_pred/X_pred[-1,:]\n",
        "points3d = X_eucl[:3].T\n",
        "fig = go.Figure(layout=dict(height=400, width=550, title=\"Triangulated 3D points (COLMAP)\"))\n",
        "points3d = X_eucl[:3].T\n",
        "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color=rgb_txt, size=2)))\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1713046b",
      "metadata": {},
      "source": [
        "When comparing the Lab 3 reconstruction with the Lab 4 result obtained using COLMAP camera parameters, ... <span style=\"color: red;\">NO SE QUE DECIR EN LA COMPARACIÓN</span>\n",
        "\n",
        "In **Figure 8**, the 3D point cloud obtained in Lab 3 is shown. This reconstruction was computed using the same triangulation pipeline but with camera matrices estimated within the lab framework and a reduced set of feature correspondences.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\"> \n",
        "    <div style=\"text-align: center;\"> \n",
        "    <img src=\"figures/triangulation_lab3.png\" style=\"width: 45%;\"> \n",
        "    </div> \n",
        "</div> \n",
        "<p align=\"center\"><b>Figure 8:</b> Triangulated 3D point cloud obtained in Lab 3 using the estimated camera matrices and feature matches from Lab 3.</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10eacbca-842d-4d9e-85bd-efc35c5dd15a",
      "metadata": {},
      "source": [
        "## 2.7 Visualize the sparse reconstruction using the 2 images from lab 3, and the complete CASTLE dataset. Comment on the differences between techniques and number of images used. (1.0)\n",
        "#### <span style='color:Green'> - Use the reconstruction from Colmap to the get the 3D points and visualize them following the same style, using two images and the complete dataset. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b076a4",
      "metadata": {},
      "source": [
        "# Using 2 images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9504a3-dcbf-4b2b-9ff6-71c5a4584742",
      "metadata": {},
      "outputs": [],
      "source": [
        "reconstruction_path = \"./castle/sparse/0\"\n",
        "database_path = \"./castle/database.db\"\n",
        "\n",
        "model = Model()\n",
        "model.read_model(reconstruction_path, ext='.bin') # Should also work with .txt\n",
        "\n",
        "model.create_window()\n",
        "model.add_points(min_track_len=2) # Filter points seen in at least 2 images\n",
        "model.add_cameras(scale=2)\n",
        "model.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5cc414d",
      "metadata": {},
      "source": [
        "# Using 30 images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb0af21",
      "metadata": {},
      "outputs": [],
      "source": [
        "reconstruction_path = \"./castle_P30/sparse/0\"\n",
        "database_path = \"./castle_P30/database.db\"\n",
        "\n",
        "model = Model()\n",
        "model.read_model(reconstruction_path, ext='.bin') # Should also work with .txt\n",
        "\n",
        "model.create_window()\n",
        "model.add_points() \n",
        "model.add_cameras(scale=0.25)\n",
        "model.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "601a6299",
      "metadata": {},
      "source": [
        "**Figure 9** compares the sparse 3D reconstructions obtained.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\"> \n",
        "    <div style=\"text-align: center; width: 45%\"> \n",
        "        <img src=\"figures/castle_2_images.png\", style=\"width: 95%;\"> \n",
        "        <p><b>(a)</b> Reconstruction with 2 images</p> \n",
        "    </div> \n",
        "    <div \n",
        "    style=\"text-align: center; width: 45%\"> \n",
        "        <img src=\"figures/castle_30_images.png\", style=\"width: 85%;\"> \n",
        "        <p><b>(b)</b> Reconstruction with 30 images</p> \n",
        "    </div> \n",
        "</div> <p align=\"center\"><b>Figure 9:</b> Comparison of sparse COLMAP reconstructions using two images and thirty images.</p>\n",
        "\n",
        "With only 2 images, the reconstructed point cloud appears sparse and irregular, with a limited number of points forming an incomplete structure and several isolated outliers. The geometry lacks continuity and clear surface definition, reflecting the reduced amount of visual information available. Additionally, the camera poses are located far from the reconstructed points; in order to make the cameras visible in the visualization, the camera scale had to be increased (`scale = 2`), as they were otherwise too small to be observed.\n",
        "\n",
        "With 30 images, the reconstructed point cloud is significantly denser and more continuous, forming well-defined surfaces and structures with far fewer isolated points. The scene geometry appears more complete and coherent, and the presence of multiple camera poses distributed around the scene is clearly visible. In contrast to the 2-image case, the cameras are naturally well-scaled with respect to the point cloud and can be clearly observed without requiring manual scale adjustments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32408947",
      "metadata": {},
      "source": [
        "# 3. Configure the reconstruction to improve the results. (1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89648246",
      "metadata": {},
      "source": [
        "To improve the reconstruction quality in the _castle_ dataset we created a custom COLMAP script (`colmap_improved.sh`) with tuned parameters. Below we describe the key parameter changes we have set and explain how they help improve reconstruction quality.\n",
        "\n",
        "The original castle reconstruction contained **missing parts and holes** in walls and the floor. These areas have relatively uniform appearance or very high-frequency texture patterns, which makes them challenging for feature-based reconstruction methods.\n",
        "\n",
        "In the following image (ref Figure!!) the mentioned missing parts and holes can be observed.\n",
        "\n",
        "<img src=\"figures/castle_dense_base.png\"/>\n",
        "\n",
        "## COLMAP Pipeline Overview\n",
        "\n",
        "COLMAP follows a sequential pipeline where each step builds upon the previous one. Understanding this pipeline helps us identify where improvements can be made:\n",
        "\n",
        "1. **Feature Extraction** → Detect and describe keypoints in each image\n",
        "2. **Feature Matching** → Find correspondences between image pairs\n",
        "3. **Sparse Reconstruction** → Estimate camera poses and triangulate a sparse 3D point cloud\n",
        "4. **Dense Stereo** → Compute dense depth maps for each image\n",
        "5. **Depth Fusion** → Merge depth maps into a consistent dense point cloud\n",
        "6. **Meshing** → Convert the point cloud into a triangular mesh\n",
        "\n",
        "## Parameter Analysis by Pipeline Stage\n",
        "\n",
        "### 1. Feature Extraction\n",
        "\n",
        "We hypothesized that the problematic low-texture areas might not have enough keypoints detected, leading to sparse or missing matches. SIFT (Scale-Invariant Feature Transform) detects keypoints by finding local extrema in a scale-space pyramid built from Difference-of-Gaussians (DoG) images.\n",
        "\n",
        "To increase keypoint detection in subtle texture regions, we modified:\n",
        "\n",
        "- **`num_octaves`: 4 → 5** — SIFT builds a pyramid of progressively downsampled images (octaves). Adding one more octave allows detection of **larger-scale features** that might be present in uniform areas where fine details are absent.\n",
        "\n",
        "- **`octave_resolution`: 3 → 4** — Within each octave, SIFT computes multiple DoG images at different scales. Increasing the resolution provides **finer scale sampling**, which can detect features at intermediate scales that would otherwise be missed.\n",
        "\n",
        "- **`peak_threshold`: 0.0067 → 0.006** — This threshold filters out keypoints with low contrast in the DoG response. Lowering it makes SIFT **more sensitive to subtle features**, accepting keypoints with weaker gradients that are common in low-texture regions.\n",
        "\n",
        "### 2. Feature Matching\n",
        "\n",
        "Even with more keypoints, low-texture areas often produce similar-looking descriptors that are hard to match uniquely. We enabled:\n",
        "\n",
        "- **`guided_matching`: 0 → 1** — After an initial set of matches is found, guided matching uses the estimated epipolar geometry to **search for additional correspondences** along epipolar lines. This is particularly useful for low-texture regions where descriptor-only matching might miss valid correspondences due to ambiguous descriptors.\n",
        "\n",
        "### 3. Sparse Reconstruction (Mapper)\n",
        "\n",
        "The mapper estimates camera poses through incremental Structure-from-Motion. We adjusted:\n",
        "\n",
        "- **`init_min_num_inliers`: 100 → 50** — The initial image pair needs sufficient inlier matches to bootstrap the reconstruction. Lowering this threshold allows initialization even when the best image pair has **fewer high-quality matches**, which can happen in challenging scenes.\n",
        "\n",
        "- **`ba_refine_principal_point`: 0 → 1** — By default, COLMAP assumes the principal point is at the image center. Enabling refinement allows bundle adjustment to **optimize the optical center position**, which can improve accuracy for cameras where the sensor is not perfectly centered.\n",
        "\n",
        "### 4. Dense Stereo (PatchMatch)\n",
        "\n",
        "This is arguably the most critical stage for our problem. PatchMatch Stereo computes dense depth by comparing image patches across views. For low-texture regions, the key insight is that **larger patches provide more context** for matching.\n",
        "\n",
        "- **`window_radius`: 5 → 7** — This changes the patch size from **11×11 to 15×15 pixels**. In uniform areas, a small patch may look identical at many depths (the aperture problem). A larger patch is more likely to include some distinctive texture at the boundaries, enabling more reliable depth estimation. This is the **most important change** for addressing holes in walls and floors.\n",
        "\n",
        "### 5. Depth Fusion\n",
        "\n",
        "The fusion step merges per-image depth maps into a single consistent point cloud. Points are only kept if they are observed consistently across multiple views.\n",
        "\n",
        "- **`min_num_pixels`: 5 → 3** — This threshold specifies the minimum number of images that must agree on a 3D point for it to be kept. Lowering it **retains more points**, including those in areas where fewer views provide reliable depth. This produces a denser point cloud at the cost of potentially more noise.\n",
        "\n",
        "### 6. Poisson Meshing\n",
        "\n",
        "Finally, Poisson surface reconstruction converts the oriented point cloud into a watertight mesh by solving a global optimization problem.\n",
        "\n",
        "- **`depth`: 13 → 10** — This controls the octree depth used for reconstruction. A lower depth produces a **smoother mesh** with less fine geometric detail but fewer artifacts from noise. This helps close small remaining gaps and produces a cleaner final result.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Our parameter tuning follows a logical chain of reasoning:\n",
        "\n",
        "1. **More keypoints** in low-texture areas (SIFT parameters) → more potential matches\n",
        "2. **Guided matching** → recover matches that descriptor-only matching misses\n",
        "3. **Relaxed initialization** → successfully start reconstruction even with fewer matches\n",
        "4. **Larger stereo patches** → reliable depth estimation despite uniform appearance\n",
        "5. **Lower fusion threshold** → retain more 3D points to fill gaps\n",
        "6. **Smoother meshing** → clean final surface with fewer artifacts\n",
        "\n",
        "## Post-processing\n",
        "Although the reconstruction quality was improved by adjusting the parameters described above, the result was still not perfect. For this reason, we applied a post-processing stage using MeshLab in order to further enhance the final reconstruction.\n",
        "\n",
        "Our objective was to combine the improved **Poisson mesh** with the **Delaunay mesh** to obtain a more complete surface and to fill the missing regions present in the reconstruction. \n",
        "\n",
        "* First, isolated regions were removed from both meshes using the Remove Isolated Pieces (w.r.t. Diameter) filter, and remaining artifacts were manually deleted. \n",
        "* Then, the Delaunay mesh was smoothed using the `HC Laplacian Smoothing filter` to obtain a smoother and more regular surface.\n",
        "* Afterwards, color information from the Poisson mesh was transferred to the Delaunay mesh using `Sampling > Vertex Attribute Transfer`.\n",
        "* Finally, both meshes were merged into a single model using `Filters > Mesh Layer > Flatten Visible Layers`.\n",
        "\n",
        "The resulting post-processed reconstruction is shown in **Figure 10**.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/castle_post-processed_1.png\" style=\"width: 85%;\">\n",
        "    <p><b>(a)</b> Front view of the post-processed reconstruction.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/castle_post-processed_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> Rear view of the post-processed reconstruction.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 10:</b> Post-processed reconstruction obtained by combining the Poisson and Delaunay meshes after artifact removal, smoothing, color transfer, and mesh merging.\n",
        "</p>\n",
        "\n",
        "\n",
        "You can download the meshes from here:\n",
        "\n",
        "* [Castle raw mesh](https://drive.google.com/file/d/1ej_pujs_XPFn0cvNjimg0Zy8eb8i7NTJ/view?usp=sharing)\n",
        "\n",
        "* [Castle configured mesh](https://drive.google.com/file/d/1pBHUrYt-E11S7HTl4ITvFyz_J9Gdu-sr/view?usp=sharing)\n",
        "\n",
        "* [Castle post-processed mesh](https://drive.google.com/file/d/19UBu3DBg2tbg9iIkrbqbvjosgrB8tcIx/view?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b353928f",
      "metadata": {},
      "source": [
        "# 4. Reconstruct a 3D mesh from images captured by you. (1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a38f53",
      "metadata": {},
      "source": [
        "Before taking our own photographs, we ensured that the camera focus and exposure were fixed. This was done to keep the camera parameters consistent across all images and to avoid changes in appearance that could negatively affect feature matching and reconstruction.\n",
        "\n",
        "## UPF Campus\n",
        "\n",
        "To reconstruct the UPF Campus, we captured the images using two different strategies in order to evaluate which one produced better reconstruction results.\n",
        "\n",
        "* In the **first strategy**, we stood approximately at the center of the campus and took photos while rotating around ourselves, capturing images in all directions with good overlap.\n",
        "\n",
        "* In the **second strategy**, we walked close to the campus walls and took photos of the opposite façades, moving around the campus in a roughly rectangular path.\n",
        "\n",
        "**Figure 11** shows the *sparse* reconstructions obtained with each acquisition strategy.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/approach_1.png\" style=\"width: 73%;\">\n",
        "    <p><b>(a)</b> Sparse reconstruction using the first strategy.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/approach_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> Sparse reconstruction using the second strategy.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 11:</b> Sparse reconstructions of the UPF Campus obtained with two different image acquisition strategies.\n",
        "</p>\n",
        "\n",
        "We did not initially expect the first strategy to produce a spherical-like reconstruction. However, after analyzing the result of the second strategy, this behavior became more understandable.\n",
        "\n",
        "Since the images in the first strategy were captured from a single location while rotating around the camera, the camera centers remain almost at the same position and only the viewing direction changes. As a result, the reconstructed points are distributed around the camera, forming a spherical structure.\n",
        "\n",
        "In contrast, in the second strategy the camera is physically moved along the campus perimeter. This produces a clearer camera trajectory and leads to a reconstruction that better reflects the actual spatial layout of the scene.\n",
        "\n",
        "Therefore, we selected the **second strategy** to perform the final reconstruction of the UPF Campus.\n",
        "\n",
        "After selecting the second strategy, we computed a *dense* reconstruction of the UPF Campus using COLMAP’s dense pipeline. The dense point cloud was then converted into a surface using the **Poisson** surface reconstruction method, producing a raw mesh of the scene.\n",
        "\n",
        "**Figure 12** shows the resulting raw dense reconstruction obtained with the Poisson mesher, visualized from two different viewpoints. This reconstruction captures the overall geometry of the campus but still contains noise and artifacts.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/upf_raw_1.png\" style=\"width: 85%;\">\n",
        "    <p><b>(a)</b> Raw dense reconstruction - view 1.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/upf_raw_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> Raw dense reconstruction - view 2.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 12:</b> Raw dense reconstruction of the UPF Campus generated using the Poisson surface reconstruction algorithm, visualized from two different viewpoints..\n",
        "</p>\n",
        "\n",
        "We also attempted to further configure and refine the reconstruction using the script described in Section 3. However, we did not observe significant improvements in the results, as the obtained raw reconstruction was already fairly complete and stable.\n",
        "\n",
        "For this reason, instead of modifying the reconstruction parameters, we decided to apply simple **post-processing** techniques in Meshlab to improve the visual quality of the model. We generated two final mesh versions, both cleaned by **manually removing artifacts and isolated regions**.\n",
        "\n",
        "The first mesh includes the **ground surface**, while the second one **removes the ground entirely**. The mesh without the ground allows for a clearer inspection of the reconstructed buildings and vertical structures, resulting in a cleaner and more visually readable model. In this version, the façades and main architectural elements are well reconstructed and easier to analyze.\n",
        "\n",
        "Nevertheless, the mesh including the ground is also interesting to analyze. Although the ground contains several holes, these gaps are consistent with the image acquisition process, since objects such as vegetation, street furniture, or pedestrians occluded parts of the scene during image capture. Despite these occlusions, the reconstruction manages to recover portions of bushes, lampposts, and some campus furniture.\n",
        "\n",
        "**Figure 13** shows a comparison between the two post-processed meshes, visualized from different viewpoints.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/upf_with_1.png\" style=\"width: 85%;\">\n",
        "    <p><b>(a)</b> Post-processed mesh with ground – view 1.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/upf_with_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> Post-processed mesh with ground – view 2.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/upf_without_1.png\" style=\"width: 85%;\">\n",
        "    <p><b>(c)</b> Post-processed mesh without ground – view 1.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/upf_without_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(d)</b> Post-processed mesh without ground – view 2.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 13:</b> Comparison of the post-processed UPF Campus meshes.\n",
        "  The top row shows the reconstruction including the ground, while the bottom row shows the mesh without the ground,\n",
        "  allowing a clearer visualization of the reconstructed buildings and structures.\n",
        "</p>\n",
        "\n",
        "You can download the meshes from here:\n",
        "\n",
        "* [UPF raw mesh](https://drive.google.com/file/d/1OFbEfYbgiofzWY8_JmnfisznnsQlpWDD/view?usp=sharing)\n",
        "\n",
        "* [UPF post-processed mesh with ground](https://drive.google.com/file/d/1s87CK6g1ttbGBUc9jiVhnS9jkwTLCfZk/view?usp=sharing)\n",
        "\n",
        "* [UPF post-processed mesh without ground](https://drive.google.com/file/d/1u4bjIr_1-OkfeCK_zTe6JBV16_2TvTI-/view?usp=sharing)\n",
        "\n",
        "(*Note: We also attempted to further improve the reconstruction by following the post-processing steps described in Section 3, including the combination of the Poisson and Delaunay meshes to fill missing regions. However, in our case, the raw dense reconstruction obtained with the Poisson mesher was already fairly complete, and merging it with the Delaunay mesh did not result in a cleaner visualization. For this reason, we decided to keep only the Poisson-based mesh, as it provided a more visually clean and consistent reconstruction.*)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76ce9497",
      "metadata": {},
      "source": [
        "## Person\n",
        "\n",
        "We attempted to reconstruct the upper body of a person without expecting a highly accurate result, as previous attempts with simpler objects such as bottles and notebooks did not lead to satisfactory 3D reconstructions. Despite these low expectations, the result was **surprisingly good**: although the reconstruction is not complete and the back part of the body is not properly recovered, the front region is clearly and consistently reconstructed, capturing the overall shape of the person to a convincing degree.\n",
        "\n",
        "The reconstruction was generated using COLMAP’s Automatic Reconstruction pipeline, followed by surface generation with the Poisson mesher. After removing isolated regions using MeshLab, the resulting model shows that the front part of the person is reconstructed reasonably well, capturing the overall shape without applying any additional configuration or parameter tuning in COLMAP. However, the back part of the body is poorly reconstructed, with fragmented and noisy geometry, making it difficult to obtain a complete and consistent surface. See **Figure 14**.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/person_1.png\" style=\"width: 85%;\">\n",
        "    <p><b>(a)</b> 3D reconstruction of a person using COLMAP's Automatic Reconstruction - view 1.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/person_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> 3D reconstruction of a person using COLMAP's Automatic Reconstruction - view 2.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 14:</b> Poisson surface reconstruction of the upper body of a person obtained using COLMAP Automatic Reconstruction and MeshLab post-processing..\n",
        "</p>\n",
        "\n",
        "You can download the mesh from here: [Person mesh](https://drive.google.com/file/d/1lLWKX8ODKQzOToXyA7IhrVe_Uc2Z392R/view?usp=sharing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d88799d",
      "metadata": {},
      "source": [
        "# 5. Use Neuralangelo or Gaussian splatting to reconstruct a 3D mesh from images captured by you (Optional 2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90fe576a",
      "metadata": {},
      "source": [
        "In this exercise, we explore **3D Gaussian Splatting (3DGS)**, a recent method for novel view synthesis that represents scenes as a collection of 3D Gaussians with learnable position, covariance, opacity, and spherical harmonic coefficients for view-dependent color.\n",
        "\n",
        "We reconstructed three scenes of varying complexity:\n",
        "- **Person**: A human subject (25 images)\n",
        "- **Brick (milk carton)**: A small object with text and uniform surfaces (23 images)\n",
        "- **UPF Campus**: An outdoor courtyard scene (55 images)\n",
        "\n",
        "## Methodology\n",
        "\n",
        "All images were captured using an iPhone with fixed exposure and focus settings to maintain consistent camera parameters across frames.\n",
        "\n",
        "For the **person** and **brick** scenes, we positioned the object at the center and captured images while moving the camera in a roughly circular trajectory around it, always looking inward at the subject.\n",
        "\n",
        "For the **UPF** scene, we walked around the inner courtyard between buildings, capturing images while looking inward at the opposite façades—similar to the second acquisition strategy described in Section 4.\n",
        "\n",
        "### Training Setup\n",
        "\n",
        "We used the official 3DGS implementation with the `convert.py` script, which internally runs COLMAP to obtain a sparse reconstruction and camera poses. Training was performed with **default parameters** for **30,000 iterations** per scene on an **RTX 4080 GPU**, taking approximately 30–45 minutes per scene.\n",
        "\n",
        "## Results\n",
        "\n",
        "### Quantitative Metrics\n",
        "\n",
        "The following metrics were computed on the **training views** (no held-out test set was used):\n",
        "\n",
        "| Scene | L1 Loss | PSNR (dB) |\n",
        "|-------|---------|-----------|\n",
        "| Person | 0.0081 | 35.49 |\n",
        "| Brick | 0.0080 | 36.89 |\n",
        "| UPF | 0.0230 | 27.92 |\n",
        "\n",
        "**Note:** These metrics represent an upper bound on reconstruction quality, as they are evaluated on the same images used for optimization. Novel view synthesis quality would be lower, particularly for views far from the training camera poses.\n",
        "\n",
        "The **brick** scene achieved the highest PSNR (36.89 dB), likely due to its simple geometry and mostly Lambertian surfaces. The **person** scene also achieved excellent results (35.49 dB), benefiting from the controlled indoor setting. The **UPF** scene has lower PSNR (27.92 dB), which is expected for an outdoor scene with complex geometry, vegetation, and moving people during capture.\n",
        "\n",
        "### Person\n",
        "\n",
        "<!-- TODO: Add figures when ready -->\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/person_real.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Real image.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/person_render.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Rendered view.</p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 11:</b> Comparison between a real training image and the corresponding 3DGS render for the person scene.</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"figures/gs/person_overview.png\" width=\"60%\">\n",
        "</p>\n",
        "<p align=\"center\"><b>Figure 12:</b> Overview of the person scene showing the Gaussian point cloud and camera positions.</p>\n",
        "\n",
        "The face and frontal body are reconstructed with high fidelity. However, when moving the virtual camera outside the training pose distribution, artifacts become apparent:\n",
        "\n",
        "- **Floaters**: Spurious Gaussian splats appear in empty space, particularly when viewing from novel angles.\n",
        "- **Head disappearing/reappearing**: When viewing the back of the head, parts of the head take on the appearance of the background wall. \n",
        "  \n",
        "  <!-- TODO: Describe this more accurately after reviewing the video -->\n",
        "  This artifact is likely caused by the subject moving slightly during capture. If the head position was inconsistent across frames, the optimization would struggle to find a coherent 3D representation, causing Gaussians to \"bake in\" the wrong background colors for certain viewing directions.\n",
        "\n",
        "<!-- TODO: Add video or GIF -->\n",
        "<p align=\"center\">\n",
        "  <video width=\"80%\" controls>\n",
        "    <source src=\"figures/gs/person_video.mp4\" type=\"video/mp4\">\n",
        "  </video>\n",
        "</p>\n",
        "<p align=\"center\"><b>Video 1:</b> Novel view synthesis of the person scene, showing quality degradation when moving outside training poses.</p>\n",
        "\n",
        "### Brick (Milk Carton)\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/brick_real.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Real image.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/brick_render.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Rendered view.</p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 13:</b> Comparison between a real training image and the corresponding 3DGS render for the brick scene.</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"figures/gs/brick_overview.png\" width=\"60%\">\n",
        "</p>\n",
        "<p align=\"center\"><b>Figure 14:</b> Overview of the brick scene.</p>\n",
        "\n",
        "The text on the milk carton is remarkably well-preserved in the reconstruction. However, from certain viewing angles, the text appears **duplicated or ghosted**. This is likely due to small inaccuracies in the estimated camera poses from COLMAP—if the poses are slightly off, the optimization may place redundant Gaussians to explain the same text from multiple slightly-shifted viewpoints.\n",
        "\n",
        "As with the other scenes, moving the camera significantly outside the training distribution causes the reconstruction to break down.\n",
        "\n",
        "<!-- TODO: Add video or GIF -->\n",
        "\n",
        "### UPF Campus\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/upf_real.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Real image.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/upf_render.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Rendered view.</p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 15:</b> Comparison between a real training image and the corresponding 3DGS render for the UPF scene.</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"figures/gs/upf_overview.png\" width=\"60%\">\n",
        "</p>\n",
        "<p align=\"center\"><b>Figure 16:</b> Overview of the UPF campus scene showing the reconstructed courtyard and camera trajectory.</p>\n",
        "\n",
        "The building façades are reconstructed with good quality, preserving architectural details and textures. However, this scene exhibits several characteristic artifacts:\n",
        "\n",
        "- **Ghost people**: Since pedestrians were moving through the courtyard during capture, they appear as semi-transparent \"ghosts\" in the reconstruction. 3DGS attempts to explain their presence across multiple frames but cannot represent them coherently since they were in different positions in each image.\n",
        "\n",
        "- **Vertical camera movement sensitivity**: Moving the virtual camera up or down (outside the roughly horizontal plane of the training trajectory) causes individual Gaussians to become visually apparent as ellipsoidal blobs rather than blending into a coherent surface.\n",
        "\n",
        "<!-- TODO: Add video or GIF -->\n",
        "<p align=\"center\">\n",
        "  <video width=\"80%\" controls>\n",
        "    <source src=\"figures/gs/upf_video.mp4\" type=\"video/mp4\">\n",
        "  </video>\n",
        "</p>\n",
        "<p align=\"center\"><b>Video 2:</b> Novel view synthesis of the UPF scene, demonstrating ghost artifacts and quality degradation when moving vertically.</p>\n",
        "\n",
        "## Discussion\n",
        "\n",
        "Across all three scenes, we observe a consistent pattern: **3DGS produces high-quality renders for viewpoints within or near the training camera distribution, but quality degrades rapidly for novel views outside this distribution.** This is a fundamental limitation of the method—the Gaussians are optimized to explain the training images, and they have no incentive to look correct from unseen angles.\n",
        "\n",
        "The quality hierarchy (brick > person > UPF) reflects scene complexity:\n",
        "- **Static, simple geometry** with Lambertian surfaces (brick) achieves the best results.\n",
        "- **Deformable subjects** (person) are challenging if they move during capture.\n",
        "- **Outdoor scenes with transient objects** (UPF with pedestrians) introduce ghosting artifacts that are difficult to avoid without explicit handling of dynamic content.\n",
        "\n",
        "For improved results, one could consider:\n",
        "- Capturing more images with greater viewpoint diversity (including vertical variation)\n",
        "- Using video capture for denser temporal sampling\n",
        "- Applying methods that explicitly handle dynamic scenes or transient objects\n",
        "- Fine-tuning COLMAP parameters to improve pose accuracy (reducing text duplication artifacts)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
