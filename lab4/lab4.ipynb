{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4daf0-04ce-4fe8-8780-46e09d239f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sqlite3\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import open3d\n",
    "import seaborn as sns\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "from visualize_model import Model\n",
    "from database import blob_to_array, pair_id_to_image_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfed4a-95ed-4c25-ae03-dfa1be34bfa4",
   "metadata": {},
   "source": [
    "# 1. 3D mesh reconstruction from a set of images from the Gerrard Hall dataset.\n",
    "\n",
    "This exercise was used as an initial exploration of COLMAP in order to become familiar with its workflow, reconstruction pipeline, and the different representations it produces.\n",
    "\n",
    "First, we installed Colmap and runned the automatic reconstruction on the Gerrard Hall dataset. This pipeline consists of two main stages: **sparse reconstruction** and **dense reconstruction**.\n",
    "\n",
    "## Sparse reconstruction\n",
    "\n",
    "After running the automatic reconstruction, we visualized the *sparse* reconstruction directly in COLMAP. This visualization shows the recovered camera poses (in red) and the 3D sparse point cloud representing the structure of the scene. As shown in **Figure 1**, this sparse model allows us to verify that the cameras have been correctly localized and that the general geometry of the scene has been recovered.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"figures/gerrard_hall_colmap.png\" width=\"45%\">\n",
    "</p>\n",
    "<p align=\"center\"><b>Figure 1:</b> Sparse reconstruction visualized in COLMAP, showing the estimated camera poses (red) and the sparse 3D point cloud.</p>\n",
    "\n",
    "## Dense reconstruction\n",
    "\n",
    "Once the sparse reconstruction was completed, COLMAP computed a *dense* reconstruction, which produces a much denser point cloud and allows generating a surface mesh.\n",
    "\n",
    "For the meshing step, different reconstruction methods can be used, such as:\n",
    "\n",
    "* **Poisson Meshing**: Produces smooth and detailed surfaces, preserving fine geometric details and color information. However, it may introduce holes or missing regions in areas with insufficient data.\n",
    "\n",
    "* **Delaunay Meshing**: Produces a more complete surface and tends to close gaps, but usually lacks fine details and does not preserve color information as well as Poisson.\n",
    "\n",
    "A visual comparison between the two meshing strategies is shown in **Figure 2**. In our case, we chose the **Poisson** mesher for the reconstruction, as it provided better visual quality, finer details, and more accurate color information.\n",
    "\n",
    "\n",
    "<div style=\"display: flex; center; gap: 10px;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/gerrard_hall_poisson.png\" width=\"85%\">\n",
    "    <p><b>(a)</b> Poisson mesh reconstruction.</p>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/gerrard_hall_delaunay.png\" width=\"85%\">\n",
    "    <p><b>(b)</b> Delaunay mesh reconstruction.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<p align=\"center\"><b>Figure 2:</b> Comparison between Poisson and Delaunay meshing. The Poisson mesh preserves fine geometric details and color information, while the Delaunay mesh produces a more complete but less detailed surface.</p>\n",
    "\n",
    "After selecting the Poisson mesher, COLMAP produced two different mesh representations: \n",
    "\n",
    "* `meshed-poisson.ply`: A smooth surface mesh obtained via Poisson reconstruction. It preserves fine details and color but may contain holes or artifacts.\n",
    "\n",
    "* `fused.ply`: A fused point-based surface representation, which is less smooth but more directly connected to the original dense point cloud.\n",
    "\n",
    "A visual comparison between both representations is shown in **Figure 3**. \n",
    "\n",
    "<div style=\"display: flex; center; gap: 10px;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/gerrard_hall_poisson.png\" style=\"width: 85%;\">\n",
    "    <p><b>(a)</b> <code>meshed-poisson.ply</code></p>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/gerrard_hall_poisson_fused.png\" style=\"width: 85%;\">\n",
    "    <p><b>(b)</b> <code>fused.ply</code></p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<p align=\"center\"><b>Figure 3:</b> Comparison between the two mesh representations generated by COLMAP. The Poisson mesh (a) produces a smoother and more visually detailed surface, while the fused representation (b) is noisier but more directly connected to the original dense point cloud.</p>\n",
    "\n",
    "Based on this comparison, we selected the `meshed-poisson.ply` representation as our \"final\" mesh, as it provides smoother surfaces, better geometric details, and more accurate color information. Although it may contain some small holes or artifacts, these can be corrected through post-processing. \n",
    "\n",
    "## Mesh Post-processing\n",
    "\n",
    "As we can see in **Figure 4**, the raw mesh contained several artifacts and noisy components, especially around the borders and less constrained regions.\n",
    "\n",
    "<div style=\"display: flex; center; gap: 10px;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/gerrard_hall_mesh_before_1.png\" style=\"width: 85%;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/gerrard_hall_mesh_before_2.png\" style=\"width: 85%;\">\n",
    "  </div>\n",
    "</div>\n",
    "<p align=\"center\"><b>Figure 4:</b> Two different views of the raw <code>meshed-poisson.ply</code> mesh, showing artifacts and noisy regions, especially near the borders.</p>\n",
    "\n",
    "\n",
    "To improve the final result, we applied different cleaning filters in MeshLab, such as:\n",
    "\n",
    "*  **Removing isolated components (wrt Diameter)**, \n",
    "* **Manually removed a connected region** that corresponded to a reconstruction artifact by selecting the undesired faces with a rectangular selection tool and deleting the selected vertices and faces. \n",
    "\n",
    "**Figure 5** hows the cleaned versions of the previous meshes, obtained after applying the post-processing steps described above.\n",
    "\n",
    "<div style=\"display: flex; center; gap: 10px;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/gerrard_hall_mesh_after_1.png\" style=\"width: 85%;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/gerrard_hall_mesh_after_2.png\" style=\"width: 85%;\">\n",
    "  </div>\n",
    "</div>\n",
    "<p align=\"center\"><b>Figure 5:</b> Two different views of the cleaned <code>meshed-poisson.ply</code> mesh after post-processing, showing a significant reduction of artifacts and noisy regions.</p>\n",
    "\n",
    "\n",
    "The final processed mesh can be downloaded from the following link:\n",
    "[Gerrard Hall Mesh](https://drive.google.com/file/d/1yNNZeB9sxfI0jd5833KQu7Z_Lk0r3WLD/view?usp=drive_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c31494-3050-4951-aed1-b463f3fba821",
   "metadata": {},
   "source": [
    "# 2. Analyze reconstructions using python\n",
    "## 2.1. Run the notebook, using the Gerrard Hall reconstruction (0.5)\n",
    "#### <span style='color:Green'> - Add the path to your reconstruction. Answer the questions at the end  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41796fd2-2304-487f-a74c-3f1a51ad83f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your path\n",
    "reconstruction_path = \"./gerrard-hall/sparse/0\"\n",
    "database_path = \"./gerrard-hall/database.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a701bb3-945a-434c-880c-849dad97a97d",
   "metadata": {},
   "source": [
    "#### Load an existing reconstruction and print its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf644962-ba41-403f-a1a4-3e1b08d16151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.read_model(reconstruction_path, ext='.bin') # Should also work with .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629e852-0407-4eff-ba62-b9d1d51015bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model.images\n",
    "cameras = model.cameras\n",
    "points3D = model.points3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47fb58-dbcd-4c6f-8168-e296832aacf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(images)} images. This is the information available for one of them:\")\n",
    "print(images[1])\n",
    "print(f\"\\nLoaded {len(cameras)} cameras. This is the information available for one of them:\")\n",
    "print(cameras[1])\n",
    "print(f\"\\nLoaded {len(points3D)} 3D points. This is the information available for one of them:\")\n",
    "print(points3D[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce97039-6c3e-40e9-af81-e0795fc5b41a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d1dde-024f-47b6-85cc-99f0801f414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ff5d3-a33f-41f5-882d-d370bf3dd489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keypoints with proper reshaping based on actual cols per image\n",
    "# Each image may have different cols (4, or 6), so we query all fields together\n",
    "keypoints = dict()\n",
    "for image_id, data, rows, cols in db.execute(\n",
    "        \"SELECT image_id, data, rows, cols FROM keypoints\"):\n",
    "    # Reshape to actual (rows, cols) and keep only (x, y) coordinates\n",
    "    kpts = blob_to_array(data, np.float32, (rows, cols))[:, :2]\n",
    "    keypoints[image_id] = kpts\n",
    "\n",
    "# Check what column counts were found\n",
    "unique_cols = set()\n",
    "for image_id, data, rows, cols in db.execute(\n",
    "        \"SELECT image_id, data, rows, cols FROM keypoints\"):\n",
    "    unique_cols.add(cols)\n",
    "print(f\"Keypoint column counts in database: {unique_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843a569-d812-4bfe-8f61-e68cc9d9dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded keypoints from {len(keypoints)} images. These are the {len(keypoints[1])} keypoints for one of them:\")\n",
    "print(keypoints[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f31f2a-d3b3-48af-b0cc-1276f5cfe177",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = dict()\n",
    "count_no_data = 0\n",
    "for pair_id, data in db.execute(\"SELECT pair_id, data FROM matches\"):\n",
    "    if data is None:\n",
    "        count_no_data += 1\n",
    "    else:\n",
    "        matches[pair_id_to_image_ids(pair_id)] = blob_to_array(data, np.uint32, (-1, 2))\n",
    "print(f\"Loaded {len(matches)} matches. {count_no_data}/{len(matches)+count_no_data} matches contained no data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8472b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal\n",
    "dir(matches)\n",
    "matches.__class__\n",
    "matches.items()\n",
    "matches.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07761b-eaec-488d-b1e9-36720e203a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These are the matches between two images:\")\n",
    "print(matches[1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c0675-6841-4519-b79f-cc2b811b94fd",
   "metadata": {},
   "source": [
    "#### Visualize the point cloud and cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95e07c-b1d6-4f08-8842-8fc41b676a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.create_window()\n",
    "model.add_points()\n",
    "model.add_cameras(scale=0.25)\n",
    "model.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5e272-83de-4136-b047-885051bd7e78",
   "metadata": {},
   "source": [
    "#### <span style='color:Green'>  How many keypoints there are in total? </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73905673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"#Keypoints total: {sum(kpts.shape[0] for kpts in keypoints.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782bbf7",
   "metadata": {},
   "source": [
    "There are **1,061,700 keypoints** in total across all 100 images. That averages to **10,617 keypoints per image**. This is a normal amount of keypoints for a SIFT detector in high resolution images of **$5616 \\times 3744$**\n",
    "\n",
    "Not all detected keypoints will contribute to the final 3D reconstruction. Only keypoints that are successfully matched across multiple images and pass geometric verification will be triangulated into 3D points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c13142-d04b-4f48-9b28-5e78b9c72a8b",
   "metadata": {},
   "source": [
    "#### <span style='color:Green'>  How many 3D points originated from a keypoint in the first image? </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points3d_originated_in_image_1 = sum(1 for point3D in points3D.values() if 1 in point3D.image_ids)\n",
    "\n",
    "print(f\"#3d points from image 1: {num_points3d_originated_in_image_1}\")\n",
    "\n",
    "num_keypoints_image_1 = keypoints[1].shape[0]\n",
    "\n",
    "print(f\"#Kepoints from image 1: {num_keypoints_image_1}\")\n",
    "\n",
    "print(f\"Conversion percentage: {num_points3d_originated_in_image_1 / num_keypoints_image_1 * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd85b6f",
   "metadata": {},
   "source": [
    "There are **2,824 3D points** that originated from a keypoint in image 1, out of the **12,207 keypoints** detected in that image. This means only about **23%** of the keypoints in image 1 were successfully triangulated into 3D points.\n",
    "\n",
    "This relatively low percentage is expected for several reasons:\n",
    "- Many keypoints may not have reliable matches in other images (due to occlusions, viewpoint changes, or non-distinctive descriptors).\n",
    "- Some matches are rejected during geometric verification (RANSAC) as outliers.\n",
    "\n",
    "The fact that image 1 contributes 2,824 points out of the total 42,815 3D points (~6.6%) is reasonable, considering there are 100 images in the dataset and each 3D point is typically observed by multiple images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a83ca-0ad4-493f-a7b7-9e11ae8153c0",
   "metadata": {},
   "source": [
    "## 2.2 Plot the 3D points coloured according to the number of images and error. (0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803fb41-cb01-43c2-9853-9f4458640a75",
   "metadata": {},
   "source": [
    "#### <span style='color:Green'> - Plot the 3D points coloured according to the **number of images** from which it originated. </span> Can you extract any conclusions from the visualization? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d55db2-d7b4-4eda-91c2-8b2001fbf6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = []\n",
    "num_images = []\n",
    "errors = []\n",
    "\n",
    "for point3D in points3D.values():\n",
    "    xyz.append(point3D.xyz)\n",
    "    num_images.append(len(point3D.image_ids))\n",
    "    errors.append(point3D.error)\n",
    "\n",
    "xyz = np.array(xyz)\n",
    "num_images = np.array(num_images)\n",
    "errors = np.array(errors)\n",
    "\n",
    "num_images_norm = (num_images - num_images.min()) / (num_images.max() - num_images.min())\n",
    "errors_norm = (errors - errors.min()) / (errors.max() - errors.min())\n",
    "\n",
    "\n",
    "colors_by_images = cm.jet(num_images_norm)[:, :3]\n",
    "colors_by_error = cm.jet(errors_norm)[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240545d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 1))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "norm = plt.Normalize(vmin=num_images.min(), vmax=num_images.max())\n",
    "cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap='jet'), \n",
    "                    cax=ax, orientation='horizontal')\n",
    "cbar.set_label('Number of images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = open3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "\n",
    "pcd = open3d.geometry.PointCloud()\n",
    "\n",
    "pcd.points = open3d.utility.Vector3dVector(xyz)\n",
    "pcd.colors = open3d.utility.Vector3dVector(colors_by_images)\n",
    "\n",
    "vis.add_geometry(pcd)\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db190967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a histogram too\n",
    "images_per_point3d = [len(p3d.image_ids) for p3d in points3D.values()]\n",
    "counts, bins = np.histogram(images_per_point3d, bins=[i for i in range(max(images_per_point3d) + 1)])\n",
    "_ = plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19cc260",
   "metadata": {},
   "source": [
    "As can be seen in the histogram, most 3D points are visible from only a few images (2 to 10 images), with the distribution peaking at 3 images and then following a long-tailed pattern. This is expected behavior in Structure-from-Motion reconstructions:\n",
    "\n",
    "- Points at the **edges of the scene** or in areas with **limited camera coverage** are only observed by a small number of views.\n",
    "- Points in **central or well-covered regions** tend to be observed by more cameras, but these are relatively rare.\n",
    "- The long tail indicates that only a small subset of points are truly \"landmark\" points visible from many viewpoints.\n",
    "\n",
    "When visualizing the point cloud colored by number of images, there are no clear regions with consistently higher or lower observation counts. The variation appears to be distributed somewhat homogeneously throughout the whole scene rather than concentrated in specific areas. This suggests that the camera trajectory provides relatively uniform coverage, and the number of images per point depends more on local properties of the point than on global position.\n",
    "\n",
    "<img src=\"figures/images_per_point3d.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da04601",
   "metadata": {},
   "source": [
    "To better understand which points are most reliably reconstructed, we visualize below only the points observed by at least 10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ed04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter points observed by at least 10 images\n",
    "min_observations = 10\n",
    "\n",
    "xyz_plus10 = []\n",
    "colors_plus10 = []\n",
    "for point3D in points3D.values():\n",
    "    if len(point3D.image_ids) >= min_observations:\n",
    "        xyz_plus10.append(point3D.xyz)\n",
    "        colors_plus10.append(point3D.rgb / 255)\n",
    "\n",
    "print(f\"Points with >= {min_observations} observations: {len(xyz_plus10)} / {len(points3D)} ({100*len(xyz_plus10)/len(points3D):.1f}%)\")\n",
    "\n",
    "vis = open3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "\n",
    "pcd = open3d.geometry.PointCloud()\n",
    "\n",
    "pcd.points = open3d.utility.Vector3dVector(xyz_plus10)\n",
    "pcd.colors = open3d.utility.Vector3dVector(colors_plus10)\n",
    "\n",
    "vis.add_geometry(pcd)\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e44073",
   "metadata": {},
   "source": [
    "<img src=\"figures/points_from_plus10_images.png\"/>\n",
    "\n",
    "The visualization shows that the 3D points visible from 10 or more images are distributed accross the 4 walls of the building. There is a gap on one of the walls close to the corner. This gap is present in the leftmost part of the backside wall of the hall. When looking at the whole pointcloud, we see that there are slightly less 3D points there than in other regions, because the area is occluded in some images by vegetation.\n",
    "\n",
    "<img src=\"figures/gerrard_hall_tree_occlusion.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba4033-e370-45e0-9dcd-2964d3c5763a",
   "metadata": {},
   "source": [
    "#### <span style='color:Green'> - Plot the 3D points coloured according to the **error**. </span> - What is this parameter? Can you extract any conclusions from the visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 1))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "norm = plt.Normalize(vmin=errors_norm.min(), vmax=errors_norm.max())\n",
    "cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap='jet'), \n",
    "                    cax=ax, orientation='horizontal')\n",
    "cbar.set_label('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb29f94-1864-4e21-b534-7eb681dd9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = open3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "\n",
    "pcd = open3d.geometry.PointCloud()\n",
    "\n",
    "pcd.points = open3d.utility.Vector3dVector(xyz)\n",
    "pcd.colors = open3d.utility.Vector3dVector(colors_by_error)\n",
    "\n",
    "vis.add_geometry(pcd)\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a histogram too\n",
    "error_per_point3d = [p3d.error for p3d in points3D.values()]\n",
    "counts, bins = np.histogram(error_per_point3d)\n",
    "_ = plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a36ce4",
   "metadata": {},
   "source": [
    "<span style='color:darkcyan'>\n",
    "\n",
    "The **error** property of a 3D point represents the **mean reprojection error** across all images that observe that point. Reprojection error measures the pixel distance between:\n",
    "- The detected 2D keypoint location in an image\n",
    "- The projected location of the triangulated 3D point back onto that image\n",
    "\n",
    "A low reprojection error indicates that the 3D point is geometrically consistent across all its observations, while a high error suggests localization uncertainty or potential outliers.\n",
    "\n",
    "From the histogram, we observe that **most points have low reprojection error** (concentrated near zero), which indicates a well-calibrated reconstruction. However, there is a **long tail of points with higher errors**.\n",
    "\n",
    "When visualizing the point cloud colored by error, **no clear spatial pattern emerges**. The high-error points appear scattered randomly throughout the scene, resembling \"salt noise\" rather than being concentrated in specific regions. This suggests that high-error points are likely caused by:\n",
    "- **Matching errors**: Incorrect feature correspondences that passed geometric verification\n",
    "- **Motion blur or defocus**: Keypoints detected on slightly blurred regions\n",
    "- **Depth discontinuities**: Points near object boundaries where small errors in localization cause large reprojection errors\n",
    "- **Repetitive textures**: Areas where similar-looking features may be incorrectly matched\n",
    "\n",
    "To better understand the error distribution, we analyze below the correlation between reprojection error and the number of observing images.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87l2y0s2rtw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation between error and number of observations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot: error vs number of images\n",
    "axes[0].scatter(num_images, errors, alpha=0.3, s=1)\n",
    "axes[0].set_xlabel('Number of observing images')\n",
    "axes[0].set_ylabel('Reprojection error (pixels)')\n",
    "axes[0].set_title('Error vs Number of Observations')\n",
    "\n",
    "# Box plot: error distribution grouped by number of images\n",
    "max_imgs_for_boxplot = min(20, num_images.max())  # Limit for readability\n",
    "mask = num_images <= max_imgs_for_boxplot\n",
    "data_for_boxplot = [errors[num_images == i] for i in range(2, max_imgs_for_boxplot + 1) if np.sum(num_images == i) > 0]\n",
    "labels = [str(i) for i in range(2, max_imgs_for_boxplot + 1) if np.sum(num_images == i) > 0]\n",
    "axes[1].boxplot(data_for_boxplot, labels=labels)\n",
    "axes[1].set_xlabel('Number of observing images')\n",
    "axes[1].set_ylabel('Reprojection error (pixels)')\n",
    "axes[1].set_title('Error Distribution by Observation Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation coefficient\n",
    "correlation = np.corrcoef(num_images, errors)[0, 1]\n",
    "print(f\"Pearson correlation coefficient: {correlation:.4f}\")\n",
    "print(f\"Mean error: {errors.mean():.4f} pixels\")\n",
    "print(f\"Median error: {np.median(errors):.4f} pixels\")\n",
    "print(f\"Std error: {errors.std():.4f} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qp6q302mvtp",
   "metadata": {},
   "source": [
    "<span style='color:darkcyan'>\n",
    "\n",
    "The analysis reveals a **weak positive correlation (0.24)** between the number of observing images and the reprojection error. This means that points seen by more images tend to have **slightly higher error**, which is counterintuitive at first glance — one might expect that more observations would lead to better-constrained points with lower error.\n",
    "\n",
    "Several factors may explain this behavior:\n",
    "\n",
    "1. **Averaging effect**: The error metric is the *mean* reprojection error across all observations. With more images, there are more opportunities for one or two views to have slightly inaccurate keypoint detections (due to motion blur, slight defocus, or viewpoint-dependent appearance changes), which increases the average error even if most observations are accurate.\n",
    "\n",
    "2. **Prominent features are harder to localize precisely**: Points visible from many viewpoints are typically located on salient features (corners, edges, high-contrast regions). These features may exhibit viewpoint-dependent appearance variations that make sub-pixel localization less consistent across views.\n",
    "\n",
    "3. **Bundle adjustment trade-offs**: During optimization, the solver must find a 3D position that minimizes the total reprojection error across all observations. With more observations, finding a single 3D point that perfectly satisfies all views becomes harder, leading to slightly higher average residuals.\n",
    "\n",
    "Despite this trend, the overall reprojection errors remain low (mean: **0.61 pixels**, median: **0.53 pixels**), indicating a high-quality reconstruction.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe177d5-893b-41b4-aa1c-6d6fb06b54b7",
   "metadata": {},
   "source": [
    "## 2.3 Plot the 3D points that correspond to a keypoint in the first image. Also plot the image with the keypoints (1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1888156a-c5f5-4695-a75b-2883f1a7c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = []\n",
    "colors = []\n",
    "\n",
    "for point3D in points3D.values():\n",
    "    if 1 not in point3D.image_ids:\n",
    "        continue\n",
    "    xyz.append(point3D.xyz)\n",
    "    colors.append(point3D.rgb / 255)\n",
    "\n",
    "vis = open3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "\n",
    "pcd = open3d.geometry.PointCloud()\n",
    "\n",
    "pcd.points = open3d.utility.Vector3dVector(xyz)\n",
    "pcd.colors = open3d.utility.Vector3dVector(colors)\n",
    "\n",
    "vis.add_geometry(pcd)\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "\n",
    "vis.poll_events()\n",
    "vis.update_renderer()\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea7ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_keypoints = keypoints[1]\n",
    "\n",
    "cv_keypoints = [cv2.KeyPoint(x=float(pt[0]), y=float(pt[1]), size=50) \n",
    "                for pt in image1_keypoints]\n",
    "\n",
    "\n",
    "images_path = \"./gerrard-hall/images/\"\n",
    "\n",
    "print(type(image1_keypoints))\n",
    "print(image1_keypoints.shape)\n",
    "print(image1_keypoints.dtype)\n",
    "\n",
    "print(type(images[1]))\n",
    "\n",
    "im1 = cv2.imread(images_path + images[1].name)\n",
    "print(type(im1))\n",
    "im_with_kpts = cv2.drawKeypoints(im1, cv_keypoints, None, \n",
    "                                   color=(255, 0, 255),\n",
    "                                   flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(cv2.cvtColor(im_with_kpts, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117ea99-bc47-41f6-8b14-a42213ee0482",
   "metadata": {},
   "source": [
    "## 2.4 Create a visualization for the number of matches between all images. (1.0)\n",
    "For example: https://seaborn.pydata.org/generated/seaborn.heatmap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fba504-a871-4287-9833-46aa1c883637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping from image_id to sorted index (by filename)\n",
    "id_name_pairs = [(img_id, img.name) for img_id, img in images.items()]\n",
    "id_name_pairs_sorted = sorted(id_name_pairs, key=lambda x: x[1])  # Sort by filename\n",
    "id_to_sorted_idx = {img_id: idx for idx, (img_id, name) in enumerate(id_name_pairs_sorted)}\n",
    "\n",
    "# Build match matrix using sorted indices\n",
    "n_images = len(images)\n",
    "match_matrix = np.zeros((n_images, n_images), dtype=np.int32)\n",
    "for (id1, id2), match_data in matches.items():\n",
    "    i = id_to_sorted_idx[id1]\n",
    "    j = id_to_sorted_idx[id2]\n",
    "    num_matches = len(match_data)\n",
    "    match_matrix[i, j] = num_matches\n",
    "    match_matrix[j, i] = num_matches\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(match_matrix, xticklabels=10, yticklabels=10)\n",
    "plt.xlabel('Image Index (sorted by filename)')\n",
    "plt.ylabel('Image Index (sorted by filename)')\n",
    "plt.title('Number of matches between images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2pknijhsfv7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check if image IDs correspond to filename order\n",
    "print(\"Image ID -> Filename mapping:\")\n",
    "id_name_pairs = [(img_id, img) for img_id, img in images.items()]\n",
    "id_name_pairs_sorted_by_id = sorted(id_name_pairs, key=lambda x: x[0])\n",
    "id_name_pairs_sorted_by_name = sorted(id_name_pairs, key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nSorted by image_id:\")\n",
    "for img_id, name in id_name_pairs_sorted_by_id[:10]:\n",
    "    print(f\"  ID {img_id}: {name}\")\n",
    "print(\"  ...\")\n",
    "\n",
    "print(\"\\nSorted by filename:\")\n",
    "for img_id, name in id_name_pairs_sorted_by_name[:10]:\n",
    "    print(f\"  ID {img_id}: {name}\")\n",
    "print(\"  ...\")\n",
    "\n",
    "# Check if the orderings match\n",
    "ids_by_id = [x[0] for x in id_name_pairs_sorted_by_id]\n",
    "ids_by_name = [x[0] for x in id_name_pairs_sorted_by_name]\n",
    "print(f\"\\nDo image IDs match filename order? {ids_by_id == ids_by_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc84087",
   "metadata": {},
   "source": [
    "Matches are concentrated around each image. An image won't have matches with images too far apart, as they represent different parts of the scene - images far appart may show different facades of the building. The fact that the corners of the heatmap are not 0 indicates that the last few images match with the first ones, indicating a somewhat closed track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab06d62-e966-439e-87d7-5d7ba973bca1",
   "metadata": {},
   "source": [
    "## 2.5 Visualize the keypoints and matches between the two images used in lab 3 using Colmap, how it compares to the results from lab 3? (1.0)\n",
    "#### <span style='color:Green'> You can use the GUI to get the keypoints and matches and then visualize it here, following the same style as in lab 3 to get comparable results. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea0b73",
   "metadata": {},
   "source": [
    "As shown in **Figure 6**, for this section, we will use the images `0001_s.png` and `0002_s.png`. \n",
    "\n",
    "<div style=\"display: flex; center; gap: 10px;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/0001_s.png\" width=\"85%\">\n",
    "    <p><b>(a)</b> <code> 0001_s.png </code> </p>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/0002_s.png\" width=\"85%\">\n",
    "    <p><b>(b)</b> <code> 0002_s.png </code> </p>\n",
    "  </div>\n",
    "</div>\n",
    "<p align=\"center\">\n",
    "  <b>Figure 6:</b> Input image pair used for keypoint detection and matching experiments: \n",
    "  (a) <code>0001_s.png</code> and (b) <code>0002_s.png</code>.\n",
    "</p>\n",
    "\n",
    "In Lab 3, the keypoints and correspondences were obtained using a classical pipeline: keypoint detection with ORB, brute-force matching, and outlier rejection using RANSAC.\n",
    "\n",
    "Now, instead, we use COLMAP to compute the keypoints and matches. COLMAP employs more advanced and robust feature detection and description methods, such as SIFT, as well as more reliable matching strategies (e.g., exhaustive, sequential, or spatial matching), which typically provide more accurate and consistent correspondences.\n",
    "\n",
    "To do this, we performed an automatic reconstruction in COLMAP using the two images `0001_s.png` and `0002_s.png`, and we follow the steps described in Section 2.1 to obtain the keypoints and matches.\n",
    "\n",
    "The same procedure can also be carried out manually by running `Processing > Feature Extraction` (selecting the camera model) and then `Processing > Feature Matching`, choosing the desired matching strategy. However, by using the automatic reconstruction, all the required data are already prepared and can be directly reused in Sections 2.6 and 2.7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50cc03b-672c-491c-bd61-e40bcc757f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_path = \"./castle/sparse/0\"\n",
    "database_path = \"./castle/database.db\"\n",
    "\n",
    "model = Model()\n",
    "model.read_model(reconstruction_path, ext='.bin') # Should also work with .txt\n",
    "\n",
    "images = model.images\n",
    "cameras = model.cameras\n",
    "points3D = model.points3D\n",
    "\n",
    "print(f\"Loaded {len(images)} images. This is the information available for one of them:\")\n",
    "print(images[1])\n",
    "print(f\"\\nLoaded {len(cameras)} cameras. This is the information available for one of them:\")\n",
    "print(cameras[1])\n",
    "print(f\"\\nLoaded {len(points3D)} 3D points. This is the information available for one of them:\")\n",
    "print(points3D[1]) \n",
    "\n",
    "db = sqlite3.connect(database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dict(\n",
    "    (image_id, name)\n",
    "    for image_id, name in db.execute(\n",
    "        \"SELECT image_id, name FROM images\")\n",
    ")\n",
    "\n",
    "# Shifted ids!!!\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c06ffc",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold;\">Attention:</span>\n",
    "\n",
    "In COLMAP, keypoints are stored in the database as a binary BLOB with shape \n",
    "<strong>(rows × cols)</strong>, where <code>rows</code> is the number of keypoints and \n",
    "<code>cols</code> is the number of values per keypoint.\n",
    "\n",
    "According to the COLMAP documentation:\n",
    "    \"*If the keypoints have 4 columns, the feature\n",
    "    geometry is a similarity and the third and fourth columns correspond to the scale and\n",
    "    orientation of the feature, following SIFT conventions. If the keypoints have 6 columns,\n",
    "    the feature geometry is an affinity and the last four columns encode the affine shape\n",
    "    of the feature.*\"\n",
    "\n",
    "In our case, <code>cols = 6</code>. Therefore, using <code>reshape(-1, 6)</code> reconstructs each\n",
    "keypoint correctly as a row with six values. Since only the 2D image coordinates are\n",
    "required for visualization and matching, we keep only the first two columns\n",
    "<code>(x, y)</code> and discard the remaining attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7304cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_id, rows, cols in db.execute(\n",
    "        \"SELECT image_id, rows, cols FROM keypoints\"):\n",
    "    print(f\"image_id={image_id}, rows={rows}, cols={cols}\")\n",
    "\n",
    "keypoints = dict(\n",
    "    (image_id, blob_to_array(data, np.float32, (-1, 6))[:, :2]) \n",
    "    for image_id, data in db.execute(\n",
    "        \"SELECT image_id, data FROM keypoints\")\n",
    ")\n",
    "print(f\"Loaded keypoints from {len(keypoints)} images\")\n",
    "print(f\"Total keypoints: {sum(kpts.shape[0] for kpts in keypoints.values())}\")\n",
    "print(f\"These are the {len(keypoints[2])} keypoints in the first image and {len(keypoints[1])} keypoints in the second image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f37bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread('./castle/images/0001_s.png')\n",
    "image2 = cv2.imread('./castle/images/0002_s.png')\n",
    "\n",
    "# Convert keypoints to cv2.KeyPoint format\n",
    "kp1 = [cv2.KeyPoint(x=pt[0], y=pt[1], size=1) for pt in keypoints[2]] \n",
    "kp2 = [cv2.KeyPoint(x=pt[0], y=pt[1], size=1) for pt in keypoints[1]]\n",
    "\n",
    "def show_keypoints(image, cv_keypoints, title=\"Keypoints\"):\n",
    "    im_with_kpts = cv2.drawKeypoints(\n",
    "        image, \n",
    "        cv_keypoints, \n",
    "        None,\n",
    "        color=(255, 0, 255),\n",
    "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(cv2.cvtColor(im_with_kpts, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_keypoints(image1, kp1, title=\"Keypoints Image 1\")\n",
    "show_keypoints(image2, kp2, title=\"Keypoints Image 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = dict()\n",
    "count_no_data = 0\n",
    "for pair_id, data in db.execute(\"SELECT pair_id, data FROM matches\"):\n",
    "    if data is None:\n",
    "        count_no_data += 1\n",
    "    else:\n",
    "        matches[pair_id_to_image_ids(pair_id)] = blob_to_array(data, np.uint32, (-1, 2))\n",
    "print(f\"Loaded {len(matches)} match. \")\n",
    "print(f\"There are {len(matches[1,2])} matches between the two images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db07d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"matches[(1,2)] shape: {matches[(1,2)].shape}\")\n",
    "print(f\"matches[(1,2)] dtype: {matches[(1,2)].dtype}\")\n",
    "print(f\"First 5 matches: {matches[(1,2)][:5]}\")\n",
    "print(f\"Max values: {matches[(1,2)].max(axis=0)}\")\n",
    "print(f\"len(keypoints[1]): {len(keypoints[1])}\")\n",
    "print(f\"len(keypoints[2]): {len(keypoints[2])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert matches to cv2.DMatch format\n",
    "cv_matches = []\n",
    "for match in matches[(1, 2)]:\n",
    "    query_idx = int(match[1])  \n",
    "    train_idx = int(match[0]) \n",
    "    cv_matches.append(cv2.DMatch(_queryIdx=query_idx, _trainIdx=train_idx, _distance=0))\n",
    "\n",
    "# Draw matches\n",
    "img_matches = cv2.drawMatches(\n",
    "    cv2.cvtColor(image1, cv2.COLOR_BGR2RGB), kp1, \n",
    "    cv2.cvtColor(image2, cv2.COLOR_BGR2RGB), kp2,\n",
    "    cv_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Display the matches\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img_matches)\n",
    "plt.axis('off')\n",
    "plt.title('Matches between Image 1 and Image 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abcf64",
   "metadata": {},
   "source": [
    "As we can see from the obtained results, the number of matches obtained with COLMAP is very high (**1571**), and most of them appear to be geometrically consistent. However, from a visual inspection alone, it is not possible to determine with certainty if all correspondences are true inliers. \n",
    "\n",
    "However, by randomly visualizing small subsets of the matches, we were able to identify some outliers (up to 6) that are not easily noticeable when all correspondences are displayed at once. This confirms that a purely visual inspection is not sufficient to reliably separate correct matches from incorrect ones when there are many matches obtained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba01a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_match_sample(img1, kp1, img2, kp2, matches, n=10, title=\"\"):\n",
    "    if len(matches) > n:\n",
    "        matches = random.sample(matches, n)\n",
    "    vis = cv2.drawMatches(\n",
    "        cv2.cvtColor(img1, cv2.COLOR_BGR2RGB), kp1,\n",
    "        cv2.cvtColor(img2, cv2.COLOR_BGR2RGB), kp2,\n",
    "        matches, None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.imshow(vis)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "draw_match_sample(image1, kp1, image2, kp2, cv_matches, n=50, title=\"Random Matches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662d192",
   "metadata": {},
   "source": [
    "If we compare these results with those obtained in Lab 3, we can observe some differences.\n",
    "\n",
    "In Lab 3, we obtained **1290** raw matches before applying RANSAC, which were reduced to **650** inliers after geometric verification. This clearly shows that a significant number of outliers were present and that applying RANSAC was essential to remove incorrect correspondences. The effect of this filtering step can be clearly observed in **Figure 7**.\n",
    "\n",
    "ORB is a simpler and faster descriptor, and it generally detects fewer keypoints compared to SIFT. As a result, the total number of matches is lower. However, after RANSAC, the remaining correspondences appear to be more geometrically consistent (*although, as mentioned before, a purely visual inspection is not sufficient to guarantee that all matches are true inliers; by randomly sampling small subsets, we were only able to identify at most **two** remaining outliers*). This suggests that, although fewer matches are obtained, they may be more reliable.\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "  <div style=\"text-align: center; width: 45%;\">\n",
    "    <img src=\"figures/lab3_matches_before_ransac.png\" style=\"width: 100%;\">\n",
    "    <p><b>(a)</b> Matches before RANSAC (1290).</p>\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 45%;\">\n",
    "    <img src=\"figures/lab3_matches_after_ransac.png\" style=\"width: 100%;\">\n",
    "    <p><b>(b)</b> Matches after RANSAC (650).</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <b>Figure 7:</b> Comparison of Lab 3 results before and after applying RANSAC. \n",
    "  A large number of incorrect correspondences are removed after geometric verification, \n",
    "  leading to a more consistent set of matches.\n",
    "</p>\n",
    "\n",
    "Overall, this comparison highlights an important trade-off between both approaches. Simpler methods such as ORB may produce fewer correspondences, but these can be highly consistent after geometric filtering. On the other hand, more advanced pipelines tend to generate a much denser set of matches, which is beneficial for tasks such as 3D reconstruction, even if some outliers are introduced. In the case of COLMAP, the number of outliers appears to be very small based on our visual inspection compared to the inliers, resulting in a high inlier ratio. This indicates that COLMAP’s matching strategy is robust and reliable, providing a dense set of correspondences while maintaining good geometric consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a4a42-5c40-4983-af8a-2c0ef286ce5d",
   "metadata": {},
   "source": [
    "## 2.6 Triangulate and visualize the 3D points from the keypoints extracted using Colmap on the two images used in lab 3, how it compares to the results from lab 3? (1.0) \n",
    "#### <span style='color:Green'> - Use the triangulation from lab 3 to the get the 3D points and visualize them following the same style. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c476baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_id, img2_id = 2, 1  # Shifted ids\n",
    "\n",
    "# Get the images\n",
    "img1 = images[img1_id]\n",
    "img2 = images[img2_id]\n",
    "\n",
    "# Get the camera (shared intrinsics)\n",
    "camera = cameras[img1.camera_id]\n",
    "\n",
    "print(\"Camera information (shared intrinsics):\")\n",
    "print(f\"Camera ID    : {camera.id}\")\n",
    "print(f\"Model        : {camera.model}\")\n",
    "print(f\"Image size   : {camera.width} x {camera.height}\")\n",
    "print(f\"Parameters   : [f, cx, cy, k] = {camera.params}\")\n",
    "\n",
    "# Intrinsic matrix\n",
    "f, cx, cy, k = camera.params\n",
    "K = np.array([[f, 0,  cx],\n",
    "              [0, f,  cy],\n",
    "              [0, 0,  1]], dtype=float)\n",
    "\n",
    "print(\"\\nIntrinsic matrix K:\")\n",
    "print(K)\n",
    "\n",
    "# Projection matrices\n",
    "def P_from_image(img, K):\n",
    "    R = img.qvec2rotmat()\n",
    "    t = img.tvec.reshape(3, 1)\n",
    "    return K @ np.hstack([R, t])\n",
    "\n",
    "P1 = P_from_image(img1, K)\n",
    "P2 = P_from_image(img2, K)\n",
    "\n",
    "print(\"\\nProjection matrices (P = K [R | t]):\")\n",
    "print(\"P1 shape:\", P1.shape)\n",
    "print(P1)\n",
    "print(\"\\nP2 shape:\", P2.shape)\n",
    "print(P2)\n",
    "\n",
    "# Camera centers\n",
    "C1 = -img1.qvec2rotmat().T @ img1.tvec\n",
    "C2 = -img2.qvec2rotmat().T @ img2.tvec\n",
    "\n",
    "print(\"\\nCamera centers in world coordinates:\")\n",
    "print(f\"C1: {C1}\")\n",
    "print(f\"C2: {C2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab3 code for triangulation\n",
    "def image_normalization_matrix(imsize):\n",
    "    #Normalization matrix based on image size, maps image coordinates to [-1,1] range\n",
    "    w, h = imsize\n",
    "    return np.array([\n",
    "        [2 / w,     0,     -1], #Scale x-coordinates to [-1, 1]\n",
    "        [0,     2 / h,     -1], #Scale y-coordinates to [-1, 1]\n",
    "        [0,         0,      1]\n",
    "    ])\n",
    "\n",
    "def hartley_normalization_matrix(x: np.ndarray) -> np.ndarray:\n",
    "    #Normalization matrix based on Hartley normalization\n",
    "    x = x.copy()\n",
    "    x = x / x[2:3, :]                #Homogeneous to Euclidean coordinates\n",
    "    pts = x[:2, :]                   #2D image coordinates\n",
    "\n",
    "    c = np.mean(pts, axis=1)         #Centroid\n",
    "    pts_c = pts - c[:, None]         #Translate points to centroid\n",
    "\n",
    "    d = np.sqrt(np.sum(pts_c**2, axis=0)) #Euclidean distance to centroid\n",
    "    mean_d = np.mean(d)                   #Mean distance from the centroid\n",
    "\n",
    "    s = np.sqrt(2) / (mean_d + 1e-12) #Scale to get mean distance equals sqrt(2)\n",
    "\n",
    "    T = np.array([\n",
    "        [s, 0, -s * c[0]],\n",
    "        [0, s, -s * c[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=float)         #Norm Matrix\n",
    "    return T\n",
    "\n",
    "def triangulate(x1, x2, P1, P2, imsize, norm='hartley') -> np.ndarray:\n",
    "    #Triangulate 3D points from two views using the DLT method\n",
    "    assert P1.shape == (3,4) == P2.shape\n",
    "    assert x1.shape == x2.shape and x1.shape[0] == 3\n",
    "\n",
    "    #Apply normalization to improve numerical stability\n",
    "    if norm == 'image_size':\n",
    "        T = image_normalization_matrix(imsize)\n",
    "        P1n = T @ P1\n",
    "        P2n = T @ P2\n",
    "        x1n = T @ x1\n",
    "        x2n = T @ x2\n",
    "    elif norm == 'hartley':\n",
    "        T1 = hartley_normalization_matrix(x1)\n",
    "        T2 = hartley_normalization_matrix(x2)\n",
    "        P1n = T1 @ P1\n",
    "        P2n = T2 @ P2\n",
    "        x1n = T1 @ x1\n",
    "        x2n = T2 @ x2\n",
    "    elif norm is None:\n",
    "        P1n, P2n, x1n, x2n = P1, P2, x1, x2\n",
    "    else:\n",
    "        raise ValueError(\"norm must be None, 'image_size' or 'hartley'\")\n",
    "\n",
    "    def generate_Ai(x, P):\n",
    "        #Construct the DLT constraint matrix for a single correspondenc\n",
    "        x = x.copy()\n",
    "        x = x / x[2] #Dehomogenize image point\n",
    "        u, v = x[0], x[1]\n",
    "        return np.vstack([\n",
    "            v * P[2,:] - P[1,:],\n",
    "            P[0,:] - u * P[2,:]\n",
    "        ])\n",
    "\n",
    "    N = x1.shape[1]\n",
    "    X = np.zeros((4, N), dtype=float) #Homogeneous 3D points\n",
    "\n",
    "    for i in range(N):\n",
    "        #Build the full DLT system by stacking constraints from both views\n",
    "        A = np.vstack([\n",
    "            generate_Ai(x1n[:, i], P1n),\n",
    "            generate_Ai(x2n[:, i], P2n)\n",
    "        ])\n",
    "\n",
    "        #Solve A X = 0 using SVD\n",
    "        _, _, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "        Xi = Vt[-1, :] #Solution associated with smallest singular value\n",
    "\n",
    "        #Normalize homogeneous coordinates\n",
    "        if abs(Xi[3]) > 1e-12:\n",
    "            Xi = Xi / Xi[3]\n",
    "\n",
    "        X[:, i] = Xi\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f479aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = matches[(1, 2)]   # shape (M,2)\n",
    "\n",
    "idx1 = m[:, 1]        # indices in image 1 keypoints\n",
    "idx2 = m[:, 0]        # indices in image 2 keypoints\n",
    "\n",
    "x1 = keypoints[img1_id][idx1].T  # shape (2,M)\n",
    "x2 = keypoints[img2_id][idx2].T  # shape (2,M)\n",
    "\n",
    "x1 = np.vstack([x1, np.ones((1, x1.shape[1]))]) # shape (3,M)\n",
    "x2 = np.vstack([x2, np.ones((1, x2.shape[1]))]) # shape (3,M)\n",
    "\n",
    "# Triangulate points with Lab3 code\n",
    "imsize = (camera.width, camera.height)\n",
    "X = triangulate(x1, x2, P1, P2, imsize, norm='hartley')\n",
    "\n",
    "X_eucl = X/X[-1,:]\n",
    "points3d = X[:3].T\n",
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "points3d = X_eucl[:3].T\n",
    "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color='rgb(30, 100, 200)', size=2)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eacbca-842d-4d9e-85bd-efc35c5dd15a",
   "metadata": {},
   "source": [
    "## 2.7 Visualize the sparse reconstruction using the 2 images from lab 3, and the complete CASTLE dataset. Comment on the differences between techniques and number of images used. (1.0)\n",
    "#### <span style='color:Green'> - Use the reconstruction from Colmap to the get the 3D points and visualize them following the same style, using two images and the complete dataset. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9504a3-dcbf-4b2b-9ff6-71c5a4584742",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32408947",
   "metadata": {},
   "source": [
    "# 3. Configure the reconstruction to improve the results. (1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89648246",
   "metadata": {},
   "source": "To improve the reconstruction quality in the _castle_ dataset we created a custom COLMAP script (`colmap_improved.sh`) with tuned parameters. Below we describe the key parameter changes we have set and explain how they help improve reconstruction quality.\n\nThe original castle reconstruction contained **missing parts and holes** in walls and the floor. These areas have relatively uniform appearance or very high-frequency texture patterns, which makes them challenging for feature-based reconstruction methods.\n\n## COLMAP Pipeline Overview\n\nCOLMAP follows a sequential pipeline where each step builds upon the previous one. Understanding this pipeline helps us identify where improvements can be made:\n\n1. **Feature Extraction** → Detect and describe keypoints in each image\n2. **Feature Matching** → Find correspondences between image pairs\n3. **Sparse Reconstruction** → Estimate camera poses and triangulate a sparse 3D point cloud\n4. **Dense Stereo** → Compute dense depth maps for each image\n5. **Depth Fusion** → Merge depth maps into a consistent dense point cloud\n6. **Meshing** → Convert the point cloud into a triangular mesh\n\n## Parameter Analysis by Pipeline Stage\n\n### 1. Feature Extraction\n\nWe hypothesized that the problematic low-texture areas might not have enough keypoints detected, leading to sparse or missing matches. SIFT (Scale-Invariant Feature Transform) detects keypoints by finding local extrema in a scale-space pyramid built from Difference-of-Gaussians (DoG) images.\n\nTo increase keypoint detection in subtle texture regions, we modified:\n\n- **`num_octaves`: 4 → 5** — SIFT builds a pyramid of progressively downsampled images (octaves). Adding one more octave allows detection of **larger-scale features** that might be present in uniform areas where fine details are absent.\n\n- **`octave_resolution`: 3 → 4** — Within each octave, SIFT computes multiple DoG images at different scales. Increasing the resolution provides **finer scale sampling**, which can detect features at intermediate scales that would otherwise be missed.\n\n- **`peak_threshold`: 0.0067 → 0.006** — This threshold filters out keypoints with low contrast in the DoG response. Lowering it makes SIFT **more sensitive to subtle features**, accepting keypoints with weaker gradients that are common in low-texture regions.\n\n### 2. Feature Matching\n\nEven with more keypoints, low-texture areas often produce similar-looking descriptors that are hard to match uniquely. We enabled:\n\n- **`guided_matching`: 0 → 1** — After an initial set of matches is found, guided matching uses the estimated epipolar geometry to **search for additional correspondences** along epipolar lines. This is particularly useful for low-texture regions where descriptor-only matching might miss valid correspondences due to ambiguous descriptors.\n\n### 3. Sparse Reconstruction (Mapper)\n\nThe mapper estimates camera poses through incremental Structure-from-Motion. We adjusted:\n\n- **`init_min_num_inliers`: 100 → 50** — The initial image pair needs sufficient inlier matches to bootstrap the reconstruction. Lowering this threshold allows initialization even when the best image pair has **fewer high-quality matches**, which can happen in challenging scenes.\n\n- **`ba_refine_principal_point`: 0 → 1** — By default, COLMAP assumes the principal point is at the image center. Enabling refinement allows bundle adjustment to **optimize the optical center position**, which can improve accuracy for cameras where the sensor is not perfectly centered.\n\n### 4. Dense Stereo (PatchMatch)\n\nThis is arguably the most critical stage for our problem. PatchMatch Stereo computes dense depth by comparing image patches across views. For low-texture regions, the key insight is that **larger patches provide more context** for matching.\n\n- **`window_radius`: 5 → 7** — This changes the patch size from **11×11 to 15×15 pixels**. In uniform areas, a small patch may look identical at many depths (the aperture problem). A larger patch is more likely to include some distinctive texture at the boundaries, enabling more reliable depth estimation. This is the **most important change** for addressing holes in walls and floors.\n\n### 5. Depth Fusion\n\nThe fusion step merges per-image depth maps into a single consistent point cloud. Points are only kept if they are observed consistently across multiple views.\n\n- **`min_num_pixels`: 5 → 3** — This threshold specifies the minimum number of images that must agree on a 3D point for it to be kept. Lowering it **retains more points**, including those in areas where fewer views provide reliable depth. This produces a denser point cloud at the cost of potentially more noise.\n\n### 6. Poisson Meshing\n\nFinally, Poisson surface reconstruction converts the oriented point cloud into a watertight mesh by solving a global optimization problem.\n\n- **`depth`: 13 → 10** — This controls the octree depth used for reconstruction. A lower depth produces a **smoother mesh** with less fine geometric detail but fewer artifacts from noise. This helps close small remaining gaps and produces a cleaner final result.\n\n## Summary\n\nOur parameter tuning follows a logical chain of reasoning:\n\n1. **More keypoints** in low-texture areas (SIFT parameters) → more potential matches\n2. **Guided matching** → recover matches that descriptor-only matching misses\n3. **Relaxed initialization** → successfully start reconstruction even with fewer matches\n4. **Larger stereo patches** → reliable depth estimation despite uniform appearance\n5. **Lower fusion threshold** → retain more 3D points to fill gaps\n6. **Smoother meshing** → clean final surface with fewer artifacts"
  },
  {
   "cell_type": "markdown",
   "id": "b353928f",
   "metadata": {},
   "source": [
    "# 4. Reconstruct a 3D mesh from images captured by you. (1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a38f53",
   "metadata": {},
   "source": [
    "Before taking our own photographs, we ensured that the camera focus and exposure were fixed. This was done to keep the camera parameters consistent across all images and to avoid changes in appearance that could negatively affect feature matching and reconstruction.\n",
    "\n",
    "## UPF Campus\n",
    "\n",
    "To reconstruct the UPF Campus, we captured the images using two different strategies in order to evaluate which one produced better reconstruction results.\n",
    "\n",
    "* In the **first strategy**, we stood approximately at the center of the campus and took photos while rotating around ourselves, capturing images in all directions with good overlap.\n",
    "\n",
    "* In the **second strategy**, we walked close to the campus walls and took photos of the opposite façades, moving around the campus in a roughly rectangular path.\n",
    "\n",
    "**Figure 8** shows the *sparse* reconstructions obtained with each acquisition strategy.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/approach_1.png\" style=\"width: 73%;\">\n",
    "    <p><b>(a)</b> Sparse reconstruction using the first strategy.</p>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"figures/approach_2.png\" style=\"width: 85%;\">\n",
    "    <p><b>(b)</b> Sparse reconstruction using the second strategy.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <b>Figure 8:</b> Sparse reconstructions of the UPF Campus obtained with two different image acquisition strategies.\n",
    "</p>\n",
    "\n",
    "We did not initially expect the first strategy to produce a spherical-like reconstruction. However, after analyzing the result of the second strategy, this behavior became more understandable.\n",
    "\n",
    "Since the images in the first strategy were captured from a single location while rotating around the camera, the camera centers remain almost at the same position and only the viewing direction changes. As a result, the reconstructed points are distributed around the camera, forming a spherical structure.\n",
    "\n",
    "In contrast, in the second strategy the camera is physically moved along the campus perimeter. This produces a clearer camera trajectory and leads to a reconstruction that better reflects the actual spatial layout of the scene.\n",
    "\n",
    "Therefore, we selected the **second strategy** to perform the final reconstruction of the UPF Campus.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}