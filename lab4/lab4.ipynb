{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa4daf0-04ce-4fe8-8780-46e09d239f06",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import sqlite3\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import open3d\n",
        "import seaborn as sns\n",
        "import random\n",
        "import sys\n",
        "import math\n",
        "import plotly.graph_objects as go\n",
        "from visualize_model import Model\n",
        "from database import blob_to_array, pair_id_to_image_ids\n",
        "import plotly.graph_objects as go\n",
        "# import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25bfed4a-95ed-4c25-ae03-dfa1be34bfa4",
      "metadata": {},
      "source": [
        "# 1. 3D mesh reconstruction from a set of images from the Gerrard Hall dataset.\n",
        "\n",
        "This exercise was used as an initial exploration of COLMAP in order to become familiar with its workflow, reconstruction pipeline, and the different representations it produces.\n",
        "\n",
        "First, we installed Colmap and runned the automatic reconstruction on the Gerrard Hall dataset. This pipeline consists of two main stages: **sparse reconstruction** and **dense reconstruction**.\n",
        "\n",
        "## Sparse reconstruction\n",
        "\n",
        "After running the automatic reconstruction, we visualized the *sparse* reconstruction directly in COLMAP. This visualization shows the recovered camera poses (in red) and the 3D sparse point cloud representing the structure of the scene. As shown in **Figure 1**, this sparse model allows us to verify that the cameras have been correctly localized and that the general geometry of the scene has been recovered.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"figures/gerrard_hall_colmap.png\" width=\"45%\">\n",
        "</p>\n",
        "<p align=\"center\"><b>Figure 1:</b> Sparse reconstruction visualized in COLMAP, showing the estimated camera poses (red) and the sparse 3D point cloud.</p>\n",
        "\n",
        "## Dense reconstruction\n",
        "\n",
        "Once the sparse reconstruction was completed, COLMAP computed a *dense* reconstruction, which produces a much denser point cloud and allows generating a surface mesh.\n",
        "\n",
        "For the meshing step, different reconstruction methods can be used, such as:\n",
        "\n",
        "* **Poisson Meshing**: Produces smooth and detailed surfaces, preserving fine geometric details and color information. However, it may introduce holes or missing regions in areas with insufficient data.\n",
        "\n",
        "* **Delaunay Meshing**: Produces a more complete surface and tends to close gaps, but usually lacks fine details and does not preserve color information as well as Poisson.\n",
        "\n",
        "A visual comparison between the two meshing strategies is shown in **Figure 2**. In our case, we chose the **Poisson** mesher for the reconstruction, as it provided better visual quality, finer details, and more accurate color information.\n",
        "\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_poisson.png\" width=\"85%\">\n",
        "    <p><b>(a)</b> Poisson mesh reconstruction.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_delaunay.png\" width=\"85%\">\n",
        "    <p><b>(b)</b> Delaunay mesh reconstruction.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\"><b>Figure 2:</b> Comparison between Poisson and Delaunay meshing. The Poisson mesh preserves fine geometric details and color information, while the Delaunay mesh produces a more complete but less detailed surface.</p>\n",
        "\n",
        "After selecting the Poisson mesher, COLMAP produced two different mesh representations: \n",
        "\n",
        "* `meshed-poisson.ply`: A smooth surface mesh obtained via Poisson reconstruction. It preserves fine details and color but may contain holes or artifacts.\n",
        "\n",
        "* `fused.ply`: A fused point-based surface representation, which is less smooth but more directly connected to the original dense point cloud.\n",
        "\n",
        "A visual comparison between both representations is shown in **Figure 3**. \n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_poisson.png\" style=\"width: 85%;\">\n",
        "    <p><b>(a)</b> <code>meshed-poisson.ply</code></p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_poisson_fused.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> <code>fused.ply</code></p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\"><b>Figure 3:</b> Comparison between the two mesh representations generated by COLMAP. The Poisson mesh (a) produces a smoother and more visually detailed surface, while the fused representation (b) is noisier but more directly connected to the original dense point cloud.</p>\n",
        "\n",
        "Based on this comparison, we selected the `meshed-poisson.ply` representation as our \"final\" mesh, as it provides smoother surfaces, better geometric details, and more accurate color information. Although it may contain some small holes or artifacts, these can be corrected through post-processing. \n",
        "\n",
        "## Mesh Post-processing\n",
        "\n",
        "As we can see in **Figure 4**, the raw mesh contained several artifacts and noisy components, especially around the borders and less constrained regions.\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_mesh_before_1.png\" style=\"width: 85%;\">\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_mesh_before_2.png\" style=\"width: 85%;\">\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 4:</b> Two different views of the raw <code>meshed-poisson.ply</code> mesh, showing artifacts and noisy regions, especially near the borders.</p>\n",
        "\n",
        "\n",
        "To improve the final result, we applied different cleaning filters in MeshLab, such as:\n",
        "\n",
        "*  **Removing isolated components (wrt Diameter)**, \n",
        "* **Manually removed a connected region** that corresponded to a reconstruction artifact by selecting the undesired faces with a rectangular selection tool and deleting the selected vertices and faces. \n",
        "\n",
        "**Figure 5** hows the cleaned versions of the previous meshes, obtained after applying the post-processing steps described above.\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_mesh_after_1.png\" style=\"width: 85%;\">\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_mesh_after_2.png\" style=\"width: 85%;\">\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 5:</b> Two different views of the cleaned <code>meshed-poisson.ply</code> mesh after post-processing, showing a significant reduction of artifacts and noisy regions.</p>\n",
        "\n",
        "\n",
        "The final processed mesh can be downloaded from the following link:\n",
        "[Gerrard Hall Mesh](https://drive.google.com/file/d/1yNNZeB9sxfI0jd5833KQu7Z_Lk0r3WLD/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19c31494-3050-4951-aed1-b463f3fba821",
      "metadata": {},
      "source": [
        "# 2. Analyze reconstructions using python\n",
        "## 2.1. Run the notebook, using the Gerrard Hall reconstruction (0.5)\n",
        "#### <span style='color:Green'> - Add the path to your reconstruction. Answer the questions at the end  </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41796fd2-2304-487f-a74c-3f1a51ad83f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add your path\n",
        "reconstruction_path = \"./gerrard-hall/sparse/0\"\n",
        "database_path = \"./gerrard-hall/database.db\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a701bb3-945a-434c-880c-849dad97a97d",
      "metadata": {},
      "source": [
        "#### Load an existing reconstruction and print its contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf644962-ba41-403f-a1a4-3e1b08d16151",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model()\n",
        "model.read_model(reconstruction_path, ext='.bin') # Should also work with .txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b629e852-0407-4eff-ba62-b9d1d51015bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "images = model.images\n",
        "cameras = model.cameras\n",
        "points3D = model.points3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca47fb58-dbcd-4c6f-8168-e296832aacf5",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(f\"Loaded {len(images)} images. This is the information available for one of them:\")\n",
        "print(images[1])\n",
        "print(f\"\\nLoaded {len(cameras)} cameras. This is the information available for one of them:\")\n",
        "print(cameras[1])\n",
        "print(f\"\\nLoaded {len(points3D)} 3D points. This is the information available for one of them:\")\n",
        "print(points3D[1]) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ce97039-6c3e-40e9-af81-e0795fc5b41a",
      "metadata": {
        "tags": []
      },
      "source": [
        "#### Load the database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74d1dde-024f-47b6-85cc-99f0801f414c",
      "metadata": {},
      "outputs": [],
      "source": [
        "db = sqlite3.connect(database_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a83e688",
      "metadata": {},
      "source": [
        "> <span style=\"font-weight:bold;\">Note:</span>\n",
        "> In COLMAP, keypoints are stored in the database as a binary BLOB with shape \n",
        "><strong>(rows Ã— cols)</strong>, where <code>rows</code> is the number of keypoints and \n",
        "><code>cols</code> is the number of values per keypoint.\n",
        ">\n",
        "> According to the COLMAP documentation:\n",
        ">    \"*If the keypoints have 4 columns, the feature\n",
        ">    geometry is a similarity and the third and fourth columns correspond to the scale and\n",
        ">    orientation of the feature, following SIFT conventions. If the keypoints have 6 columns,\n",
        ">    the feature geometry is an affinity and the last four columns encode the affine shape\n",
        ">    of the feature.*\"\n",
        ">\n",
        "> Therefore, we had to change the initial code to load the keypoints properly. Same is taken into account in the subsequent sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6ff5d3-a33f-41f5-882d-d370bf3dd489",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load keypoints with proper reshaping based on actual cols per image\n",
        "# Each image may have different cols (4, or 6), so we query all fields together\n",
        "keypoints = dict()\n",
        "for image_id, data, rows, cols in db.execute(\n",
        "        \"SELECT image_id, data, rows, cols FROM keypoints\"):\n",
        "    # Reshape to actual (rows, cols) and keep only (x, y) coordinates\n",
        "    kpts = blob_to_array(data, np.float32, (rows, cols))[:, :2]\n",
        "    keypoints[image_id] = kpts\n",
        "\n",
        "# Check what column counts were found\n",
        "unique_cols = set()\n",
        "for image_id, data, rows, cols in db.execute(\n",
        "        \"SELECT image_id, data, rows, cols FROM keypoints\"):\n",
        "    unique_cols.add(cols)\n",
        "print(f\"Keypoint column counts in database: {unique_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c843a569-d812-4bfe-8f61-e68cc9d9dfbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loaded keypoints from {len(keypoints)} images. These are the {len(keypoints[1])} keypoints for one of them:\")\n",
        "print(keypoints[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27f31f2a-d3b3-48af-b0cc-1276f5cfe177",
      "metadata": {},
      "outputs": [],
      "source": [
        "matches = dict()\n",
        "count_no_data = 0\n",
        "for pair_id, data in db.execute(\"SELECT pair_id, data FROM matches\"):\n",
        "    if data is None:\n",
        "        count_no_data += 1\n",
        "    else:\n",
        "        matches[pair_id_to_image_ids(pair_id)] = blob_to_array(data, np.uint32, (-1, 2))\n",
        "print(f\"Loaded {len(matches)} matches. {count_no_data}/{len(matches)+count_no_data} matches contained no data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa07761b-eaec-488d-b1e9-36720e203a31",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"These are the matches between two images:\")\n",
        "print(matches[1,3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a12c0675-6841-4519-b79f-cc2b811b94fd",
      "metadata": {},
      "source": [
        "#### Visualize the point cloud and cameras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb95e07c-b1d6-4f08-8842-8fc41b676a10",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.create_window()\n",
        "model.add_points()\n",
        "model.add_cameras(scale=0.25)\n",
        "model.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d5e272-83de-4136-b047-885051bd7e78",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'>  How many keypoints there are in total? </span> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73905673",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"#Keypoints total: {sum(kpts.shape[0] for kpts in keypoints.values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c782bbf7",
      "metadata": {},
      "source": [
        "There are **1,061,700 keypoints** in total across all 100 images. That averages to **10,617 keypoints per image**. This is a normal amount of keypoints for a SIFT detector in high resolution images of **$5616 \\times 3744$**\n",
        "\n",
        "Not all detected keypoints will contribute to the final 3D reconstruction. Only keypoints that are successfully matched across multiple images and pass geometric verification will be triangulated into 3D points."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c13142-d04b-4f48-9b28-5e78b9c72a8b",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'>  How many 3D points originated from a keypoint in the first image? </span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d1b7c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_points3d_originated_in_image_1 = sum(1 for point3D in points3D.values() if 1 in point3D.image_ids)\n",
        "\n",
        "print(f\"#3d points from image 1: {num_points3d_originated_in_image_1}\")\n",
        "\n",
        "num_keypoints_image_1 = keypoints[1].shape[0]\n",
        "\n",
        "print(f\"#Kepoints from image 1: {num_keypoints_image_1}\")\n",
        "\n",
        "print(f\"Conversion percentage: {num_points3d_originated_in_image_1 / num_keypoints_image_1 * 100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acd85b6f",
      "metadata": {},
      "source": [
        "There are **2,824 3D points** that originated from a keypoint in image 1, out of the **12,207 keypoints** detected in that image. This means only about **23%** of the keypoints in image 1 were successfully triangulated into 3D points.\n",
        "\n",
        "This relatively low percentage is expected for several reasons:\n",
        "- Many keypoints may not have reliable matches in other images (occlusions, changes in the viewpoint, or their descriptor not being distinct enough-Lowe's ratio).\n",
        "- Some matches are rejected during geometric verification (RANSAC) as outliers.\n",
        "\n",
        "The fact that image 1 contributes 2,824 points out of the total 42,815 3D points (~6.6%) is reasonable, considering there are 100 images in the dataset and each 3D point is typically observed by multiple images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "075a83ca-0ad4-493f-a7b7-9e11ae8153c0",
      "metadata": {},
      "source": [
        "## 2.2 Plot the 3D points coloured according to the number of images and error. (0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6803fb41-cb01-43c2-9853-9f4458640a75",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'> - Plot the 3D points coloured according to the **number of images** from which it originated. </span> Can you extract any conclusions from the visualization? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d55db2-d7b4-4eda-91c2-8b2001fbf6e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "xyz = []\n",
        "num_images = []\n",
        "errors = []\n",
        "\n",
        "for point3D in points3D.values():\n",
        "    xyz.append(point3D.xyz)\n",
        "    num_images.append(len(point3D.image_ids))\n",
        "    errors.append(point3D.error)\n",
        "\n",
        "xyz = np.array(xyz)\n",
        "num_images = np.array(num_images)\n",
        "errors = np.array(errors)\n",
        "\n",
        "num_images_norm = (num_images - num_images.min()) / (num_images.max() - num_images.min())\n",
        "errors_norm = (errors - errors.min()) / (errors.max() - errors.min())\n",
        "\n",
        "\n",
        "colors_by_images = cm.jet(num_images_norm)[:, :3]\n",
        "colors_by_error = cm.jet(errors_norm)[:, :3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "240545d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 1))\n",
        "fig.subplots_adjust(bottom=0.5)\n",
        "\n",
        "norm = plt.Normalize(vmin=num_images.min(), vmax=num_images.max())\n",
        "cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap='jet'), \n",
        "                    cax=ax, orientation='horizontal')\n",
        "cbar.set_label('Number of images')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e4a222",
      "metadata": {},
      "outputs": [],
      "source": [
        "vis = open3d.visualization.Visualizer()\n",
        "vis.create_window()\n",
        "\n",
        "pcd = open3d.geometry.PointCloud()\n",
        "\n",
        "pcd.points = open3d.utility.Vector3dVector(xyz)\n",
        "pcd.colors = open3d.utility.Vector3dVector(colors_by_images)\n",
        "\n",
        "vis.add_geometry(pcd)\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "vis.run()\n",
        "vis.destroy_window()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db190967",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's plot a histogram too\n",
        "images_per_point3d = [len(p3d.image_ids) for p3d in points3D.values()]\n",
        "counts, bins = np.histogram(images_per_point3d, bins=[i for i in range(max(images_per_point3d) + 1)])\n",
        "_ = plt.hist(bins[:-1], bins, weights=counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a19cc260",
      "metadata": {},
      "source": [
        "As can be seen in the histogram, most 3D points are visible from only a few images (2 to 10 images), with the distribution peaking at 3 images and then following a long-tailed pattern. This is expected behavior in Structure-from-Motion reconstructions:\n",
        "\n",
        "- Points in areas with **limited camera coverage**, like the buildings and vegetation around the hall, are only observed by a small number of views.\n",
        "- Points in **well-covered regions**, in this case the hall building, tend to be observed by more cameras.\n",
        "- The long tail indicates that only a small subset of points are visible from many viewpoints.\n",
        "\n",
        "When visualizing the point cloud colored by number of images, there are no clear regions with consistently higher or lower observation counts. The variation appears to be distributed somewhat homogeneously throughout the whole scene rather than concentrated in specific areas. This suggests that the camera trajectory provides relatively uniform coverage, and the number of images per point depends more on local properties of the point than on global position.\n",
        "\n",
        "\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/images_per_point3d.png\"/>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\">\n",
        "  <b>Figure 6:</b> Example visualization of the number of images that observe each 3D point.\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da04601",
      "metadata": {},
      "source": [
        "To better understand which points are most reliably reconstructed, we visualize below only the points observed by at least 10 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29ed04b",
      "metadata": {},
      "outputs": [],
      "source": [
        "min_observations = 10\n",
        "\n",
        "xyz_plus10 = []\n",
        "colors_plus10 = []\n",
        "for point3D in points3D.values():\n",
        "    if len(point3D.image_ids) >= min_observations:\n",
        "        xyz_plus10.append(point3D.xyz)\n",
        "        colors_plus10.append(point3D.rgb / 255)\n",
        "\n",
        "print(f\"Points with >= {min_observations} observations: {len(xyz_plus10)} / {len(points3D)} ({100*len(xyz_plus10)/len(points3D):.1f}%)\")\n",
        "\n",
        "vis = open3d.visualization.Visualizer()\n",
        "vis.create_window()\n",
        "\n",
        "pcd = open3d.geometry.PointCloud()\n",
        "\n",
        "pcd.points = open3d.utility.Vector3dVector(xyz_plus10)\n",
        "pcd.colors = open3d.utility.Vector3dVector(colors_plus10)\n",
        "\n",
        "vis.add_geometry(pcd)\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "vis.run()\n",
        "vis.destroy_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e44073",
      "metadata": {},
      "source": [
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/points_from_plus10_images.png\"/>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\">\n",
        "  <b>Figure 7:</b> 3D points visibles from at least 10 images.\n",
        "</p>\n",
        "\n",
        "\n",
        "The visualization shows that the 3D points visible from 10 or more images are distributed accross the 4 walls of the building. There is a gap on one of the walls close to the corner. This gap is present in the leftmost part of the backside wall of the hall. When looking at the whole pointcloud, we see that there are slightly less 3D points there than in other regions, because the area is occluded in some images by vegetation.\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_tree_occlusion.jpg\" width=\"600\"/>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\">\n",
        "  <b>Figure 8:</b> Tree occluding part of the wall.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ba4033-e370-45e0-9dcd-2964d3c5763a",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'> - Plot the 3D points coloured according to the **error**. </span> - What is this parameter? Can you extract any conclusions from the visualization?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b23532f",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 1))\n",
        "fig.subplots_adjust(bottom=0.5)\n",
        "\n",
        "norm = plt.Normalize(vmin=errors_norm.min(), vmax=errors_norm.max())\n",
        "cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap='jet'), \n",
        "                    cax=ax, orientation='horizontal')\n",
        "cbar.set_label('Error')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb29f94-1864-4e21-b534-7eb681dd9057",
      "metadata": {},
      "outputs": [],
      "source": [
        "vis = open3d.visualization.Visualizer()\n",
        "vis.create_window()\n",
        "\n",
        "pcd = open3d.geometry.PointCloud()\n",
        "\n",
        "pcd.points = open3d.utility.Vector3dVector(xyz)\n",
        "pcd.colors = open3d.utility.Vector3dVector(colors_by_error)\n",
        "\n",
        "vis.add_geometry(pcd)\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "vis.run()\n",
        "vis.destroy_window()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d01f26e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's plot a histogram too\n",
        "error_per_point3d = [p3d.error for p3d in points3D.values()]\n",
        "counts, bins = np.histogram(error_per_point3d)\n",
        "_ = plt.hist(bins[:-1], bins, weights=counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a36ce4",
      "metadata": {},
      "source": [
        "The **error** property of a 3D point represents the **mean reprojection error** across all images that observe that point. Reprojection error measures the pixel distance between:\n",
        "- The detected 2D keypoint location in an image\n",
        "- The projected location of the triangulated 3D point back onto that image\n",
        "\n",
        "A low reprojection error indicates that the 3D point is geometrically consistent across all its observations, while a high error suggests localization uncertainty or an outlier.\n",
        "\n",
        "From the histogram, we observe that **most points have low reprojection error**, around 0.5, which indicates that the reconstruction is good. However, there is a **tail of points with higher errors**.\n",
        "\n",
        "When visualizing the point cloud colored by error, there is no clear spatial pattern. The high-error points appear scattered throughout the scene, resembling \"salt noise\" rather than being concentrated in specific regions. This suggests that high-error points are likely caused by:\n",
        "- **Matching errors**: Incorrect feature correspondences that passed geometric verification\n",
        "- **Depth discontinuities**: Points near object boundaries where small errors in localization cause high reprojection errors\n",
        "- **Repetitive textures**: Areas where similar-looking features may be incorrectly matched. This is specially relevant in the wall of bricks, that may contain some repeating patterns that cause incorrect matches.\n",
        "\n",
        "As already mentioned, in **Figure 9**, the points with the most error are not concentrated  in specific areas, but are distributed around the scene.\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_error_viz.png\" width=\"600\">\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\">\n",
        "  <b>Figure 9:</b> Example visualization of the reprojection error per point.\n",
        "</p>\n",
        "\n",
        "Visualizing both the number of observing images and the reprojection error on each 3D point raises the question whether they are related or not. We analyze below the correlation between reprojection error and the number of observing images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87l2y0s2rtw",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze correlation between error and number of observations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scatter plot: error vs number of images\n",
        "axes[0].scatter(num_images, errors, alpha=0.3, s=1)\n",
        "axes[0].set_xlabel('Number of observing images')\n",
        "axes[0].set_ylabel('Reprojection error (pixels)')\n",
        "axes[0].set_title('Error vs Number of Observations')\n",
        "\n",
        "# Box plot: error distribution grouped by number of images\n",
        "max_imgs_for_boxplot = min(20, num_images.max())  # Limit for readability\n",
        "mask = num_images <= max_imgs_for_boxplot\n",
        "data_for_boxplot = [errors[num_images == i] for i in range(2, max_imgs_for_boxplot + 1) if np.sum(num_images == i) > 0]\n",
        "labels = [str(i) for i in range(2, max_imgs_for_boxplot + 1) if np.sum(num_images == i) > 0]\n",
        "axes[1].boxplot(data_for_boxplot, labels=labels, showmeans=True, meanline=True)\n",
        "axes[1].set_xlabel('Number of observing images')\n",
        "axes[1].set_ylabel('Reprojection error (pixels)')\n",
        "axes[1].set_title('Error Distribution by Observation Count')\n",
        "legend_elements = [\n",
        "    Line2D([0], [0], color='orange', linewidth=1.5, label='Median'),\n",
        "    Line2D([0], [0], color='green', linestyle='--', linewidth=1.5, label='Mean')\n",
        "]\n",
        "axes[1].legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compute correlation coefficient\n",
        "correlation = np.corrcoef(num_images, errors)[0, 1]\n",
        "print(f\"Pearson correlation coefficient: {correlation:.4f}\")\n",
        "print(f\"Mean error: {errors.mean():.4f} pixels\")\n",
        "print(f\"Median error: {np.median(errors):.4f} pixels\")\n",
        "print(f\"Std error: {errors.std():.4f} pixels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qp6q302mvtp",
      "metadata": {},
      "source": [
        "The results show a weak positive correlation (0.24) between the number of images observing a 3D point and its reprojection error. This suggests that points seen in more images tend to have slightly higher errors.\n",
        "\n",
        "One possible reason for this is that the reprojection error is computed as the mean of all errors over the observations. If a point appears in many images, there is a higher chance that some views contain small inaccuracies in keypoint detection (for example due to noise, or changes in viewpoint). This can increase the average error.\n",
        "\n",
        "Overall, the reprojection errors are still low (mean 0.64 pixels, median 0.54 pixels), which indicates that the reconstruction quality is good."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe177d5-893b-41b4-aa1c-6d6fb06b54b7",
      "metadata": {},
      "source": [
        "## 2.3 Plot the 3D points that correspond to a keypoint in the first image. Also plot the image with the keypoints (1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1888156a-c5f5-4695-a75b-2883f1a7c8b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "xyz = []\n",
        "colors = []\n",
        "\n",
        "# The images are not sorted as in the folder\n",
        "first_image_id = None\n",
        "for id, image in images.items():\n",
        "    if image.name == \"IMG_2331.JPG\":\n",
        "        first_image_id = id\n",
        "\n",
        "for point3D in points3D.values():\n",
        "    if first_image_id not in point3D.image_ids:\n",
        "        continue\n",
        "    xyz.append(point3D.xyz)\n",
        "    colors.append(point3D.rgb / 255)\n",
        "\n",
        "vis = open3d.visualization.Visualizer()\n",
        "vis.create_window()\n",
        "\n",
        "pcd = open3d.geometry.PointCloud()\n",
        "\n",
        "pcd.points = open3d.utility.Vector3dVector(xyz)\n",
        "pcd.colors = open3d.utility.Vector3dVector(colors)\n",
        "\n",
        "vis.add_geometry(pcd)\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "\n",
        "vis.poll_events()\n",
        "vis.update_renderer()\n",
        "vis.run()\n",
        "vis.destroy_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40c16d16",
      "metadata": {},
      "source": [
        "As can be seen in the visualization and **Figure 10**, the 3D points follow the shape of the wall visible on the first image. The points concentrate on the right part of the wall, which is the side that is not occluded by a tree in  most images.\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_image1_points3d.png\">\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\">\n",
        "  <b>Figure 10:</b> The 3D points triangulated from image 1.\n",
        "</p>\n",
        "\n",
        "\n",
        "In the **Figure 11** image, which is **not the first image** but one of the last ones (IMG_2429.JPG), it can be seen that the leftmost part of the wall is occluded by a tree, which may cause problems when finding keypoints and matches in that region. We show the first image in the following cells.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/gerrard_hall_tree_occlusion_image1.jpg\" width=\"600\">\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\">\n",
        "  <b>Figure 11:</b> Tree occludes part of the wall seen from image  1.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea7ff3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "image1_keypoints = keypoints[first_image_id]\n",
        "\n",
        "cv_keypoints = [cv2.KeyPoint(x=float(pt[0]), y=float(pt[1]), size=50) \n",
        "                for pt in image1_keypoints]\n",
        "\n",
        "\n",
        "images_path = \"./gerrard-hall/images/\"\n",
        "\n",
        "print(type(image1_keypoints))\n",
        "print(image1_keypoints.shape)\n",
        "print(image1_keypoints.dtype)\n",
        "\n",
        "print(type(images[1]))\n",
        "\n",
        "im1 = cv2.imread(images_path + images[first_image_id].name)\n",
        "print(type(im1))\n",
        "im_with_kpts = cv2.drawKeypoints(im1, cv_keypoints, None, \n",
        "                                   color=(255, 0, 255),\n",
        "                                   flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "plt.imshow(cv2.cvtColor(im_with_kpts, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "955842fd",
      "metadata": {},
      "source": [
        "As can be seen, most keypoints eppear in the vegetation and not on the wall. These keypoints produce very few matches between images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e117ea99-bc47-41f6-8b14-a42213ee0482",
      "metadata": {},
      "source": [
        "## 2.4 Create a visualization for the number of matches between all images. (1.0)\n",
        "For example: https://seaborn.pydata.org/generated/seaborn.heatmap.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13fba504-a871-4287-9833-46aa1c883637",
      "metadata": {},
      "outputs": [],
      "source": [
        "# THE IMAGES ARE NOT SORTED BY FILENAME IN COLMAP\n",
        "\n",
        "# Create mapping from image_id to sorted index (by filename)\n",
        "id_name_pairs = [(img_id, img.name) for img_id, img in images.items()]\n",
        "id_name_pairs_sorted = sorted(id_name_pairs, key=lambda x: x[1])  # Sort by filename\n",
        "id_to_sorted_idx = {img_id: idx for idx, (img_id, name) in enumerate(id_name_pairs_sorted)}\n",
        "\n",
        "# Build match matrix using sorted indices\n",
        "n_images = len(images)\n",
        "match_matrix = np.zeros((n_images, n_images), dtype=np.int32)\n",
        "for (id1, id2), match_data in matches.items():\n",
        "    i = id_to_sorted_idx[id1]\n",
        "    j = id_to_sorted_idx[id2]\n",
        "    num_matches = len(match_data)\n",
        "    match_matrix[i, j] = num_matches\n",
        "    match_matrix[j, i] = num_matches\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(match_matrix, xticklabels=10, yticklabels=10)\n",
        "plt.xlabel('Image Index (sorted by filename)')\n",
        "plt.ylabel('Image Index (sorted by filename)')\n",
        "plt.title('Number of matches between images')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f97d7d24",
      "metadata": {},
      "source": [
        "Matches are concentrated around each image. An image does not have matches with images too far apart, as they represent different parts of the scene - images far appart may show different facades of the building. The fact that the top-right and bottom-left corners of the heatmap are not 0 indicates that the last few images match with the first ones, indicating a somewhat closed track."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2pknijhsfv7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Check if image IDs correspond to filename order\n",
        "print(\"Image ID -> Filename mapping:\")\n",
        "id_name_pairs = [(img_id, img) for img_id, img in images.items()]\n",
        "id_name_pairs_sorted_by_id = sorted(id_name_pairs, key=lambda x: x[0])\n",
        "id_name_pairs_sorted_by_name = sorted(id_name_pairs, key=lambda x: x[1])\n",
        "\n",
        "print(\"\\nSorted by image_id:\")\n",
        "for img_id, name in id_name_pairs_sorted_by_id[:10]:\n",
        "    print(f\"  ID {img_id}: {name}\")\n",
        "print(\"  ...\")\n",
        "\n",
        "print(\"\\nSorted by filename:\")\n",
        "for img_id, name in id_name_pairs_sorted_by_name[:10]:\n",
        "    print(f\"  ID {img_id}: {name}\")\n",
        "print(\"  ...\")\n",
        "\n",
        "# Check if the orderings match\n",
        "ids_by_id = [x[0] for x in id_name_pairs_sorted_by_id]\n",
        "ids_by_name = [x[0] for x in id_name_pairs_sorted_by_name]\n",
        "print(f\"\\nDo image IDs match filename order? {ids_by_id == ids_by_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab06d62-e966-439e-87d7-5d7ba973bca1",
      "metadata": {},
      "source": [
        "## 2.5 Visualize the keypoints and matches between the two images used in lab 3 using Colmap, how it compares to the results from lab 3? (1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36969388",
      "metadata": {},
      "source": [
        "> **Note:**  \n",
        "> Before proceeding, we clarify the choice of camera intrinsics used in Sections 2.5, 2.6, and 2.7. It was not clear whether to use the intrinsic matrix $K$ estimated in Lab 3 or to let COLMAP estimate the intrinsics again.\n",
        "> \n",
        "> We chose to let COLMAP estimate the camera intrinsics automatically. In particular, we used the `SIMPLE_RADIAL` camera model, which assumes a single focal length ($f_x = f_y$) and one radial distortion parameter. We also tested the `PINHOLE` model, since in Lab 3 $f_x$ and $f_y$ were different, but it did not produce better results. Therefore, we used the `SIMPLE_RADIAL` intrinsics for the following experiments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c883bb90",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'> You can use the GUI to get the keypoints and matches and then visualize it here, following the same style as in lab 3 to get comparable results. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50ea0b73",
      "metadata": {},
      "source": [
        "As shown in **Figure 12**, for this section, we will use the images `0001_s.png` and `0002_s.png`. \n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/0001_s.png\" width=\"85%\">\n",
        "    <p><b>(a)</b> <code> 0001_s.png </code> </p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/0002_s.png\" width=\"85%\">\n",
        "    <p><b>(b)</b> <code> 0002_s.png </code> </p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\">\n",
        "  <b>Figure 12:</b> Input image pair used for keypoint detection and matching experiments: \n",
        "  (a) <code>0001_s.png</code> and (b) <code>0002_s.png</code>.\n",
        "</p>\n",
        "\n",
        "In Lab 3, the keypoints and correspondences were obtained using a classical pipeline: keypoint detection with ORB, brute-force matching, and outlier rejection using RANSAC.\n",
        "\n",
        "Now, instead, we use COLMAP to compute the keypoints and matches. COLMAP employs more advanced and robust feature detection and description methods, such as SIFT, as well as more reliable matching strategies (e.g., exhaustive, sequential, or spatial matching), which typically provide more accurate and consistent correspondences (we will use the default strategy: `sequential`).\n",
        "\n",
        "To do this, we performed an automatic reconstruction in COLMAP using the two images `0001_s.png` and `0002_s.png`, and we follow the steps described in Section 2.1 to obtain the keypoints and matches.\n",
        "\n",
        "It is important to note that the matches stored in the COLMAP database (`matches` table) correspond to **raw descriptor-based correspondences**, which have **not yet undergone geometric verification**. The geometric consistency of these matches is evaluated in a later stage, during reconstruction (`two_view_geometries` table). \n",
        "\n",
        "The same procedure can also be carried out manually by running `Processing > Feature Extraction` (selecting the camera model) and then `Processing > Feature Matching`, choosing the desired matching strategy. However, by using the automatic reconstruction, all the required data are already prepared and can be directly reused in Sections 2.6 and 2.7.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f50cc03b-672c-491c-bd61-e40bcc757f43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# reconstruction_path = \"./castle_dense/sparse/0\"\n",
        "# database_path = \"./castle_dense/database.db\"\n",
        "\n",
        "reconstruction_path = \"./castle/sparse/0\"\n",
        "database_path = \"./castle/database.db\"\n",
        "\n",
        "model = Model()\n",
        "model.read_model(reconstruction_path, ext='.bin') # Should also work with .txt\n",
        "\n",
        "images = model.images\n",
        "cameras = model.cameras\n",
        "points3D = model.points3D\n",
        "\n",
        "print(f\"Loaded {len(images)} images. This is the information available for one of them:\")\n",
        "print(images[1])\n",
        "print(f\"\\nLoaded {len(cameras)} cameras. This is the information available for one of them:\")\n",
        "print(cameras[1])\n",
        "print(f\"\\nLoaded {len(points3D)} 3D points. This is the information available for one of them:\")\n",
        "print(points3D[1]) \n",
        "\n",
        "db = sqlite3.connect(database_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec45c7c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "image_id_to_name = dict(\n",
        "    (image_id, name)\n",
        "    for image_id, name in db.execute(\n",
        "        \"SELECT image_id, name FROM images\")\n",
        ")\n",
        "\n",
        "# Shifted ids!!!\n",
        "print(image_id_to_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7304cf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load keypoints with proper reshaping based on actual cols per image\n",
        "# Each image may have different cols (4, or 6), so we query all fields together\n",
        "keypoints = dict()\n",
        "for image_id, data, rows, cols in db.execute(\n",
        "        \"SELECT image_id, data, rows, cols FROM keypoints\"):\n",
        "    # Reshape to actual (rows, cols) and keep only (x, y) coordinates\n",
        "    kpts = blob_to_array(data, np.float32, (rows, cols))[:, :2]\n",
        "    keypoints[image_id] = kpts\n",
        "\n",
        "# Check what column counts were found\n",
        "unique_cols = set()\n",
        "for image_id, data, rows, cols in db.execute(\n",
        "        \"SELECT image_id, data, rows, cols FROM keypoints\"):\n",
        "    unique_cols.add(cols)\n",
        "    \n",
        "print(f\"Keypoint column counts in database: {unique_cols}\")\n",
        "print(f\"Loaded keypoints from {len(keypoints)} images\")\n",
        "print(f\"Total keypoints: {sum(kpts.shape[0] for kpts in keypoints.values())}\")\n",
        "print(f\"These are the {len(keypoints[2])} keypoints in the first image and {len(keypoints[1])} keypoints in the second image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f37bcb",
      "metadata": {},
      "outputs": [],
      "source": [
        "image1 = cv2.imread('./castle/Images/0001_s.png')\n",
        "image2 = cv2.imread('./castle/Images/0002_s.png')\n",
        "\n",
        "# Convert keypoints to cv2.KeyPoint format\n",
        "kp1 = [cv2.KeyPoint(x=pt[0], y=pt[1], size=1) for pt in keypoints[2]] \n",
        "kp2 = [cv2.KeyPoint(x=pt[0], y=pt[1], size=1) for pt in keypoints[1]]\n",
        "\n",
        "def show_keypoints(image, cv_keypoints, title=\"Keypoints\"):\n",
        "    im_with_kpts = cv2.drawKeypoints(\n",
        "        image, \n",
        "        cv_keypoints, \n",
        "        None,\n",
        "        color=(255, 0, 255),\n",
        "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
        "    )\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(cv2.cvtColor(im_with_kpts, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "show_keypoints(image1, kp1, title=\"Keypoints Image 1\")\n",
        "show_keypoints(image2, kp2, title=\"Keypoints Image 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76fcc6bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "matches = dict()\n",
        "count_no_data = 0\n",
        "for pair_id, data in db.execute(\"SELECT pair_id, data FROM matches\"):\n",
        "    if data is None:\n",
        "        count_no_data += 1\n",
        "    else:\n",
        "        matches[pair_id_to_image_ids(pair_id)] = blob_to_array(data, np.uint32, (-1, 2))\n",
        "print(f\"Loaded {len(matches)} match. \")\n",
        "print(f\"There are {len(matches[1,2])} matches between the two images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db07d78",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"matches[(1,2)] shape: {matches[(1,2)].shape}\")\n",
        "print(f\"matches[(1,2)] dtype: {matches[(1,2)].dtype}\")\n",
        "print(f\"First 5 matches: {matches[(1,2)][:5]}\")\n",
        "print(f\"Max values: {matches[(1,2)].max(axis=0)}\")\n",
        "print(f\"len(keypoints[1]): {len(keypoints[1])}\")\n",
        "print(f\"len(keypoints[2]): {len(keypoints[2])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5238c735",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert matches to cv2.DMatch format\n",
        "cv_matches = []\n",
        "for match in matches[(1, 2)]:\n",
        "    query_idx = int(match[1])  \n",
        "    train_idx = int(match[0]) \n",
        "    cv_matches.append(cv2.DMatch(_queryIdx=query_idx, _trainIdx=train_idx, _distance=0))\n",
        "\n",
        "# Draw matches\n",
        "img_matches = cv2.drawMatches(\n",
        "    cv2.cvtColor(image1, cv2.COLOR_BGR2RGB), kp1, \n",
        "    cv2.cvtColor(image2, cv2.COLOR_BGR2RGB), kp2,\n",
        "    cv_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "# Display the matches\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(img_matches)\n",
        "plt.axis('off')\n",
        "plt.title('Matches between Image 1 and Image 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30abcf64",
      "metadata": {},
      "source": [
        "As we can see from the obtained results, the number of matches obtained with COLMAP is very high (**1571**), and most of them appear to be geometrically consistent. However, from a visual inspection alone, it is not possible to determine with certainty if all correspondences are true inliers. \n",
        "\n",
        "However, by randomly visualizing small subsets of the matches, we were able to identify some outliers (up to 6) that are not easily noticeable when all correspondences are displayed at once. This confirms that a purely visual inspection is not sufficient to reliably separate correct matches from incorrect ones when there are many matches obtained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba01a86",
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_match_sample(img1, kp1, img2, kp2, matches, n=10, title=\"\"):\n",
        "    if len(matches) > n:\n",
        "        matches = random.sample(matches, n)\n",
        "    vis = cv2.drawMatches(\n",
        "        cv2.cvtColor(img1, cv2.COLOR_BGR2RGB), kp1,\n",
        "        cv2.cvtColor(img2, cv2.COLOR_BGR2RGB), kp2,\n",
        "        matches, None,\n",
        "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
        "    )\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    plt.imshow(vis)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "draw_match_sample(image1, kp1, image2, kp2, cv_matches, n=50, title=\"Random Matches\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6662d192",
      "metadata": {},
      "source": [
        "If we compare these results with those obtained in Lab 3, we can observe some differences.\n",
        "\n",
        "In Lab 3, we obtained **1290** raw matches before applying RANSAC, which were reduced to **650** inliers after geometric verification. This clearly shows that a significant number of outliers were present and that applying RANSAC was essential to remove incorrect correspondences. The effect of this filtering step can be clearly observed in **Figure 13**.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/lab3_matches_before_ransac.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Matches before RANSAC (1290).</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/lab3_matches_after_ransac.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Matches after RANSAC (650).</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 13:</b> Comparison of Lab 3 results before and after applying RANSAC. \n",
        "  A large number of incorrect correspondences are removed after geometric verification, \n",
        "  leading to a more consistent set of matches.\n",
        "</p>\n",
        "\n",
        "ORB is a simpler and faster descriptor, and it generally detects fewer keypoints compared to SIFT. As a result, the total number of matches is lower. However, after RANSAC, the remaining correspondences appear to be more geometrically consistent (*although, as mentioned before, a purely visual inspection is not sufficient to guarantee that all matches are true inliers; by randomly sampling small subsets, we were only able to identify at most **two** remaining outliers*). \n",
        "\n",
        "Overall, this comparison highlights an important trade-off between both approaches. Simpler methods such as ORB may produce fewer correspondences, but these can be highly consistent after geometric filtering. On the other hand, more advanced pipelines tend to generate a much denser set of matches, which is beneficial for tasks such as 3D reconstruction, even if some outliers are introduced.\n",
        "\n",
        "In the case of COLMAP, the matches shown at this stage have **not yet passed any geometric verification**. These are raw descriptor-based matches stored in the database, and some outliers are still present. Geometric verification is applied later during the reconstruction process, where inconsistent matches are filtered out and only geometrically valid correspondences are used to create the 3D points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "244a4a42-5c40-4983-af8a-2c0ef286ce5d",
      "metadata": {},
      "source": [
        "## 2.6 Triangulate and visualize the 3D points from the keypoints extracted using Colmap on the two images used in lab 3, how it compares to the results from lab 3? (1.0) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27dc65de",
      "metadata": {},
      "source": [
        "In this section, we triangulate 3D points from the two images used in Lab 3 (`0001_s.png` and `0002_s.png`) by reusing the triangulation pipeline developed in that lab, but replacing the keypoints and matches with those obtained from COLMAP.\n",
        "\n",
        "We use the camera intrinsics and extrinsics estimated by COLMAP during the automatic reconstruction to build the projection matrices for both images. We will compare these intrinsics and camera poses with those estimated in Lab 3. Using the resulting projection matrices and the 2Dâ€“2D correspondences from the COLMAP matches, we triangulate the 3D points with the same DLT-based method used in Lab 3.\n",
        "\n",
        "The resulting 3D point cloud is then visualized following the same style as in Lab 3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e805e938",
      "metadata": {},
      "source": [
        "#### <span style='color:Green'> - Use the triangulation from lab 3 to the get the 3D points and visualize them following the same style. </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9c9b69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters for COLMAP\n",
        "img1_id, img2_id = 2, 1  # Shifted ids\n",
        "\n",
        "# Get the images\n",
        "img1 = images[img1_id]\n",
        "img2 = images[img2_id]\n",
        "\n",
        "# Get the camera (shared intrinsics)\n",
        "camera = cameras[img1.camera_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c476baa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison between Lab 3 camera matrices and COLMAP estimated intrinsic matrices\n",
        "\n",
        "# Lab 3 K matrix\n",
        "K_3 = np.array([[708.636, 0,  456.207],\n",
        "              [0, 709.836,  302.043],\n",
        "              [0, 0,  1]], dtype=float)\n",
        "print(\"\\nIntrinsic matrix K from Lab 3:\")\n",
        "print(K_3)\n",
        "\n",
        "# COLMAP estimated K matrix\n",
        "f, cx, cy, k = camera.params\n",
        "K = np.array([[f, 0,  cx],\n",
        "              [0, f,  cy],\n",
        "              [0, 0,  1]], dtype=float)\n",
        "print(\"\\nIntrinsic matrix estimated by COLMAP::\")\n",
        "print(K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e898272d",
      "metadata": {},
      "source": [
        "In Lab 3 we obtained $f_x = 708.64$ and $f_y = 709.84$, while COLMAP estimates a single focal length $f_x = f_y = 810.46$. This corresponds to an increase of approximately **+101.8 px** with respect to $f_x$ and **+100.6 px** with respect to $f_y$, which is around **14% larger**.\n",
        "\n",
        "For the principal point, Lab 3 gives $(c_x, c_y) = (456.21, 302.04)$, whereas COLMAP estimates $(c_x, c_y) = (461.00, 307.00)$ . This represents a shift of about **(+4.8 px, +5.0 px)**, which is relatively small compared to the image resolution.\n",
        "\n",
        "Overall, COLMAP recovers a principal point very close to the one estimated in Lab 3, and a focal length of the same order of magnitude but a bit larger. This difference is expected given the simplified camera model (`SIMPLE_RADIAL`) and the fact that the estimation is performed using only two images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9a7885",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison between Lab 3 camera matrices and COLMAP estimated camera matrices\n",
        "\n",
        "P1_3 = np.array([[708.636,   0.   , 456.207,   0.   ],\n",
        "               [  0.   , 709.836, 302.043,   0.   ],\n",
        "               [  0.   ,   0.   ,   1.   ,   0.   ]], dtype=float)\n",
        "\n",
        "P2_3 = np.array([[ 6.224e+02,  3.430e+01,  5.672e+02, -5.331e+02],\n",
        "                [-8.658e+01,  7.075e+02,  2.951e+02,  1.645e+02],      \n",
        "                [-1.655e-01, -4.747e-03,  9.862e-01,  3.030e-01]], dtype=float)\n",
        "\n",
        "print(\"\\nProjection matrices from Lab 3:\")\n",
        "print(\"P1: \\n\", P1_3)\n",
        "\n",
        "print(\"P2: \\n\", P2_3)\n",
        "\n",
        "# Projection matrices\n",
        "def P_from_image(img, K):\n",
        "    R = img.qvec2rotmat()\n",
        "    t = img.tvec.reshape(3, 1)\n",
        "    return K @ np.hstack([R, t])\n",
        "\n",
        "P1 = P_from_image(img1, K)\n",
        "P2 = P_from_image(img2, K)\n",
        "\n",
        "print(\"\\nProjection matrices from COLMAP:\")\n",
        "print(\"P1: \\n\", P1)\n",
        "print(\"P2: \\n\", P2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dc5272",
      "metadata": {},
      "source": [
        "The numerical values clearly reflect the different choices of reference frame used in each case.\n",
        "\n",
        "In Lab 3, the first camera matrix is exactly of the form $P_1 = K[I \\mid 0]$, with zero translation and an identity rotation, confirming that the world coordinate system was explicitly fixed at the first camera. Consequently, the second camera matrix $P_2$ encodes a relative rotation and translation with respect to this canonical reference.\n",
        "\n",
        "In contrast, the projection matrices estimated by COLMAP show non-zero translation components and non-identity rotations already in $P_1$. This indicates that COLMAP does not fix any camera to a canonical pose, but instead estimates all camera poses jointly in a global coordinate system. The large translation values observed in both $P_1$ and $P_2$ are therefore expected and simply reflect the arbitrary global reference frame chosen by COLMAP during optimization.\n",
        "\n",
        "Despite these differences in absolute values, the rotation blocks of the matrices are well-formed and consistent, and the relative geometry between the two cameras is preserved. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81dd9718",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lab3 code for triangulation\n",
        "def image_normalization_matrix(imsize):\n",
        "    #Normalization matrix based on image size, maps image coordinates to [-1,1] range\n",
        "    w, h = imsize\n",
        "    return np.array([\n",
        "        [2 / w,     0,     -1], #Scale x-coordinates to [-1, 1]\n",
        "        [0,     2 / h,     -1], #Scale y-coordinates to [-1, 1]\n",
        "        [0,         0,      1]\n",
        "    ])\n",
        "\n",
        "def hartley_normalization_matrix(x: np.ndarray) -> np.ndarray:\n",
        "    #Normalization matrix based on Hartley normalization\n",
        "    x = x.copy()\n",
        "    x = x / x[2:3, :]                #Homogeneous to Euclidean coordinates\n",
        "    pts = x[:2, :]                   #2D image coordinates\n",
        "\n",
        "    c = np.mean(pts, axis=1)         #Centroid\n",
        "    pts_c = pts - c[:, None]         #Translate points to centroid\n",
        "\n",
        "    d = np.sqrt(np.sum(pts_c**2, axis=0)) #Euclidean distance to centroid\n",
        "    mean_d = np.mean(d)                   #Mean distance from the centroid\n",
        "\n",
        "    s = np.sqrt(2) / (mean_d + 1e-12) #Scale to get mean distance equals sqrt(2)\n",
        "\n",
        "    T = np.array([\n",
        "        [s, 0, -s * c[0]],\n",
        "        [0, s, -s * c[1]],\n",
        "        [0, 0, 1]\n",
        "    ], dtype=float)         #Norm Matrix\n",
        "    return T\n",
        "\n",
        "def triangulate(x1, x2, P1, P2, imsize, norm='hartley') -> np.ndarray:\n",
        "    #Triangulate 3D points from two views using the DLT method\n",
        "    assert P1.shape == (3,4) == P2.shape\n",
        "    assert x1.shape == x2.shape and x1.shape[0] == 3\n",
        "\n",
        "    #Apply normalization to improve numerical stability\n",
        "    if norm == 'image_size':\n",
        "        T = image_normalization_matrix(imsize)\n",
        "        P1n = T @ P1\n",
        "        P2n = T @ P2\n",
        "        x1n = T @ x1\n",
        "        x2n = T @ x2\n",
        "    elif norm == 'hartley':\n",
        "        T1 = hartley_normalization_matrix(x1)\n",
        "        T2 = hartley_normalization_matrix(x2)\n",
        "        P1n = T1 @ P1\n",
        "        P2n = T2 @ P2\n",
        "        x1n = T1 @ x1\n",
        "        x2n = T2 @ x2\n",
        "    elif norm is None:\n",
        "        P1n, P2n, x1n, x2n = P1, P2, x1, x2\n",
        "    else:\n",
        "        raise ValueError(\"norm must be None, 'image_size' or 'hartley'\")\n",
        "\n",
        "    def generate_Ai(x, P):\n",
        "        #Construct the DLT constraint matrix for a single correspondenc\n",
        "        x = x.copy()\n",
        "        x = x / x[2] #Dehomogenize image point\n",
        "        u, v = x[0], x[1]\n",
        "        return np.vstack([\n",
        "            v * P[2,:] - P[1,:],\n",
        "            P[0,:] - u * P[2,:]\n",
        "        ])\n",
        "\n",
        "    N = x1.shape[1]\n",
        "    X = np.zeros((4, N), dtype=float) #Homogeneous 3D points\n",
        "\n",
        "    for i in range(N):\n",
        "        #Build the full DLT system by stacking constraints from both views\n",
        "        A = np.vstack([\n",
        "            generate_Ai(x1n[:, i], P1n),\n",
        "            generate_Ai(x2n[:, i], P2n)\n",
        "        ])\n",
        "\n",
        "        #Solve A X = 0 using SVD\n",
        "        _, _, Vt = np.linalg.svd(A, full_matrices=False)\n",
        "        Xi = Vt[-1, :] #Solution associated with smallest singular value\n",
        "\n",
        "        #Normalize homogeneous coordinates\n",
        "        if abs(Xi[3]) > 1e-12:\n",
        "            Xi = Xi / Xi[3]\n",
        "\n",
        "        X[:, i] = Xi\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f479aff",
      "metadata": {},
      "outputs": [],
      "source": [
        "m = matches[(1, 2)]   # shape (M,2)\n",
        "\n",
        "idx1 = m[:, 1]        # indices in image 1 keypoints\n",
        "idx2 = m[:, 0]        # indices in image 2 keypoints\n",
        "\n",
        "x1 = keypoints[img1_id][idx1].T  # shape (2,M)\n",
        "x2 = keypoints[img2_id][idx2].T  # shape (2,M)\n",
        "\n",
        "x1 = np.vstack([x1, np.ones((1, x1.shape[1]))]) # shape (3,M)\n",
        "x2 = np.vstack([x2, np.ones((1, x2.shape[1]))]) # shape (3,M)\n",
        "\n",
        "# Color\n",
        "x_img = np.transpose(x1[:2]).astype(int)\n",
        "rgb_txt = (image1[x_img[:,1], x_img[:,0]])/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6de078f",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_pred = triangulate(x1, x2, P1, P2, [camera.width, camera.height])\n",
        "X_eucl = X_pred/X_pred[-1,:]\n",
        "\n",
        "pcd = open3d.geometry.PointCloud()\n",
        "pcd.points = open3d.utility.Vector3dVector(X_eucl[:3].T)\n",
        "pcd.colors = open3d.utility.Vector3dVector(rgb_txt)\n",
        "pcd_clean, inlier_idx = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
        "\n",
        "# Visualize or convert back to numpy\n",
        "points3d = np.asarray(pcd_clean.points)\n",
        "rgb_txt = np.asarray(pcd_clean.colors)\n",
        "\n",
        "# points3d = X_eucl[:3].T\n",
        "fig = go.Figure(layout=dict(height=400, width=550, title=\"Triangulated 3D points (COLMAP)\"))\n",
        "# points3d = X_eucl[:3].T\n",
        "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color=rgb_txt, size=2)))\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1713046b",
      "metadata": {},
      "source": [
        "When comparing the Lab 3 reconstruction with the Lab 4 result obtained using COLMAP camera parameters, we can see that there are way more points that get triangulated to 3D space. In the Lab 3 triangulation most points are concentrated in high detail regions and seem to not be present in the regions corresponding to to the relatively low texture wall. We believe this is not an issue with the triangulation, but with the feature detection and matching. Overall, both triangulations allow to recognize both walls and the tower in the corner.\n",
        "\n",
        "In **Figure 14**, the 3D point cloud obtained in Lab 3 is shown. This reconstruction was computed using the same triangulation pipeline but with camera matrices estimated within the lab framework and a reduced set of feature correspondences.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;\"> \n",
        "    <div style=\"text-align: center;\"> \n",
        "    <img src=\"figures/triangulation_lab3.png\" style=\"width: 45%;\"> \n",
        "    </div> \n",
        "</div> \n",
        "<p align=\"center\"><b>Figure 14:</b> Triangulated 3D point cloud obtained in Lab 3 using the estimated camera matrices and feature matches from Lab 3.</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10eacbca-842d-4d9e-85bd-efc35c5dd15a",
      "metadata": {},
      "source": [
        "## 2.7 Visualize the sparse reconstruction using the 2 images from lab 3, and the complete CASTLE dataset. Comment on the differences between techniques and number of images used. (1.0)\n",
        "#### <span style='color:Green'> - Use the reconstruction from Colmap to the get the 3D points and visualize them following the same style, using two images and the complete dataset. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b076a4",
      "metadata": {},
      "source": [
        "# Using 2 images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9504a3-dcbf-4b2b-9ff6-71c5a4584742",
      "metadata": {},
      "outputs": [],
      "source": [
        "reconstruction_path = \"./castle/sparse/0\"\n",
        "database_path = \"./castle/database.db\"\n",
        "\n",
        "model = Model()\n",
        "model.read_model(reconstruction_path, ext='.bin') # Should also work with .txt\n",
        "\n",
        "model.create_window()\n",
        "model.add_points(min_track_len=2) # Filter points seen in at least 2 images\n",
        "model.add_cameras(scale=2)\n",
        "model.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5cc414d",
      "metadata": {},
      "source": [
        "# Using 30 images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb0af21",
      "metadata": {},
      "outputs": [],
      "source": [
        "reconstruction_path = \"./castle_P30/sparse/0\"\n",
        "database_path = \"./castle_P30/database.db\"\n",
        "\n",
        "model = Model()\n",
        "model.read_model(reconstruction_path, ext='.bin') # Should also work with .txt\n",
        "\n",
        "model.create_window()\n",
        "model.add_points() \n",
        "model.add_cameras(scale=0.25)\n",
        "model.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "601a6299",
      "metadata": {},
      "source": [
        "**Figure 15** compares the sparse 3D reconstructions obtained.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\"> \n",
        "    <div style=\"text-align: center; width: 45%\"> \n",
        "        <img src=\"figures/castle_2_images.png\", style=\"width: 60%;\"> \n",
        "        <p><b>(a)</b> Reconstruction with 2 images</p> \n",
        "    </div> \n",
        "    <div \n",
        "    style=\"text-align: center; width: 45%\"> \n",
        "        <img src=\"figures/castle_30_images.png\", style=\"width: 85%;\"> \n",
        "        <p><b>(b)</b> Reconstruction with 30 images</p> \n",
        "    </div> \n",
        "</div> <p align=\"center\"><b>Figure 15:</b> Comparison of sparse COLMAP reconstructions using two images and thirty images.</p>\n",
        "\n",
        "With only 2 images, the reconstructed point cloud appears sparse and irregular, with a limited number of points forming an incomplete structure and several isolated outliers. The geometry lacks continuity and clear surface definition, reflecting the reduced amount of visual information available. Additionally, the camera poses are located far from the reconstructed points; in order to make the cameras visible in the visualization, the camera scale had to be increased (`scale = 2`), as they were otherwise too small to be observed.\n",
        "\n",
        "With 30 images, the reconstructed point cloud is significantly denser and more continuous, forming well-defined surfaces and structures with far fewer isolated points. The scene geometry appears more complete and coherent, and the presence of multiple camera poses distributed around the scene is clearly visible. In contrast to the 2-image case, the cameras are naturally well-scaled with respect to the point cloud and can be clearly observed without requiring manual scale adjustments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32408947",
      "metadata": {},
      "source": [
        "# 3. Configure the reconstruction to improve the results. (1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89648246",
      "metadata": {},
      "source": [
        "To improve the reconstruction quality of the *castle* dataset, we created a custom COLMAP script (`colmap_improved.sh`) with adjusted parameters. In this section, we describe the main parameters we used and explain why they help improve the reconstruction.\n",
        "\n",
        "The original reconstruction of the castle contained missing regions and holes, especially on walls and the floor. These areas either have very little texture or very repetitive patterns, which makes them difficult for feature-based reconstruction methods to handle.\n",
        "\n",
        "**Figure 16** shows examples of these missing parts and holes.\n",
        "\n",
        "<div style=\"display: flex; center; gap: 10px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/castle_dense_base.png\" height=\"300\"/>\n",
        "    <p><b>(a)</b> Base reconstruction</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/castle_dense_better.png\" height=\"300\"/>\n",
        "    <p><b>(b)</b> Improved reconstruction</p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 16:</b> Reconstructed mesh of <i>castle</i> scene both with automatic reconstruction and with custom script.</p>\n",
        "\n",
        "## COLMAP Pipeline Overview\n",
        "\n",
        "COLMAP uses a step-by-step pipeline, where each stage depends on the previous one:\n",
        "\n",
        "1. **Feature Extraction**: Detect keypoints and compute descriptors\n",
        "2. **Feature Matching**: Match keypoints between images\n",
        "3. **Sparse Reconstruction**: Estimate camera poses and triangulate 3D points\n",
        "4. **Dense Stereo**: Compute depth maps for each image\n",
        "5. **Depth Fusion**: Merge depth maps into a dense point cloud\n",
        "6. **Meshing**: Convert the point cloud into a mesh\n",
        "\n",
        "By understanding this pipeline, we can identify which stages are most relevant for fixing the observed issues.\n",
        "\n",
        "We have checked the parameters <a href=\"https://github.com/mwtarnowski/colmap-parameters\">here</a>.\n",
        "\n",
        "## Parameter Analysis by Pipeline Stage\n",
        "\n",
        "### 1. Feature Extraction\n",
        "\n",
        "We assumed that low-texture areas do not contain enough detected keypoints, which leads to missing matches. COLMAP uses SIFT to detect keypoints at different scales.\n",
        "\n",
        "To increase the number of detected keypoints, especially in low-texture regions, we changed the following parameters:\n",
        "\n",
        "* **`num_octaves`: 4 --> 5**\n",
        "  Adding an extra octave allows SIFT to detect larger-scale features. This can help in areas where fine details are missing, like in the flat walls.\n",
        "<!--\n",
        "* **`octave_resolution`: 3 -> 4**\n",
        "  Increasing the number of scales per octave enables more precise scale sampling, which can help detect features that would otherwise be skipped.\n",
        "-->\n",
        "\n",
        "* **`peak_threshold`: 0.0067 --> 0.006**\n",
        "  We lower this threshold to make SIFT more sensitive to low-contrast features, which are common in flat or weakly textured regions.\n",
        "\n",
        "### 2. Feature Matching\n",
        "\n",
        "Even when more keypoints are detected, matching them in low-texture regions is still difficult because descriptors can look very similar. To address this, we enabled:\n",
        "\n",
        "* **`guided_matching`: 0 --> 1**\n",
        "  As recommended by the COLMAP tutorial, we enable guided matching, which uses the estimated epipolar geometry to search for additional matches. This helps find valid matches that descriptor-only matching might not find.\n",
        "\n",
        "### 3. Sparse Reconstruction (Mapper)\n",
        "\n",
        "The sparse reconstruction step estimates camera poses and 3D points incrementally. We modified:\n",
        "\n",
        "* **`ba_refine_principal_point`: 0 --> 1**\n",
        "  Enabling this option allows bundle adjustment to optimize the principal point, which can slightly improve camera calibration accuracy.\n",
        "\n",
        "We think this change is not critical to improving the reconstruction, but can help marginally.\n",
        "\n",
        "### 4. Dense Stereo (PatchMatch)\n",
        "\n",
        "Dense stereo is the most important stage for fixing the holes in walls and floors. PatchMatch compares image patches across views to estimate depth.\n",
        "\n",
        "* **`window_radius`: 5 --> 7**\n",
        "  This increases the patch size from $11 \\times 11$ to $15 \\times 15$ pixels. Larger patches provide more context, which helps find patch matches in low-texture areas. We beleive this parameter is critical in fixing the missing parts.\n",
        "\n",
        "### 5. Depth Fusion\n",
        "\n",
        "During depth fusion, depth estimates from multiple images are combined into a single dense point cloud.\n",
        "\n",
        "* **`min_num_pixels`: 5 --> 3**\n",
        "  By lowering this threshold we allow points supported by only 3 views to be kept. This results in a denser point cloud, especially in regions supported by very few views.\n",
        "\n",
        "### 6. Poisson Meshing\n",
        "\n",
        "Poisson meshing is used to create a surface from the point cloud.\n",
        "\n",
        "* **`depth`: 13 --> 10**\n",
        "  Using a lower depth produces a smoother mesh and reduces artifacts caused by noisy points, helping to close small remaining gaps. This causes the issue of potentially losing fine scene detail.\n",
        "\n",
        "## Summary\n",
        "\n",
        "In summary, our parameter changes follow these principles:\n",
        "\n",
        "1. Detect more keypoints in difficult areas\n",
        "2. Recover additional matches using geometry\n",
        "3. Allow reconstruction to start with fewer matches\n",
        "4. Use larger patches for more reliable depth estimation\n",
        "5. Keep more points during fusion\n",
        "6. Apply smoother meshing to reduce artifacts\n",
        "\n",
        "Together, these changes significantly reduce holes and missing regions in the final reconstruction.\n",
        "\n",
        "## Post-processing\n",
        "Although the reconstruction quality was improved by adjusting the parameters described above, the result was still not perfect. For this reason, we applied a post-processing stage using MeshLab in order to further enhance the final reconstruction.\n",
        "\n",
        "Our objective was to combine the improved **Poisson mesh** with the **Delaunay mesh** to obtain a more complete surface and to fill the missing regions present in the reconstruction. \n",
        "\n",
        "* First, isolated regions were removed from both meshes using the Remove Isolated Pieces (w.r.t. Diameter) filter, and remaining artifacts were manually deleted. \n",
        "* Then, the Delaunay mesh was smoothed using the `HC Laplacian Smoothing filter` to obtain a smoother and more regular surface.\n",
        "* Afterwards, color information from the Poisson mesh was transferred to the Delaunay mesh using `Sampling > Vertex Attribute Transfer`.\n",
        "* Finally, both meshes were merged into a single model using `Filters > Mesh Layer > Flatten Visible Layers`.\n",
        "\n",
        "The resulting post-processed reconstruction is shown in **Figure 17**.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/castle_post-processed_1.png\" style=\"width: 85%;\">\n",
        "    <p><b>(a)</b> Front view of the post-processed reconstruction.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/castle_post-processed_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> Rear view of the post-processed reconstruction.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 17:</b> Post-processed reconstruction obtained by combining the Poisson and Delaunay meshes after artifact removal, smoothing, color transfer, and mesh merging.\n",
        "</p>\n",
        "\n",
        "\n",
        "You can download the meshes from here:\n",
        "\n",
        "* [Castle raw mesh](https://drive.google.com/file/d/1ej_pujs_XPFn0cvNjimg0Zy8eb8i7NTJ/view?usp=sharing)\n",
        "\n",
        "* [Castle configured mesh](https://drive.google.com/file/d/1pBHUrYt-E11S7HTl4ITvFyz_J9Gdu-sr/view?usp=sharing)\n",
        "\n",
        "* [Castle post-processed mesh](https://drive.google.com/file/d/19UBu3DBg2tbg9iIkrbqbvjosgrB8tcIx/view?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b353928f",
      "metadata": {},
      "source": [
        "# 4. Reconstruct a 3D mesh from images captured by you. (1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a38f53",
      "metadata": {},
      "source": [
        "Before taking our own photographs, we ensured that the camera focus and exposure were fixed. This was done to keep the camera parameters consistent across all images and to avoid changes in appearance that could negatively affect feature matching and reconstruction.\n",
        "\n",
        "## UPF Campus\n",
        "\n",
        "To reconstruct the UPF Campus, we captured the images using two different strategies in order to evaluate which one produced better reconstruction results.\n",
        "\n",
        "* In the **first strategy**, we stood approximately at the center of the campus and took photos while rotating around ourselves, capturing images in all directions with good overlap.\n",
        "\n",
        "* In the **second strategy**, we walked close to the campus walls and took photos of the opposite faÃ§ades, moving around the campus in a roughly rectangular path.\n",
        "\n",
        "**Figure 18** shows the *sparse* reconstructions obtained with each acquisition strategy.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/approach_1.png\" style=\"width: 73%;\">\n",
        "    <p><b>(a)</b> Sparse reconstruction using the first strategy.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/approach_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> Sparse reconstruction using the second strategy.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 18:</b> Sparse reconstructions of the UPF Campus obtained with two different image acquisition strategies.\n",
        "</p>\n",
        "\n",
        "We did not initially expect the first strategy to produce a spherical-like reconstruction. However, after analyzing the result of the second strategy, this behavior became more understandable.\n",
        "\n",
        "Since the images in the first strategy were captured from a single location while rotating around the camera, the camera centers remain almost at the same position and only the viewing direction changes. As a result, the reconstructed points are distributed around the camera, forming a spherical structure.\n",
        "\n",
        "In contrast, in the second strategy the camera is physically moved along the campus perimeter. This produces a clearer camera trajectory and leads to a reconstruction that better reflects the actual spatial layout of the scene.\n",
        "\n",
        "Therefore, we selected the **second strategy** to perform the final reconstruction of the UPF Campus.\n",
        "\n",
        "After selecting the second strategy, we computed a *dense* reconstruction of the UPF Campus using COLMAPâ€™s dense pipeline. The dense point cloud was then converted into a surface using the **Poisson** surface reconstruction method, producing a raw mesh of the scene.\n",
        "\n",
        "**Figure 19** shows the resulting raw dense reconstruction obtained with the Poisson mesher, visualized from two different viewpoints. This reconstruction captures the overall geometry of the campus but still contains noise and artifacts.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/upf_raw_1.png\" style=\"width: 85%;\">\n",
        "    <p><b>(a)</b> Raw dense reconstruction - view 1.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/upf_raw_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> Raw dense reconstruction - view 2.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 19:</b> Raw dense reconstruction of the UPF Campus generated using the Poisson surface reconstruction algorithm, visualized from two different viewpoints..\n",
        "</p>\n",
        "\n",
        "We also attempted to further configure and refine the reconstruction using the script described in Section 3. However, we did not observe significant improvements in the results, as the obtained raw reconstruction was already fairly complete and stable.\n",
        "\n",
        "For this reason, instead of modifying the reconstruction parameters, we decided to apply simple **post-processing** techniques in Meshlab to improve the visual quality of the model. We generated two final mesh versions, both cleaned by **manually removing artifacts and isolated regions**.\n",
        "\n",
        "The first mesh includes the **ground surface**, while the second one **removes the ground entirely**. The mesh without the ground allows for a clearer inspection of the reconstructed buildings and vertical structures, resulting in a cleaner and more visually readable model. In this version, the faÃ§ades and main architectural elements are well reconstructed and easier to analyze.\n",
        "\n",
        "Nevertheless, the mesh including the ground is also interesting to analyze. Although the ground contains several holes, these gaps are consistent with the image acquisition process, since objects such as vegetation, street furniture, or pedestrians occluded parts of the scene during image capture. Despite these occlusions, the reconstruction manages to recover portions of bushes, lampposts, and some campus furniture.\n",
        "\n",
        "**Figure 20** shows a comparison between the two post-processed meshes, visualized from different viewpoints.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/upf_with_1.png\" style=\"width: 85%;\">\n",
        "    <p><b>(a)</b> Post-processed mesh with ground â€“ view 1.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/upf_with_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> Post-processed mesh with ground â€“ view 2.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/upf_without_1.png\" style=\"width: 85%;\">\n",
        "    <p><b>(c)</b> Post-processed mesh without ground â€“ view 1.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/upf_without_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(d)</b> Post-processed mesh without ground â€“ view 2.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 20:</b> Comparison of the post-processed UPF Campus meshes.\n",
        "  The top row shows the reconstruction including the ground, while the bottom row shows the mesh without the ground,\n",
        "  allowing a clearer visualization of the reconstructed buildings and structures.\n",
        "</p>\n",
        "\n",
        "You can download the meshes from here:\n",
        "\n",
        "* [UPF raw mesh](https://drive.google.com/file/d/1OFbEfYbgiofzWY8_JmnfisznnsQlpWDD/view?usp=sharing)\n",
        "\n",
        "* [UPF post-processed mesh with ground](https://drive.google.com/file/d/1s87CK6g1ttbGBUc9jiVhnS9jkwTLCfZk/view?usp=sharing)\n",
        "\n",
        "* [UPF post-processed mesh without ground](https://drive.google.com/file/d/1u4bjIr_1-OkfeCK_zTe6JBV16_2TvTI-/view?usp=sharing)\n",
        "\n",
        "(*Note: We also attempted to further improve the reconstruction by following the post-processing steps described in Section 3, including the combination of the Poisson and Delaunay meshes to fill missing regions. However, in our case, the raw dense reconstruction obtained with the Poisson mesher was already fairly complete, and merging it with the Delaunay mesh did not result in a cleaner visualization. For this reason, we decided to keep only the Poisson-based mesh, as it provided a more visually clean and consistent reconstruction.*)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76ce9497",
      "metadata": {},
      "source": [
        "## Person\n",
        "\n",
        "We attempted to reconstruct the upper body of a person without expecting a highly accurate result, as previous attempts with simpler objects such as bottles and notebooks did not lead to satisfactory 3D reconstructions. Despite these low expectations, the result was **surprisingly good**: although the reconstruction is not complete and the back part of the body is not properly recovered, the front region is clearly and consistently reconstructed, capturing the overall shape of the person to a convincing degree.\n",
        "\n",
        "The reconstruction was generated using COLMAPâ€™s Automatic Reconstruction pipeline, followed by surface generation with the Poisson mesher. After removing isolated regions using MeshLab, the resulting model shows that the front part of the person is reconstructed reasonably well, capturing the overall shape without applying any additional configuration or parameter tuning in COLMAP. However, the back part of the body is poorly reconstructed, with fragmented and noisy geometry, making it difficult to obtain a complete and consistent surface. See **Figure 21**.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/person_1.png\" style=\"width: 85%;\">\n",
        "    <p><b>(a)</b> 3D reconstruction of a person using COLMAP's Automatic Reconstruction - view 1.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"figures/person_2.png\" style=\"width: 85%;\">\n",
        "    <p><b>(b)</b> 3D reconstruction of a person using COLMAP's Automatic Reconstruction - view 2.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <b>Figure 21:</b> Poisson surface reconstruction of the upper body of a person obtained using COLMAP Automatic Reconstruction and MeshLab post-processing..\n",
        "</p>\n",
        "\n",
        "You can download the mesh from here: [Person mesh](https://drive.google.com/file/d/1lLWKX8ODKQzOToXyA7IhrVe_Uc2Z392R/view?usp=sharing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90fe576a",
      "metadata": {},
      "source": [
        "# 5. Use Neuralangelo or Gaussian splatting to reconstruct a 3D mesh from images captured by you (Optional 2.0)\n",
        "\n",
        "In this exercise, we apply **3D Gaussian Splatting (3DGS)** to reconstruct three scenes. 3DGS represents scenes as learnable 3D Gaussians with position, covariance, opacity, and spherical harmonic coefficients for view-dependent appearance.\n",
        "\n",
        "We reconstructed:\n",
        "- **Person** (25 images): the same subject from Section 4\n",
        "- **Brick (milk carton)** (23 images): a small milk carton with text, captured by positioning the object at the center and moving the camera around it\n",
        "- **UPF Campus** (55 images): the same scene and acquisition strategy from Section 4\n",
        "\n",
        "### Training Setup\n",
        "\n",
        "We used the official 3DGS implementation with `convert.py`, which runs COLMAP internally to obtain sparse reconstruction and camera poses. Training was performed with default parameters for 30,000 iterations per scene on an RTX 4080 GPU (~30â€“45 minutes per scene).\n",
        "\n",
        "\n",
        "## Results\n",
        "\n",
        "### Quantitative Metrics\n",
        "\n",
        "We split the images into training and test sets to evaluate generalization. The following metrics were computed at iteration 30,000:\n",
        "\n",
        "| Scene | Train L1 | Train PSNR (dB) | Test L1 | Test PSNR (dB) |\n",
        "|-------|----------|-----------------|---------|----------------|\n",
        "| Person | 0.0075 | 36.98 | 0.0650 | 19.93 |\n",
        "| Brick | 0.0058 | 40.47 | 0.1330 | 16.03 |\n",
        "| UPF | 0.0211 | 28.36 | 0.0542 | 21.02 |\n",
        "\n",
        "The **Brick** scene shows the largest train/test gap (40.47 â†’ 16.03 dB PSNR, Î” = 24.44 dB), followed by **Person** (36.98 â†’ 19.93 dB, Î” = 17.05 dB), while **UPF** has the smallest gap (28.36 â†’ 21.02 dB, Î” = 7.34 dB).\n",
        "\n",
        "\n",
        "\n",
        "### Person\n",
        "\n",
        "In **Figure 22**, we show an actual captured frame and a reconstruction from the same camera position. As can be seen the images look very similar with only almost unnoticeable differences in the background.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/person/train_gt.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Real image.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/person/train_rec.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Rendered view.</p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 22:</b> Comparison between a real training image and the corresponding 3DGS render for the person scene.</p>\n",
        "\n",
        "\n",
        "In **Figure 23**, we shoud the same but for test images. As can be seen the reconstruction quality is worse and some regions appear blurry or badly reconstructed.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/person/test_gt.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Real image.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/person/test_rec.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Rendered view.</p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 23:</b> Comparison between a real test image and the corresponding 3DGS render for the person scene.</p>\n",
        "\n",
        "\n",
        "An overview of the whole Gaussian Splatting scene can be seen in **Figure 24**. It can be clearly seen that the whole scene is composed of 3D Gaussians.\n",
        "<p align=\"center\">\n",
        "  <img src=\"figures/gs/person/overview.png\" width=\"60%\">\n",
        "</p>\n",
        "<p align=\"center\"><b>Figure 24:</b> Overview of the person scene showing the Gaussian point cloud and camera positions.</p>\n",
        "\n",
        "The face and frontal body are reconstructed with high fidelity. However, when moving the virtual camera outside the training pose distribution, artifacts appear:\n",
        "\n",
        "- **Floaters**: Spurious Gaussian splats appear in empty space, particularly when viewing from camera poses other than the training ones.\n",
        "- **Head disappearing/reappearing**: When viewing the back of the head, parts of the head take on the appearance of the background wall. \n",
        "  \n",
        "This artifact is likely caused by the subject moving slightly during capture. If the head position was inconsistent across frames, the optimization would struggle to find a coherent 3D representation, causing Gaussians to \"bake in\" the wrong background colors for certain viewing directions.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <video width=\"80%\" controls>\n",
        "    <source src=\"figures/gs/person/orbit.mp4\" type=\"video/mp4\">\n",
        "  </video>\n",
        "</p>\n",
        "<p align=\"center\"><b>Video 1:</b> Novel view synthesis of the person scene, showing quality degradation when moving outside training poses.</p>\n",
        "\n",
        "### Brick (Milk Carton)\n",
        "\n",
        "In **Figures 25 and 26** we show two examples of reconstruction, the first from a camera pos seen during training and the second from an unseen camera pose. As with the previous scene, test views test views show lower quality than train ones.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/brick/train_gt.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Real image.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/brick/train_rec.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Rendered view.</p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 25:</b> Comparison between a real training image and the corresponding 3DGS render for the brick scene.</p>\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/brick/test_gt.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Real image.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/brick/test_rec.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Rendered view.</p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 26:</b> Comparison between a real test image and the corresponding 3DGS render for the brick scene.</p>\n",
        "\n",
        "The text on the milk carton is well-preserved and clear in the reconstruction. However, from certain viewing angles, the text appears duplicated or ghosted. This is likely due to small inaccuracies in the estimated camera poses from COLMAP. If the poses are slightly off, the optimization may place redundant Gaussians to explain the same text from multiple slightly-shifted viewpoints.\n",
        "\n",
        "As with the other scenes, moving the camera significantly outside the training distribution causes the reconstruction to break down.\n",
        "\n",
        "### UPF Campus\n",
        "\n",
        "We show reconstructions from a seen and unseen camera in **figures 27 and 28**. The same lower quality in test views that happened in the previous scenes can be seen. Also, moving elements (people) appear as \"ghosts\" in the scene, since during training they were seen only in some images in certain positions.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/upf/train_gt.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Real image.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/upf/train_rec.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Rendered view.</p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 27:</b> Comparison between a real training image and the corresponding 3DGS render for the UPF scene.</p>\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/upf/test_gt.png\" style=\"width: 100%;\">\n",
        "    <p><b>(a)</b> Real image.</p>\n",
        "  </div>\n",
        "  <div style=\"text-align: center; width: 45%;\">\n",
        "    <img src=\"figures/gs/upf/test_rec.png\" style=\"width: 100%;\">\n",
        "    <p><b>(b)</b> Rendered view.</p>\n",
        "  </div>\n",
        "</div>\n",
        "<p align=\"center\"><b>Figure 28:</b> Comparison between a real test image and the corresponding 3DGS render for the UPF scene.</p>\n",
        "\n",
        "The building faÃ§ades are reconstructed with good quality, preserving architectural details and textures. However, this scene exhibits several characteristic artifacts:\n",
        "\n",
        "- **Ghost people**: Since people were moving through the courtyard during capture, they appear as semi-transparent \"ghosts\" in the reconstruction. 3DGS attempts to explain their presence across multiple frames but cannot represent them coherently since they were in different positions in each image.\n",
        "\n",
        "- **Vertical camera movement sensitivity**: Moving the virtual camera up or down (outside the roughly horizontal plane of the training trajectory) causes individual Gaussians to become visually apparent as ellipsoidal blobs rather than blending into a coherent surface.\n",
        "\n",
        "\n",
        "In **Video 2**, the vertical camera movement sensitivity can be seen. When the camera gets to close or too far from the ground, the scene starts to appear broken and Gaussians become apparent.\n",
        "<p align=\"center\">\n",
        "  <video width=\"80%\" controls>\n",
        "    <source src=\"figures/gs/upf/upf.mp4\" type=\"video/mp4\">\n",
        "  </video>\n",
        "</p>\n",
        "<p align=\"center\"><b>Video 2:</b> Novel view synthesis of the UPF scene, demonstrating ghost artifacts and quality degradation when outside of the training camera poses.</p>\n",
        "\n",
        "\n",
        "## Discussion\n",
        "\n",
        "The train/test metrics reveal significant overfitting to the training views, with the effect varying substantially across scenes:\n",
        "\n",
        "**Brick (highest overfitting):** Despite achieving the best training metrics (40.47 dB), the Brick scene generalizes worst to test views (16.03 dB). This 24 dB gap suggests severe overfitting. With only 23 images of a small object, the Gaussians likely memorized specific viewpoints rather than learning a consistent 3D representation. The text duplication artifacts observed qualitatively align with this-the optimization placed redundant Gaussians that look correct from training angles but fail from novel views.\n",
        "\n",
        "**Person (moderate overfitting):** The Person scene drops from 36.98 dB to 19.93 dB (Î” = 17 dB). The subject's movement during capture (noted in the qualitative analysis as causing head appearance artifacts) likely contributes to this gap-inconsistent geometry across training frames leads to a representation that cannot generalize coherently to held-out views.\n",
        "\n",
        "**UPF (best generalization):** Despite having the lowest training PSNR (28.36 dB), the UPF scene exhibits the smallest train/test gap (7.34 dB) and the best test PSNR (21.02 dB). This is somewhat counterintuitive given the scene's complexity (outdoor, vegetation, ghost pedestrians). However, with 55 images covering a large area, the training set likely provides better viewpoint coverage relative to the test views. The scene's large scale may also regularize the optimization, preventing extreme overfitting to individual views.\n",
        "\n",
        "**Key insight:** Lower training error does not imply better reconstruction quality. The simpler scenes (Brick, Person) achieve excellent training metrics by memorizing viewpoints, while the complex UPF scene-forced to learn a more general representation due to scene diversity-actually generalizes better to novel views.\n",
        "\n",
        "For improved generalization, one could consider:\n",
        "- Capturing more images with greater viewpoint diversity\n",
        "- Using video capture for denser temporal sampling\n",
        "- Applying regularization techniques designed to prevent overfitting\n",
        "- For scenes with moving subjects (Person), ensuring the subject remains completely static\n",
        "\n",
        "\n",
        "The PLY files can be downloaded from here:\n",
        "\n",
        "* [Person](https://drive.google.com/file/d/1miI8UPN_XwW6Y-pHB7XRcxrQ0HdarJSI/view?usp=sharing)\n",
        "\n",
        "* [Brick](https://drive.google.com/file/d/1RyqKIR0lLqOeEH6oU9prRxarFQM8yM5d/view?usp=sharing)\n",
        "\n",
        "* [UPF](https://drive.google.com/file/d/1Ee3NvTs8VkVgxGK6NFylO3v2dybrZo8A/view?usp=sharing)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
